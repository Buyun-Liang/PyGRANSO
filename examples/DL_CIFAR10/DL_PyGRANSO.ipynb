{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a4d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2032e6e7ef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ae4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Net(nn.Module):\n",
    "                def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "                        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "                        self.pool = nn.MaxPool2d(2, 2)\n",
    "                        self.conv2 = nn.Conv2d(6, 10, 5)\n",
    "                        self.conv2_bn = nn.BatchNorm2d(10)\n",
    "                        self.fc1 = nn.Linear(10 * 5 * 5, 80)\n",
    "                        self.fc1_bn = nn.BatchNorm1d(80)\n",
    "                        self.fc2 = nn.Linear(80, 40)\n",
    "                        self.fc2_bn = nn.BatchNorm1d(40)\n",
    "                        self.fc3 = nn.Linear(40, 10)\n",
    "\n",
    "                def forward(self, x):\n",
    "                        x = self.pool(F.elu( self.conv1_bn(self.conv1(x))  ))\n",
    "                        x = self.pool(F.elu( self.conv2_bn(self.conv2(x))  ))\n",
    "                        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "                        x = F.elu( self.fc1_bn(self.fc1(x)) )\n",
    "                        x = F.elu( self.fc2_bn(self.fc2(x)) )\n",
    "                        x = self.fc3(x)\n",
    "                        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaef0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5237f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.768 acc: 0.429\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1092)\n",
      "norm of grandient =  tensor(5.1809e-07)\n",
      "norm of grandient =  tensor(0.0122)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2165)\n",
      "norm of grandient =  tensor(9.8646e-08)\n",
      "norm of grandient =  tensor(0.0175)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3464)\n",
      "norm of grandient =  tensor(7.1989e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2253)\n",
      "norm of grandient =  tensor(8.3488e-09)\n",
      "norm of grandient =  tensor(0.0457)\n",
      "norm of grandient =  tensor(0.0202)\n",
      "norm of grandient =  tensor(0.2895)\n",
      "norm of grandient =  tensor(0.0266)\n",
      "[2,     1] loss: 1.765 acc: 0.431\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1071)\n",
      "norm of grandient =  tensor(4.0165e-07)\n",
      "norm of grandient =  tensor(0.0118)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.2151)\n",
      "norm of grandient =  tensor(7.0286e-08)\n",
      "norm of grandient =  tensor(0.0174)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3450)\n",
      "norm of grandient =  tensor(7.5953e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2243)\n",
      "norm of grandient =  tensor(1.0313e-08)\n",
      "norm of grandient =  tensor(0.0459)\n",
      "norm of grandient =  tensor(0.0202)\n",
      "norm of grandient =  tensor(0.2892)\n",
      "norm of grandient =  tensor(0.0264)\n",
      "[3,     1] loss: 1.762 acc: 0.433\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1076)\n",
      "norm of grandient =  tensor(2.3176e-07)\n",
      "norm of grandient =  tensor(0.0115)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.2151)\n",
      "norm of grandient =  tensor(6.7754e-08)\n",
      "norm of grandient =  tensor(0.0173)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3437)\n",
      "norm of grandient =  tensor(7.3803e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2234)\n",
      "norm of grandient =  tensor(7.9342e-09)\n",
      "norm of grandient =  tensor(0.0461)\n",
      "norm of grandient =  tensor(0.0202)\n",
      "norm of grandient =  tensor(0.2889)\n",
      "norm of grandient =  tensor(0.0263)\n",
      "[4,     1] loss: 1.758 acc: 0.434\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1073)\n",
      "norm of grandient =  tensor(3.9803e-07)\n",
      "norm of grandient =  tensor(0.0115)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.2143)\n",
      "norm of grandient =  tensor(5.5348e-08)\n",
      "norm of grandient =  tensor(0.0172)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3424)\n",
      "norm of grandient =  tensor(8.9940e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2224)\n",
      "norm of grandient =  tensor(7.6144e-09)\n",
      "norm of grandient =  tensor(0.0463)\n",
      "norm of grandient =  tensor(0.0202)\n",
      "norm of grandient =  tensor(0.2886)\n",
      "norm of grandient =  tensor(0.0262)\n",
      "[5,     1] loss: 1.755 acc: 0.434\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1075)\n",
      "norm of grandient =  tensor(7.9006e-07)\n",
      "norm of grandient =  tensor(0.0118)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2136)\n",
      "norm of grandient =  tensor(1.1757e-07)\n",
      "norm of grandient =  tensor(0.0170)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3411)\n",
      "norm of grandient =  tensor(8.4559e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2214)\n",
      "norm of grandient =  tensor(1.0639e-08)\n",
      "norm of grandient =  tensor(0.0465)\n",
      "norm of grandient =  tensor(0.0203)\n",
      "norm of grandient =  tensor(0.2883)\n",
      "norm of grandient =  tensor(0.0261)\n",
      "[6,     1] loss: 1.752 acc: 0.436\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1077)\n",
      "norm of grandient =  tensor(5.4312e-07)\n",
      "norm of grandient =  tensor(0.0120)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2132)\n",
      "norm of grandient =  tensor(7.5599e-08)\n",
      "norm of grandient =  tensor(0.0169)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3398)\n",
      "norm of grandient =  tensor(8.4278e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2204)\n",
      "norm of grandient =  tensor(9.8057e-09)\n",
      "norm of grandient =  tensor(0.0467)\n",
      "norm of grandient =  tensor(0.0203)\n",
      "norm of grandient =  tensor(0.2880)\n",
      "norm of grandient =  tensor(0.0260)\n",
      "[7,     1] loss: 1.749 acc: 0.439\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1073)\n",
      "norm of grandient =  tensor(2.3689e-07)\n",
      "norm of grandient =  tensor(0.0120)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2124)\n",
      "norm of grandient =  tensor(6.3527e-08)\n",
      "norm of grandient =  tensor(0.0168)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3385)\n",
      "norm of grandient =  tensor(8.0138e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2195)\n",
      "norm of grandient =  tensor(9.9742e-09)\n",
      "norm of grandient =  tensor(0.0469)\n",
      "norm of grandient =  tensor(0.0203)\n",
      "norm of grandient =  tensor(0.2876)\n",
      "norm of grandient =  tensor(0.0258)\n",
      "[8,     1] loss: 1.746 acc: 0.441\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1068)\n",
      "norm of grandient =  tensor(5.8548e-07)\n",
      "norm of grandient =  tensor(0.0122)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2107)\n",
      "norm of grandient =  tensor(1.0911e-07)\n",
      "norm of grandient =  tensor(0.0167)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3373)\n",
      "norm of grandient =  tensor(7.2758e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2185)\n",
      "norm of grandient =  tensor(7.0617e-09)\n",
      "norm of grandient =  tensor(0.0471)\n",
      "norm of grandient =  tensor(0.0203)\n",
      "norm of grandient =  tensor(0.2873)\n",
      "norm of grandient =  tensor(0.0257)\n",
      "[9,     1] loss: 1.743 acc: 0.442\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1064)\n",
      "norm of grandient =  tensor(6.0282e-07)\n",
      "norm of grandient =  tensor(0.0122)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2100)\n",
      "norm of grandient =  tensor(6.1366e-08)\n",
      "norm of grandient =  tensor(0.0166)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3361)\n",
      "norm of grandient =  tensor(7.5837e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2176)\n",
      "norm of grandient =  tensor(8.1788e-09)\n",
      "norm of grandient =  tensor(0.0472)\n",
      "norm of grandient =  tensor(0.0204)\n",
      "norm of grandient =  tensor(0.2870)\n",
      "norm of grandient =  tensor(0.0256)\n",
      "[10,     1] loss: 1.740 acc: 0.444\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1063)\n",
      "norm of grandient =  tensor(2.4610e-07)\n",
      "norm of grandient =  tensor(0.0122)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2090)\n",
      "norm of grandient =  tensor(8.2990e-08)\n",
      "norm of grandient =  tensor(0.0165)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3349)\n",
      "norm of grandient =  tensor(7.6504e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2167)\n",
      "norm of grandient =  tensor(1.0563e-08)\n",
      "norm of grandient =  tensor(0.0474)\n",
      "norm of grandient =  tensor(0.0204)\n",
      "norm of grandient =  tensor(0.2867)\n",
      "norm of grandient =  tensor(0.0255)\n",
      "[11,     1] loss: 1.737 acc: 0.447\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1080)\n",
      "norm of grandient =  tensor(3.4037e-07)\n",
      "norm of grandient =  tensor(0.0123)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.2083)\n",
      "norm of grandient =  tensor(8.8983e-08)\n",
      "norm of grandient =  tensor(0.0164)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3338)\n",
      "norm of grandient =  tensor(7.4441e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2158)\n",
      "norm of grandient =  tensor(9.4321e-09)\n",
      "norm of grandient =  tensor(0.0476)\n",
      "norm of grandient =  tensor(0.0204)\n",
      "norm of grandient =  tensor(0.2864)\n",
      "norm of grandient =  tensor(0.0254)\n",
      "[12,     1] loss: 1.734 acc: 0.450\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1086)\n",
      "norm of grandient =  tensor(5.2818e-07)\n",
      "norm of grandient =  tensor(0.0121)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.2070)\n",
      "norm of grandient =  tensor(1.2126e-07)\n",
      "norm of grandient =  tensor(0.0163)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3327)\n",
      "norm of grandient =  tensor(8.2067e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2149)\n",
      "norm of grandient =  tensor(1.0165e-08)\n",
      "norm of grandient =  tensor(0.0478)\n",
      "norm of grandient =  tensor(0.0205)\n",
      "norm of grandient =  tensor(0.2861)\n",
      "norm of grandient =  tensor(0.0253)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13,     1] loss: 1.731 acc: 0.450\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1088)\n",
      "norm of grandient =  tensor(4.2362e-07)\n",
      "norm of grandient =  tensor(0.0121)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.2053)\n",
      "norm of grandient =  tensor(8.4695e-08)\n",
      "norm of grandient =  tensor(0.0162)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3317)\n",
      "norm of grandient =  tensor(7.2933e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2140)\n",
      "norm of grandient =  tensor(7.5687e-09)\n",
      "norm of grandient =  tensor(0.0480)\n",
      "norm of grandient =  tensor(0.0205)\n",
      "norm of grandient =  tensor(0.2857)\n",
      "norm of grandient =  tensor(0.0252)\n",
      "[14,     1] loss: 1.728 acc: 0.452\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1074)\n",
      "norm of grandient =  tensor(3.7669e-07)\n",
      "norm of grandient =  tensor(0.0123)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.2050)\n",
      "norm of grandient =  tensor(8.9537e-08)\n",
      "norm of grandient =  tensor(0.0160)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3306)\n",
      "norm of grandient =  tensor(8.7634e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2132)\n",
      "norm of grandient =  tensor(7.8715e-09)\n",
      "norm of grandient =  tensor(0.0481)\n",
      "norm of grandient =  tensor(0.0205)\n",
      "norm of grandient =  tensor(0.2854)\n",
      "norm of grandient =  tensor(0.0251)\n",
      "[15,     1] loss: 1.725 acc: 0.454\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1070)\n",
      "norm of grandient =  tensor(5.8197e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2052)\n",
      "norm of grandient =  tensor(1.6060e-07)\n",
      "norm of grandient =  tensor(0.0159)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3296)\n",
      "norm of grandient =  tensor(8.0734e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2124)\n",
      "norm of grandient =  tensor(1.0187e-08)\n",
      "norm of grandient =  tensor(0.0483)\n",
      "norm of grandient =  tensor(0.0206)\n",
      "norm of grandient =  tensor(0.2851)\n",
      "norm of grandient =  tensor(0.0250)\n",
      "[16,     1] loss: 1.722 acc: 0.454\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1072)\n",
      "norm of grandient =  tensor(3.8321e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2046)\n",
      "norm of grandient =  tensor(1.1464e-07)\n",
      "norm of grandient =  tensor(0.0158)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3287)\n",
      "norm of grandient =  tensor(8.5845e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2116)\n",
      "norm of grandient =  tensor(7.3473e-09)\n",
      "norm of grandient =  tensor(0.0485)\n",
      "norm of grandient =  tensor(0.0206)\n",
      "norm of grandient =  tensor(0.2848)\n",
      "norm of grandient =  tensor(0.0249)\n",
      "[17,     1] loss: 1.719 acc: 0.455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1058)\n",
      "norm of grandient =  tensor(8.0678e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2040)\n",
      "norm of grandient =  tensor(6.0222e-08)\n",
      "norm of grandient =  tensor(0.0157)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3277)\n",
      "norm of grandient =  tensor(7.6878e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2108)\n",
      "norm of grandient =  tensor(8.7023e-09)\n",
      "norm of grandient =  tensor(0.0486)\n",
      "norm of grandient =  tensor(0.0206)\n",
      "norm of grandient =  tensor(0.2844)\n",
      "norm of grandient =  tensor(0.0248)\n",
      "[18,     1] loss: 1.716 acc: 0.455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1036)\n",
      "norm of grandient =  tensor(5.1949e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2032)\n",
      "norm of grandient =  tensor(6.9557e-08)\n",
      "norm of grandient =  tensor(0.0156)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3268)\n",
      "norm of grandient =  tensor(8.8181e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2101)\n",
      "norm of grandient =  tensor(9.9636e-09)\n",
      "norm of grandient =  tensor(0.0488)\n",
      "norm of grandient =  tensor(0.0207)\n",
      "norm of grandient =  tensor(0.2841)\n",
      "norm of grandient =  tensor(0.0247)\n",
      "[19,     1] loss: 1.713 acc: 0.455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1039)\n",
      "norm of grandient =  tensor(7.3395e-07)\n",
      "norm of grandient =  tensor(0.0124)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.2023)\n",
      "norm of grandient =  tensor(7.3357e-08)\n",
      "norm of grandient =  tensor(0.0155)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3259)\n",
      "norm of grandient =  tensor(8.7199e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2093)\n",
      "norm of grandient =  tensor(1.0125e-08)\n",
      "norm of grandient =  tensor(0.0490)\n",
      "norm of grandient =  tensor(0.0207)\n",
      "norm of grandient =  tensor(0.2838)\n",
      "norm of grandient =  tensor(0.0246)\n",
      "[20,     1] loss: 1.710 acc: 0.455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1034)\n",
      "norm of grandient =  tensor(4.8691e-07)\n",
      "norm of grandient =  tensor(0.0127)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2013)\n",
      "norm of grandient =  tensor(7.9233e-08)\n",
      "norm of grandient =  tensor(0.0154)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3250)\n",
      "norm of grandient =  tensor(6.6896e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2086)\n",
      "norm of grandient =  tensor(1.0401e-08)\n",
      "norm of grandient =  tensor(0.0491)\n",
      "norm of grandient =  tensor(0.0207)\n",
      "norm of grandient =  tensor(0.2834)\n",
      "norm of grandient =  tensor(0.0245)\n",
      "[21,     1] loss: 1.707 acc: 0.456\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1042)\n",
      "norm of grandient =  tensor(5.7075e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2018)\n",
      "norm of grandient =  tensor(9.2459e-08)\n",
      "norm of grandient =  tensor(0.0153)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3241)\n",
      "norm of grandient =  tensor(7.5825e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2078)\n",
      "norm of grandient =  tensor(8.7300e-09)\n",
      "norm of grandient =  tensor(0.0493)\n",
      "norm of grandient =  tensor(0.0208)\n",
      "norm of grandient =  tensor(0.2831)\n",
      "norm of grandient =  tensor(0.0244)\n",
      "[22,     1] loss: 1.704 acc: 0.457\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1039)\n",
      "norm of grandient =  tensor(8.8776e-07)\n",
      "norm of grandient =  tensor(0.0127)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.2004)\n",
      "norm of grandient =  tensor(1.1410e-07)\n",
      "norm of grandient =  tensor(0.0152)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3232)\n",
      "norm of grandient =  tensor(7.9309e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2071)\n",
      "norm of grandient =  tensor(9.0342e-09)\n",
      "norm of grandient =  tensor(0.0494)\n",
      "norm of grandient =  tensor(0.0208)\n",
      "norm of grandient =  tensor(0.2828)\n",
      "norm of grandient =  tensor(0.0243)\n",
      "[23,     1] loss: 1.701 acc: 0.455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1013)\n",
      "norm of grandient =  tensor(3.5582e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1989)\n",
      "norm of grandient =  tensor(1.3897e-07)\n",
      "norm of grandient =  tensor(0.0151)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3223)\n",
      "norm of grandient =  tensor(8.6868e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2064)\n",
      "norm of grandient =  tensor(9.5541e-09)\n",
      "norm of grandient =  tensor(0.0496)\n",
      "norm of grandient =  tensor(0.0208)\n",
      "norm of grandient =  tensor(0.2824)\n",
      "norm of grandient =  tensor(0.0242)\n",
      "[24,     1] loss: 1.699 acc: 0.457\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1010)\n",
      "norm of grandient =  tensor(3.9575e-07)\n",
      "norm of grandient =  tensor(0.0127)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1982)\n",
      "norm of grandient =  tensor(7.7810e-08)\n",
      "norm of grandient =  tensor(0.0150)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3215)\n",
      "norm of grandient =  tensor(7.9438e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2057)\n",
      "norm of grandient =  tensor(9.7914e-09)\n",
      "norm of grandient =  tensor(0.0498)\n",
      "norm of grandient =  tensor(0.0209)\n",
      "norm of grandient =  tensor(0.2821)\n",
      "norm of grandient =  tensor(0.0242)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25,     1] loss: 1.696 acc: 0.460\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1008)\n",
      "norm of grandient =  tensor(7.2942e-07)\n",
      "norm of grandient =  tensor(0.0130)\n",
      "norm of grandient =  tensor(0.0087)\n",
      "norm of grandient =  tensor(0.1979)\n",
      "norm of grandient =  tensor(9.2747e-08)\n",
      "norm of grandient =  tensor(0.0149)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3207)\n",
      "norm of grandient =  tensor(8.9587e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0057)\n",
      "norm of grandient =  tensor(0.2050)\n",
      "norm of grandient =  tensor(9.4695e-09)\n",
      "norm of grandient =  tensor(0.0499)\n",
      "norm of grandient =  tensor(0.0209)\n",
      "norm of grandient =  tensor(0.2818)\n",
      "norm of grandient =  tensor(0.0241)\n",
      "[26,     1] loss: 1.693 acc: 0.462\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1012)\n",
      "norm of grandient =  tensor(4.9339e-07)\n",
      "norm of grandient =  tensor(0.0130)\n",
      "norm of grandient =  tensor(0.0087)\n",
      "norm of grandient =  tensor(0.1978)\n",
      "norm of grandient =  tensor(1.2402e-07)\n",
      "norm of grandient =  tensor(0.0148)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3199)\n",
      "norm of grandient =  tensor(9.2128e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2044)\n",
      "norm of grandient =  tensor(6.5163e-09)\n",
      "norm of grandient =  tensor(0.0501)\n",
      "norm of grandient =  tensor(0.0209)\n",
      "norm of grandient =  tensor(0.2814)\n",
      "norm of grandient =  tensor(0.0240)\n",
      "[27,     1] loss: 1.690 acc: 0.464\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1023)\n",
      "norm of grandient =  tensor(2.9381e-07)\n",
      "norm of grandient =  tensor(0.0126)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1973)\n",
      "norm of grandient =  tensor(1.1771e-07)\n",
      "norm of grandient =  tensor(0.0146)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3191)\n",
      "norm of grandient =  tensor(8.2897e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2037)\n",
      "norm of grandient =  tensor(8.1528e-09)\n",
      "norm of grandient =  tensor(0.0502)\n",
      "norm of grandient =  tensor(0.0210)\n",
      "norm of grandient =  tensor(0.2811)\n",
      "norm of grandient =  tensor(0.0239)\n",
      "[28,     1] loss: 1.687 acc: 0.468\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1026)\n",
      "norm of grandient =  tensor(5.9400e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0087)\n",
      "norm of grandient =  tensor(0.1968)\n",
      "norm of grandient =  tensor(9.9349e-08)\n",
      "norm of grandient =  tensor(0.0145)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3183)\n",
      "norm of grandient =  tensor(8.2713e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2030)\n",
      "norm of grandient =  tensor(8.5148e-09)\n",
      "norm of grandient =  tensor(0.0504)\n",
      "norm of grandient =  tensor(0.0210)\n",
      "norm of grandient =  tensor(0.2808)\n",
      "norm of grandient =  tensor(0.0239)\n",
      "[29,     1] loss: 1.685 acc: 0.468\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1033)\n",
      "norm of grandient =  tensor(4.3833e-07)\n",
      "norm of grandient =  tensor(0.0130)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1967)\n",
      "norm of grandient =  tensor(9.7710e-08)\n",
      "norm of grandient =  tensor(0.0144)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3175)\n",
      "norm of grandient =  tensor(7.8991e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2024)\n",
      "norm of grandient =  tensor(8.6412e-09)\n",
      "norm of grandient =  tensor(0.0505)\n",
      "norm of grandient =  tensor(0.0210)\n",
      "norm of grandient =  tensor(0.2804)\n",
      "norm of grandient =  tensor(0.0238)\n",
      "[30,     1] loss: 1.682 acc: 0.470\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1029)\n",
      "norm of grandient =  tensor(4.8424e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1960)\n",
      "norm of grandient =  tensor(7.2786e-08)\n",
      "norm of grandient =  tensor(0.0143)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3168)\n",
      "norm of grandient =  tensor(6.7630e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2018)\n",
      "norm of grandient =  tensor(9.3355e-09)\n",
      "norm of grandient =  tensor(0.0507)\n",
      "norm of grandient =  tensor(0.0211)\n",
      "norm of grandient =  tensor(0.2801)\n",
      "norm of grandient =  tensor(0.0237)\n",
      "[31,     1] loss: 1.679 acc: 0.472\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1033)\n",
      "norm of grandient =  tensor(5.2080e-07)\n",
      "norm of grandient =  tensor(0.0130)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.1954)\n",
      "norm of grandient =  tensor(1.5502e-07)\n",
      "norm of grandient =  tensor(0.0142)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3161)\n",
      "norm of grandient =  tensor(7.7214e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2012)\n",
      "norm of grandient =  tensor(8.2938e-09)\n",
      "norm of grandient =  tensor(0.0508)\n",
      "norm of grandient =  tensor(0.0211)\n",
      "norm of grandient =  tensor(0.2798)\n",
      "norm of grandient =  tensor(0.0236)\n",
      "[32,     1] loss: 1.676 acc: 0.472\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1048)\n",
      "norm of grandient =  tensor(3.7916e-07)\n",
      "norm of grandient =  tensor(0.0131)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1949)\n",
      "norm of grandient =  tensor(1.3601e-07)\n",
      "norm of grandient =  tensor(0.0141)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3154)\n",
      "norm of grandient =  tensor(8.4863e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2006)\n",
      "norm of grandient =  tensor(8.5924e-09)\n",
      "norm of grandient =  tensor(0.0510)\n",
      "norm of grandient =  tensor(0.0212)\n",
      "norm of grandient =  tensor(0.2795)\n",
      "norm of grandient =  tensor(0.0236)\n",
      "[33,     1] loss: 1.674 acc: 0.474\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1064)\n",
      "norm of grandient =  tensor(4.2570e-07)\n",
      "norm of grandient =  tensor(0.0131)\n",
      "norm of grandient =  tensor(0.0087)\n",
      "norm of grandient =  tensor(0.1944)\n",
      "norm of grandient =  tensor(9.5727e-08)\n",
      "norm of grandient =  tensor(0.0140)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3147)\n",
      "norm of grandient =  tensor(8.3342e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.2000)\n",
      "norm of grandient =  tensor(1.1266e-08)\n",
      "norm of grandient =  tensor(0.0511)\n",
      "norm of grandient =  tensor(0.0212)\n",
      "norm of grandient =  tensor(0.2791)\n",
      "norm of grandient =  tensor(0.0235)\n",
      "[34,     1] loss: 1.671 acc: 0.477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1078)\n",
      "norm of grandient =  tensor(5.5127e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0088)\n",
      "norm of grandient =  tensor(0.1943)\n",
      "norm of grandient =  tensor(1.0873e-07)\n",
      "norm of grandient =  tensor(0.0139)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3140)\n",
      "norm of grandient =  tensor(8.4178e-09)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1994)\n",
      "norm of grandient =  tensor(9.4130e-09)\n",
      "norm of grandient =  tensor(0.0513)\n",
      "norm of grandient =  tensor(0.0212)\n",
      "norm of grandient =  tensor(0.2788)\n",
      "norm of grandient =  tensor(0.0234)\n",
      "[35,     1] loss: 1.668 acc: 0.479\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1090)\n",
      "norm of grandient =  tensor(3.5594e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.1936)\n",
      "norm of grandient =  tensor(5.8245e-08)\n",
      "norm of grandient =  tensor(0.0138)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3134)\n",
      "norm of grandient =  tensor(6.4007e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1989)\n",
      "norm of grandient =  tensor(8.5273e-09)\n",
      "norm of grandient =  tensor(0.0514)\n",
      "norm of grandient =  tensor(0.0213)\n",
      "norm of grandient =  tensor(0.2785)\n",
      "norm of grandient =  tensor(0.0234)\n",
      "[36,     1] loss: 1.665 acc: 0.479\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1097)\n",
      "norm of grandient =  tensor(5.1180e-07)\n",
      "norm of grandient =  tensor(0.0127)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1929)\n",
      "norm of grandient =  tensor(7.7506e-08)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3127)\n",
      "norm of grandient =  tensor(7.8259e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1983)\n",
      "norm of grandient =  tensor(8.7346e-09)\n",
      "norm of grandient =  tensor(0.0516)\n",
      "norm of grandient =  tensor(0.0213)\n",
      "norm of grandient =  tensor(0.2782)\n",
      "norm of grandient =  tensor(0.0233)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37,     1] loss: 1.663 acc: 0.478\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1099)\n",
      "norm of grandient =  tensor(3.6946e-07)\n",
      "norm of grandient =  tensor(0.0128)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1925)\n",
      "norm of grandient =  tensor(6.5785e-08)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3121)\n",
      "norm of grandient =  tensor(8.0996e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1978)\n",
      "norm of grandient =  tensor(8.7934e-09)\n",
      "norm of grandient =  tensor(0.0517)\n",
      "norm of grandient =  tensor(0.0213)\n",
      "norm of grandient =  tensor(0.2779)\n",
      "norm of grandient =  tensor(0.0233)\n",
      "[38,     1] loss: 1.660 acc: 0.482\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1093)\n",
      "norm of grandient =  tensor(3.5704e-07)\n",
      "norm of grandient =  tensor(0.0128)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1919)\n",
      "norm of grandient =  tensor(8.6361e-08)\n",
      "norm of grandient =  tensor(0.0135)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3116)\n",
      "norm of grandient =  tensor(7.8542e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1973)\n",
      "norm of grandient =  tensor(7.4525e-09)\n",
      "norm of grandient =  tensor(0.0519)\n",
      "norm of grandient =  tensor(0.0214)\n",
      "norm of grandient =  tensor(0.2776)\n",
      "norm of grandient =  tensor(0.0232)\n",
      "[39,     1] loss: 1.657 acc: 0.483\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1075)\n",
      "norm of grandient =  tensor(6.4704e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1917)\n",
      "norm of grandient =  tensor(6.5111e-08)\n",
      "norm of grandient =  tensor(0.0134)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3110)\n",
      "norm of grandient =  tensor(8.2170e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1968)\n",
      "norm of grandient =  tensor(1.1118e-08)\n",
      "norm of grandient =  tensor(0.0520)\n",
      "norm of grandient =  tensor(0.0214)\n",
      "norm of grandient =  tensor(0.2773)\n",
      "norm of grandient =  tensor(0.0231)\n",
      "[40,     1] loss: 1.655 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1064)\n",
      "norm of grandient =  tensor(6.4686e-07)\n",
      "norm of grandient =  tensor(0.0128)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1910)\n",
      "norm of grandient =  tensor(1.0247e-07)\n",
      "norm of grandient =  tensor(0.0134)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3104)\n",
      "norm of grandient =  tensor(9.3261e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1963)\n",
      "norm of grandient =  tensor(9.7035e-09)\n",
      "norm of grandient =  tensor(0.0521)\n",
      "norm of grandient =  tensor(0.0215)\n",
      "norm of grandient =  tensor(0.2770)\n",
      "norm of grandient =  tensor(0.0231)\n",
      "[41,     1] loss: 1.652 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1058)\n",
      "norm of grandient =  tensor(3.1257e-07)\n",
      "norm of grandient =  tensor(0.0126)\n",
      "norm of grandient =  tensor(0.0089)\n",
      "norm of grandient =  tensor(0.1903)\n",
      "norm of grandient =  tensor(9.7698e-08)\n",
      "norm of grandient =  tensor(0.0133)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3099)\n",
      "norm of grandient =  tensor(7.6231e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1958)\n",
      "norm of grandient =  tensor(1.0309e-08)\n",
      "norm of grandient =  tensor(0.0523)\n",
      "norm of grandient =  tensor(0.0215)\n",
      "norm of grandient =  tensor(0.2767)\n",
      "norm of grandient =  tensor(0.0230)\n",
      "[42,     1] loss: 1.649 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1069)\n",
      "norm of grandient =  tensor(5.4535e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1896)\n",
      "norm of grandient =  tensor(9.9641e-08)\n",
      "norm of grandient =  tensor(0.0132)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3094)\n",
      "norm of grandient =  tensor(8.5396e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1953)\n",
      "norm of grandient =  tensor(8.9470e-09)\n",
      "norm of grandient =  tensor(0.0524)\n",
      "norm of grandient =  tensor(0.0215)\n",
      "norm of grandient =  tensor(0.2764)\n",
      "norm of grandient =  tensor(0.0229)\n",
      "[43,     1] loss: 1.647 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1071)\n",
      "norm of grandient =  tensor(6.5115e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1899)\n",
      "norm of grandient =  tensor(1.0776e-07)\n",
      "norm of grandient =  tensor(0.0131)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3088)\n",
      "norm of grandient =  tensor(7.6469e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1949)\n",
      "norm of grandient =  tensor(7.8323e-09)\n",
      "norm of grandient =  tensor(0.0526)\n",
      "norm of grandient =  tensor(0.0216)\n",
      "norm of grandient =  tensor(0.2761)\n",
      "norm of grandient =  tensor(0.0229)\n",
      "[44,     1] loss: 1.644 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1068)\n",
      "norm of grandient =  tensor(7.1364e-07)\n",
      "norm of grandient =  tensor(0.0124)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1897)\n",
      "norm of grandient =  tensor(1.6713e-07)\n",
      "norm of grandient =  tensor(0.0130)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3083)\n",
      "norm of grandient =  tensor(9.2751e-09)\n",
      "norm of grandient =  tensor(0.0136)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1944)\n",
      "norm of grandient =  tensor(9.2020e-09)\n",
      "norm of grandient =  tensor(0.0527)\n",
      "norm of grandient =  tensor(0.0216)\n",
      "norm of grandient =  tensor(0.2759)\n",
      "norm of grandient =  tensor(0.0228)\n",
      "[45,     1] loss: 1.641 acc: 0.486\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1056)\n",
      "norm of grandient =  tensor(4.5793e-07)\n",
      "norm of grandient =  tensor(0.0123)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.1891)\n",
      "norm of grandient =  tensor(1.5227e-07)\n",
      "norm of grandient =  tensor(0.0129)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3078)\n",
      "norm of grandient =  tensor(7.8105e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1940)\n",
      "norm of grandient =  tensor(9.2287e-09)\n",
      "norm of grandient =  tensor(0.0529)\n",
      "norm of grandient =  tensor(0.0217)\n",
      "norm of grandient =  tensor(0.2756)\n",
      "norm of grandient =  tensor(0.0228)\n",
      "[46,     1] loss: 1.639 acc: 0.486\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1058)\n",
      "norm of grandient =  tensor(6.2379e-07)\n",
      "norm of grandient =  tensor(0.0123)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.1890)\n",
      "norm of grandient =  tensor(1.1456e-07)\n",
      "norm of grandient =  tensor(0.0128)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3073)\n",
      "norm of grandient =  tensor(9.0693e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1935)\n",
      "norm of grandient =  tensor(1.0632e-08)\n",
      "norm of grandient =  tensor(0.0530)\n",
      "norm of grandient =  tensor(0.0217)\n",
      "norm of grandient =  tensor(0.2753)\n",
      "norm of grandient =  tensor(0.0227)\n",
      "[47,     1] loss: 1.636 acc: 0.486\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1051)\n",
      "norm of grandient =  tensor(3.0186e-07)\n",
      "norm of grandient =  tensor(0.0123)\n",
      "norm of grandient =  tensor(0.0091)\n",
      "norm of grandient =  tensor(0.1885)\n",
      "norm of grandient =  tensor(1.1264e-07)\n",
      "norm of grandient =  tensor(0.0127)\n",
      "norm of grandient =  tensor(0.0046)\n",
      "norm of grandient =  tensor(0.3069)\n",
      "norm of grandient =  tensor(8.1616e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1931)\n",
      "norm of grandient =  tensor(9.9014e-09)\n",
      "norm of grandient =  tensor(0.0531)\n",
      "norm of grandient =  tensor(0.0218)\n",
      "norm of grandient =  tensor(0.2751)\n",
      "norm of grandient =  tensor(0.0227)\n",
      "[48,     1] loss: 1.634 acc: 0.486\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1053)\n",
      "norm of grandient =  tensor(9.0017e-07)\n",
      "norm of grandient =  tensor(0.0121)\n",
      "norm of grandient =  tensor(0.0092)\n",
      "norm of grandient =  tensor(0.1882)\n",
      "norm of grandient =  tensor(9.4901e-08)\n",
      "norm of grandient =  tensor(0.0126)\n",
      "norm of grandient =  tensor(0.0045)\n",
      "norm of grandient =  tensor(0.3064)\n",
      "norm of grandient =  tensor(7.8722e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1926)\n",
      "norm of grandient =  tensor(8.3784e-09)\n",
      "norm of grandient =  tensor(0.0533)\n",
      "norm of grandient =  tensor(0.0218)\n",
      "norm of grandient =  tensor(0.2748)\n",
      "norm of grandient =  tensor(0.0226)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49,     1] loss: 1.631 acc: 0.486\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1049)\n",
      "norm of grandient =  tensor(2.3391e-07)\n",
      "norm of grandient =  tensor(0.0122)\n",
      "norm of grandient =  tensor(0.0092)\n",
      "norm of grandient =  tensor(0.1888)\n",
      "norm of grandient =  tensor(1.0572e-07)\n",
      "norm of grandient =  tensor(0.0126)\n",
      "norm of grandient =  tensor(0.0045)\n",
      "norm of grandient =  tensor(0.3059)\n",
      "norm of grandient =  tensor(8.0800e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1922)\n",
      "norm of grandient =  tensor(9.8537e-09)\n",
      "norm of grandient =  tensor(0.0534)\n",
      "norm of grandient =  tensor(0.0218)\n",
      "norm of grandient =  tensor(0.2745)\n",
      "norm of grandient =  tensor(0.0226)\n",
      "[50,     1] loss: 1.628 acc: 0.485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Print Gradient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "norm of grandient =  tensor(0.1043)\n",
      "norm of grandient =  tensor(7.2713e-07)\n",
      "norm of grandient =  tensor(0.0124)\n",
      "norm of grandient =  tensor(0.0090)\n",
      "norm of grandient =  tensor(0.1883)\n",
      "norm of grandient =  tensor(1.5083e-07)\n",
      "norm of grandient =  tensor(0.0125)\n",
      "norm of grandient =  tensor(0.0045)\n",
      "norm of grandient =  tensor(0.3054)\n",
      "norm of grandient =  tensor(8.0326e-09)\n",
      "norm of grandient =  tensor(0.0137)\n",
      "norm of grandient =  tensor(0.0056)\n",
      "norm of grandient =  tensor(0.1918)\n",
      "norm of grandient =  tensor(9.4217e-09)\n",
      "norm of grandient =  tensor(0.0536)\n",
      "norm of grandient =  tensor(0.0219)\n",
      "norm of grandient =  tensor(0.2743)\n",
      "norm of grandient =  tensor(0.0225)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss = loss.item()\n",
    "\n",
    "    acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, 0 + 1, running_loss, acc ))\n",
    "        \n",
    "    print(\"\\n\\n\\n\\n\\nPrint Gradient\\n\\n\\n\\n\\n\")\n",
    "    lst = list(model.parameters())\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "#         print(lst[i].grad.shape)\n",
    "        print( \"norm of grandient = \", torch.norm(lst[i].grad) )\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "838cfc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "tensor([[[-0.0006, -0.0007, -0.0005, -0.0005, -0.0007],\n",
      "         [-0.0005, -0.0006, -0.0006, -0.0006, -0.0008],\n",
      "         [-0.0006, -0.0007, -0.0009, -0.0009, -0.0009],\n",
      "         [-0.0008, -0.0010, -0.0010, -0.0009, -0.0007],\n",
      "         [-0.0009, -0.0009, -0.0008, -0.0006, -0.0004]],\n",
      "\n",
      "        [[-0.0010, -0.0011, -0.0009, -0.0009, -0.0011],\n",
      "         [-0.0009, -0.0009, -0.0009, -0.0010, -0.0011],\n",
      "         [-0.0010, -0.0011, -0.0013, -0.0012, -0.0012],\n",
      "         [-0.0011, -0.0013, -0.0013, -0.0012, -0.0011],\n",
      "         [-0.0012, -0.0012, -0.0012, -0.0010, -0.0008]],\n",
      "\n",
      "        [[-0.0013, -0.0014, -0.0012, -0.0013, -0.0015],\n",
      "         [-0.0013, -0.0013, -0.0013, -0.0014, -0.0016],\n",
      "         [-0.0014, -0.0015, -0.0016, -0.0016, -0.0016],\n",
      "         [-0.0015, -0.0016, -0.0016, -0.0016, -0.0015],\n",
      "         [-0.0015, -0.0015, -0.0014, -0.0013, -0.0011]]])\n",
      "torch.Size([6])\n",
      "tensor(0.0026)\n",
      "torch.Size([10, 6, 5, 5])\n",
      "tensor([[[ 1.3255e-04,  1.3354e-05,  2.8905e-05, -2.1223e-04, -2.1434e-04],\n",
      "         [-2.1801e-04, -2.5700e-04, -2.9606e-04, -4.4883e-04, -3.3695e-04],\n",
      "         [-6.2272e-04, -6.7513e-04, -6.9545e-04, -7.7728e-04, -7.1340e-04],\n",
      "         [-8.2827e-04, -8.9910e-04, -8.7618e-04, -8.9680e-04, -8.1400e-04],\n",
      "         [-8.7943e-04, -8.6256e-04, -6.7355e-04, -6.9507e-04, -6.2300e-04]],\n",
      "\n",
      "        [[-6.3242e-04, -4.6638e-04, -3.5448e-04, -4.3667e-04, -4.9575e-04],\n",
      "         [-5.8961e-04, -4.8185e-04, -4.4298e-04, -4.6435e-04, -5.1405e-04],\n",
      "         [-2.8181e-04, -2.0603e-04, -1.5071e-04, -2.2222e-04, -3.1314e-04],\n",
      "         [-1.1001e-04, -1.9006e-05,  7.5355e-05, -2.8241e-05, -8.7327e-05],\n",
      "         [-7.4391e-05, -3.3771e-05, -5.6362e-07, -9.8115e-05, -1.3786e-04]],\n",
      "\n",
      "        [[ 1.2358e-05, -1.7349e-04, -1.7271e-04, -2.0618e-04, -1.6830e-04],\n",
      "         [-1.2346e-04, -1.2598e-04, -2.0011e-04, -3.0327e-04, -3.2765e-04],\n",
      "         [-5.8658e-04, -5.3941e-04, -7.8350e-04, -6.5599e-04, -4.0349e-04],\n",
      "         [-8.5552e-04, -8.6200e-04, -1.0301e-03, -9.3039e-04, -7.3480e-04],\n",
      "         [-8.8327e-04, -9.1687e-04, -8.6458e-04, -5.9788e-04, -6.3524e-04]],\n",
      "\n",
      "        [[-1.6342e-05, -4.3890e-05, -5.5485e-05, -3.5239e-07, -1.2039e-05],\n",
      "         [ 1.0003e-05,  1.1549e-05,  1.4958e-06,  6.6383e-05,  2.1924e-05],\n",
      "         [ 7.8242e-05,  5.5960e-05, -1.5860e-06,  2.8068e-05, -1.2223e-05],\n",
      "         [-3.1885e-05, -4.1935e-05, -5.1152e-05, -2.4380e-05, -1.0103e-04],\n",
      "         [ 3.4443e-05, -2.3271e-05, -1.0461e-04, -2.0038e-04, -1.2823e-04]],\n",
      "\n",
      "        [[ 2.3712e-04,  1.1813e-04,  2.6825e-05, -8.0951e-05,  3.5508e-05],\n",
      "         [-4.6619e-05,  2.5586e-05, -1.1229e-04, -8.6894e-05, -2.7901e-05],\n",
      "         [-6.4257e-05, -2.6459e-04, -3.3109e-04, -2.5548e-04, -2.2641e-04],\n",
      "         [-3.1601e-04, -3.8732e-04, -3.7929e-04, -3.8869e-04, -3.7489e-04],\n",
      "         [-3.0358e-04, -2.7946e-04, -2.8556e-04, -2.6050e-04, -2.0371e-04]],\n",
      "\n",
      "        [[-5.9771e-04, -6.8271e-04, -6.5972e-04, -5.5353e-04, -4.3967e-04],\n",
      "         [-4.8370e-04, -5.5102e-04, -4.3387e-04, -2.8823e-04, -2.4169e-04],\n",
      "         [-2.0509e-04, -1.5937e-04, -1.7173e-04, -4.8217e-05, -9.8525e-06],\n",
      "         [-1.4883e-04, -4.1937e-05, -1.4015e-04, -3.6510e-05, -3.0617e-05],\n",
      "         [-1.7511e-04, -1.6671e-04, -2.0664e-04, -1.7630e-04, -2.0032e-04]]])\n",
      "torch.Size([10])\n",
      "tensor(-0.0026)\n",
      "torch.Size([80, 250])\n",
      "tensor([-8.8628e-05, -1.2830e-04, -1.3095e-04, -1.2244e-04, -1.2774e-04,\n",
      "        -4.8315e-06, -6.9297e-06, -9.6711e-06, -1.8936e-05, -3.4066e-05,\n",
      "         2.6651e-05,  4.0949e-05,  6.2514e-05,  2.7077e-05,  1.7974e-05,\n",
      "        -9.5224e-05, -9.6981e-05, -2.1783e-05, -2.2716e-05, -5.8576e-05,\n",
      "        -1.2052e-04, -1.7234e-04, -1.1173e-04, -1.0809e-04, -4.9988e-05,\n",
      "        -2.3421e-04, -2.1788e-04, -1.9935e-04, -1.8269e-04, -2.1077e-04,\n",
      "        -2.7713e-04, -1.7360e-04, -1.7966e-04, -1.4518e-04, -2.4436e-04,\n",
      "        -2.2895e-04, -1.9729e-04, -1.2129e-04, -1.1438e-04, -1.8679e-04,\n",
      "        -1.9659e-04, -1.8253e-04, -1.6471e-04, -1.6670e-04, -2.1854e-04,\n",
      "        -2.3074e-04, -2.2490e-04, -1.9238e-04, -2.5630e-04, -2.5619e-04,\n",
      "        -7.9120e-06, -5.4370e-06, -1.2169e-05,  7.4669e-06, -7.3914e-06,\n",
      "         5.2221e-07,  2.4696e-06,  4.4681e-06, -4.5875e-06,  4.9864e-06,\n",
      "        -4.3110e-06, -3.6447e-06,  1.1053e-05,  1.4935e-06, -1.0278e-05,\n",
      "        -2.5057e-05,  2.6621e-08,  1.3900e-06,  1.4229e-06, -2.2199e-06,\n",
      "        -4.0408e-05, -4.0061e-05, -1.8713e-05,  9.7792e-06, -7.7595e-06,\n",
      "         1.4746e-05,  2.4393e-05,  3.1367e-05,  2.1046e-05,  2.6157e-05,\n",
      "        -2.7087e-05,  3.3007e-06,  8.3226e-06,  4.6525e-05,  1.6098e-06,\n",
      "        -2.3933e-05, -1.2014e-05,  2.9336e-05, -2.7052e-05, -3.7184e-06,\n",
      "        -1.1810e-06, -3.2883e-05, -9.9772e-06, -2.6376e-05, -4.6385e-05,\n",
      "         1.5176e-05,  2.6241e-05,  4.3161e-05,  2.5523e-05, -2.0621e-05,\n",
      "        -1.6400e-04, -1.1829e-04, -1.3797e-04, -1.5525e-04, -1.4641e-04,\n",
      "        -6.3510e-05, -1.8453e-05, -5.2216e-05, -1.1289e-04, -1.4294e-04,\n",
      "        -1.6277e-05,  9.1351e-05,  5.3278e-05, -3.4794e-05, -7.1931e-05,\n",
      "        -5.3078e-05,  8.9715e-06, -5.7562e-06,  6.4661e-05, -1.9541e-05,\n",
      "        -1.9480e-04, -1.8549e-04, -1.7092e-04, -8.1656e-05, -3.1453e-05,\n",
      "        -1.5315e-04, -1.1231e-04, -5.7811e-05, -1.3425e-04, -1.4177e-04,\n",
      "        -5.6721e-05, -7.6296e-05, -9.3454e-06, -7.9204e-07, -1.0476e-04,\n",
      "        -5.7800e-05, -3.9223e-05, -3.6400e-06, -5.6095e-05, -8.8709e-05,\n",
      "        -1.5366e-04, -8.2922e-05, -1.1936e-04, -1.0456e-04, -9.0072e-05,\n",
      "        -1.5802e-04, -1.6138e-04, -2.2329e-04, -1.7375e-04, -1.0613e-04,\n",
      "         2.6732e-05,  4.3916e-05,  1.6108e-05,  3.2702e-06,  1.5066e-05,\n",
      "        -4.0421e-05, -5.9679e-05,  2.0491e-06, -3.7980e-06, -8.7043e-06,\n",
      "        -5.0268e-05, -2.0025e-05, -4.7019e-06,  2.3644e-06,  3.0627e-05,\n",
      "         5.6403e-06,  5.2384e-06,  3.2618e-05,  5.4932e-05,  7.4186e-05,\n",
      "        -6.3892e-05, -5.4976e-05,  2.4630e-05, -1.3386e-05,  3.7054e-05,\n",
      "         1.8571e-05,  2.8849e-05,  7.1144e-05,  4.1273e-05,  2.7199e-05,\n",
      "         6.1998e-06, -7.3399e-06,  4.6997e-05,  4.3714e-05,  6.9815e-05,\n",
      "        -7.9570e-05, -1.0674e-04, -9.6150e-05, -4.8239e-05, -1.7426e-06,\n",
      "        -9.1240e-05, -9.7320e-05, -1.1997e-04, -9.6752e-05, -1.0429e-04,\n",
      "        -2.9931e-05, -1.9211e-05, -3.6773e-05,  1.5579e-05, -7.9670e-05,\n",
      "        -1.8798e-04, -1.5706e-04, -2.4435e-04, -2.6621e-04, -2.0604e-04,\n",
      "        -2.8898e-04, -2.3958e-04, -2.1643e-04, -2.3780e-04, -3.0678e-04,\n",
      "        -2.6739e-04, -1.5617e-04, -2.4535e-04, -2.3924e-04, -3.2053e-04,\n",
      "        -2.5885e-04, -2.5111e-04, -2.6532e-04, -2.2773e-04, -2.4949e-04,\n",
      "        -3.3316e-04, -3.0031e-04, -3.0687e-04, -1.7487e-04, -1.2434e-04,\n",
      "        -2.6560e-05, -4.0073e-05,  1.5838e-05, -2.6939e-05, -2.7937e-05,\n",
      "        -7.1562e-06, -1.6067e-05,  3.0995e-05,  1.0775e-05, -7.5649e-06,\n",
      "        -3.5305e-05, -2.0639e-05, -3.1222e-05, -9.4844e-06, -6.6896e-06,\n",
      "        -9.3902e-05, -1.1325e-04, -1.4300e-04, -9.4699e-05, -7.2346e-05,\n",
      "        -2.3106e-05, -5.1821e-05, -9.8624e-05, -5.8260e-05, -4.8048e-05])\n",
      "torch.Size([80])\n",
      "tensor(-0.0001)\n",
      "torch.Size([40, 80])\n",
      "tensor([ 9.7235e-04, -6.6699e-05,  0.0000e+00,  1.6726e-04,  1.9723e-03,\n",
      "         0.0000e+00,  1.0752e-04, -9.6365e-05,  2.4603e-04,  1.7888e-04,\n",
      "        -2.9165e-07, -1.1959e-04, -2.6624e-04,  6.0432e-04, -1.2570e-05,\n",
      "         7.5821e-04,  1.7268e-04, -1.2349e-04, -2.4283e-04,  3.0697e-04,\n",
      "         0.0000e+00,  5.4547e-04,  1.5296e-04,  3.7190e-04,  4.8096e-04,\n",
      "        -1.8664e-08,  9.8708e-04,  2.7064e-04, -3.3960e-05, -1.0654e-04,\n",
      "         7.1469e-04,  2.1456e-05,  7.6744e-04,  5.3294e-04, -2.8262e-04,\n",
      "         6.0556e-04, -2.8897e-04,  1.9630e-04,  1.3850e-04, -3.5375e-05,\n",
      "        -8.0611e-05,  9.3373e-04,  1.5621e-04,  4.3660e-05,  0.0000e+00,\n",
      "         4.9694e-05,  0.0000e+00,  1.0104e-06,  7.5352e-07,  9.9906e-10,\n",
      "         2.3101e-04, -1.6343e-04,  6.4468e-04,  4.2825e-04,  3.8673e-05,\n",
      "        -1.3778e-05, -4.8686e-05,  6.0291e-04,  3.7877e-04,  7.2090e-04,\n",
      "        -9.7999e-05, -3.0480e-06,  1.0654e-03,  3.5732e-04,  0.0000e+00,\n",
      "         1.7416e-05, -8.9374e-06, -4.8435e-05,  7.6867e-04,  1.1214e-03,\n",
      "         9.1433e-04,  3.4548e-04,  2.0865e-06,  7.5180e-04,  2.3749e-04,\n",
      "         1.3599e-05, -1.3126e-05, -8.4105e-05,  0.0000e+00, -5.6577e-06])\n",
      "torch.Size([40])\n",
      "tensor(0.0003)\n",
      "torch.Size([10, 40])\n",
      "tensor([ 1.8686e-03, -4.4229e-05,  0.0000e+00,  4.0702e-04,  2.2796e-03,\n",
      "         2.1996e-03,  1.4533e-03,  1.1991e-03,  0.0000e+00,  3.9865e-05,\n",
      "        -2.4652e-04,  1.5800e-03,  1.2227e-03, -9.7524e-05,  5.8163e-04,\n",
      "         3.2071e-05,  3.3704e-06,  1.9242e-03,  0.0000e+00, -5.9813e-04,\n",
      "         4.6936e-04,  3.3289e-06,  0.0000e+00, -4.5382e-05,  1.9766e-03,\n",
      "         4.8656e-04,  3.9552e-05,  3.3553e-04,  0.0000e+00,  0.0000e+00,\n",
      "         8.6963e-05,  7.8008e-04,  3.2241e-04,  3.9951e-04,  0.0000e+00,\n",
      "         2.6913e-07,  2.5214e-03,  3.8052e-04,  0.0000e+00,  1.4530e-03])\n",
      "torch.Size([10])\n",
      "tensor(0.0087)\n"
     ]
    }
   ],
   "source": [
    "# nvar = 10160\n",
    "# x = .1 * np.ones((nvar,1))\n",
    "# x_torch = torch.from_numpy(x).cuda()\n",
    "# torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    print(lst[i].grad.shape)\n",
    "    print(lst[i].grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caa17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97ca10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "nvar = 10160\n",
    "x = .1 * np.ones((nvar,1))\n",
    "x_torch = torch.from_numpy(x).cuda()\n",
    "torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6395253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0067b6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   196] loss: 1.884\n",
      "[1,   196] loss: 0.000 acc: 0.350\n",
      "[2,   196] loss: 1.874\n",
      "[2,   196] loss: 0.000 acc: 0.362\n",
      "[3,   196] loss: 1.865\n",
      "[3,   196] loss: 0.000 acc: 0.362\n",
      "[4,   196] loss: 1.855\n",
      "[4,   196] loss: 0.000 acc: 0.362\n",
      "[5,   196] loss: 1.847\n",
      "[5,   196] loss: 0.000 acc: 0.375\n",
      "[6,   196] loss: 1.838\n",
      "[6,   196] loss: 0.000 acc: 0.362\n",
      "[7,   196] loss: 1.831\n",
      "[7,   196] loss: 0.000 acc: 0.362\n",
      "[8,   196] loss: 1.823\n",
      "[8,   196] loss: 0.000 acc: 0.375\n",
      "[9,   196] loss: 1.816\n",
      "[9,   196] loss: 0.000 acc: 0.388\n",
      "[10,   196] loss: 1.809\n",
      "[10,   196] loss: 0.000 acc: 0.388\n",
      "[11,   196] loss: 1.802\n",
      "[11,   196] loss: 0.000 acc: 0.388\n",
      "[12,   196] loss: 1.796\n",
      "[12,   196] loss: 0.000 acc: 0.388\n",
      "[13,   196] loss: 1.790\n",
      "[13,   196] loss: 0.000 acc: 0.388\n",
      "[14,   196] loss: 1.784\n",
      "[14,   196] loss: 0.000 acc: 0.375\n",
      "[15,   196] loss: 1.779\n",
      "[15,   196] loss: 0.000 acc: 0.375\n",
      "[16,   196] loss: 1.773\n",
      "[16,   196] loss: 0.000 acc: 0.375\n",
      "[17,   196] loss: 1.768\n",
      "[17,   196] loss: 0.000 acc: 0.375\n",
      "[18,   196] loss: 1.763\n",
      "[18,   196] loss: 0.000 acc: 0.375\n",
      "[19,   196] loss: 1.758\n",
      "[19,   196] loss: 0.000 acc: 0.375\n",
      "[20,   196] loss: 1.754\n",
      "[20,   196] loss: 0.000 acc: 0.375\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 196 == 195:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 196))\n",
    "            running_loss = 0.0\n",
    "            acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "            print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2f7bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2.150 acc: 0.215\n",
      "[1,     2] loss: 2.125 acc: 0.270\n",
      "[1,     3] loss: 2.120 acc: 0.266\n",
      "[1,     4] loss: 2.166 acc: 0.219\n",
      "[1,     5] loss: 2.107 acc: 0.273\n",
      "[1,     6] loss: 2.129 acc: 0.277\n",
      "[1,     7] loss: 2.136 acc: 0.273\n",
      "[1,     8] loss: 2.190 acc: 0.238\n",
      "[1,     9] loss: 2.154 acc: 0.234\n",
      "[1,    10] loss: 2.116 acc: 0.281\n",
      "[1,    11] loss: 2.122 acc: 0.246\n",
      "[1,    12] loss: 2.132 acc: 0.262\n",
      "[1,    13] loss: 2.145 acc: 0.227\n",
      "[1,    14] loss: 2.131 acc: 0.273\n",
      "[1,    15] loss: 2.127 acc: 0.266\n",
      "[1,    16] loss: 2.158 acc: 0.254\n",
      "[1,    17] loss: 2.153 acc: 0.238\n",
      "[1,    18] loss: 2.161 acc: 0.207\n",
      "[1,    19] loss: 2.094 acc: 0.316\n",
      "[1,    20] loss: 2.151 acc: 0.211\n",
      "[1,    21] loss: 2.138 acc: 0.246\n",
      "[1,    22] loss: 2.127 acc: 0.289\n",
      "[1,    23] loss: 2.113 acc: 0.266\n",
      "[1,    24] loss: 2.124 acc: 0.227\n",
      "[1,    25] loss: 2.133 acc: 0.262\n",
      "[1,    26] loss: 2.161 acc: 0.180\n",
      "[1,    27] loss: 2.172 acc: 0.152\n",
      "[1,    28] loss: 2.132 acc: 0.254\n",
      "[1,    29] loss: 2.140 acc: 0.234\n",
      "[1,    30] loss: 2.147 acc: 0.254\n",
      "[1,    31] loss: 2.129 acc: 0.242\n",
      "[1,    32] loss: 2.109 acc: 0.254\n",
      "[1,    33] loss: 2.143 acc: 0.258\n",
      "[1,    34] loss: 2.142 acc: 0.258\n",
      "[1,    35] loss: 2.160 acc: 0.227\n",
      "[1,    36] loss: 2.137 acc: 0.207\n",
      "[1,    37] loss: 2.133 acc: 0.273\n",
      "[1,    38] loss: 2.127 acc: 0.227\n",
      "[1,    39] loss: 2.142 acc: 0.230\n",
      "[1,    40] loss: 2.148 acc: 0.270\n",
      "[1,    41] loss: 2.144 acc: 0.230\n",
      "[1,    42] loss: 2.119 acc: 0.246\n",
      "[1,    43] loss: 2.121 acc: 0.254\n",
      "[1,    44] loss: 2.118 acc: 0.254\n",
      "[1,    45] loss: 2.164 acc: 0.207\n",
      "[1,    46] loss: 2.096 acc: 0.285\n",
      "[1,    47] loss: 2.153 acc: 0.227\n",
      "[1,    48] loss: 2.122 acc: 0.254\n",
      "[1,    49] loss: 2.135 acc: 0.230\n",
      "[1,    50] loss: 2.129 acc: 0.234\n",
      "[1,    51] loss: 2.156 acc: 0.250\n",
      "[1,    52] loss: 2.137 acc: 0.234\n",
      "[1,    53] loss: 2.123 acc: 0.289\n",
      "[1,    54] loss: 2.096 acc: 0.238\n",
      "[1,    55] loss: 2.138 acc: 0.227\n",
      "[1,    56] loss: 2.153 acc: 0.215\n",
      "[1,    57] loss: 2.135 acc: 0.238\n",
      "[1,    58] loss: 2.128 acc: 0.273\n",
      "[1,    59] loss: 2.134 acc: 0.250\n",
      "[1,    60] loss: 2.125 acc: 0.227\n",
      "[1,    61] loss: 2.149 acc: 0.262\n",
      "[1,    62] loss: 2.127 acc: 0.250\n",
      "[1,    63] loss: 2.137 acc: 0.215\n",
      "[1,    64] loss: 2.137 acc: 0.234\n",
      "[1,    65] loss: 2.129 acc: 0.246\n",
      "[1,    66] loss: 2.160 acc: 0.227\n",
      "[1,    67] loss: 2.137 acc: 0.242\n",
      "[1,    68] loss: 2.150 acc: 0.234\n",
      "[1,    69] loss: 2.161 acc: 0.215\n",
      "[1,    70] loss: 2.130 acc: 0.246\n",
      "[1,    71] loss: 2.125 acc: 0.238\n",
      "[1,    72] loss: 2.123 acc: 0.238\n",
      "[1,    73] loss: 2.144 acc: 0.262\n",
      "[1,    74] loss: 2.108 acc: 0.312\n",
      "[1,    75] loss: 2.142 acc: 0.270\n",
      "[1,    76] loss: 2.152 acc: 0.207\n",
      "[1,    77] loss: 2.097 acc: 0.273\n",
      "[1,    78] loss: 2.134 acc: 0.223\n",
      "[1,    79] loss: 2.136 acc: 0.273\n",
      "[1,    80] loss: 2.107 acc: 0.254\n",
      "[1,    81] loss: 2.122 acc: 0.273\n",
      "[1,    82] loss: 2.107 acc: 0.258\n",
      "[1,    83] loss: 2.126 acc: 0.254\n",
      "[1,    84] loss: 2.094 acc: 0.262\n",
      "[1,    85] loss: 2.093 acc: 0.301\n",
      "[1,    86] loss: 2.128 acc: 0.230\n",
      "[1,    87] loss: 2.133 acc: 0.223\n",
      "[1,    88] loss: 2.114 acc: 0.238\n",
      "[1,    89] loss: 2.113 acc: 0.246\n",
      "[1,    90] loss: 2.130 acc: 0.234\n",
      "[1,    91] loss: 2.117 acc: 0.250\n",
      "[1,    92] loss: 2.095 acc: 0.297\n",
      "[1,    93] loss: 2.089 acc: 0.262\n",
      "[1,    94] loss: 2.099 acc: 0.258\n",
      "[1,    95] loss: 2.081 acc: 0.266\n",
      "[1,    96] loss: 2.097 acc: 0.285\n",
      "[1,    97] loss: 2.108 acc: 0.277\n",
      "[1,    98] loss: 2.137 acc: 0.254\n",
      "[1,    99] loss: 2.093 acc: 0.281\n",
      "[1,   100] loss: 2.129 acc: 0.270\n",
      "[1,   101] loss: 2.095 acc: 0.312\n",
      "[1,   102] loss: 2.095 acc: 0.250\n",
      "[1,   103] loss: 2.107 acc: 0.242\n",
      "[1,   104] loss: 2.118 acc: 0.246\n",
      "[1,   105] loss: 2.086 acc: 0.320\n",
      "[1,   106] loss: 2.134 acc: 0.215\n",
      "[1,   107] loss: 2.133 acc: 0.234\n",
      "[1,   108] loss: 2.132 acc: 0.258\n",
      "[1,   109] loss: 2.125 acc: 0.262\n",
      "[1,   110] loss: 2.119 acc: 0.258\n",
      "[1,   111] loss: 2.131 acc: 0.254\n",
      "[1,   112] loss: 2.130 acc: 0.184\n",
      "[1,   113] loss: 2.104 acc: 0.258\n",
      "[1,   114] loss: 2.127 acc: 0.219\n",
      "[1,   115] loss: 2.118 acc: 0.234\n",
      "[1,   116] loss: 2.125 acc: 0.215\n",
      "[1,   117] loss: 2.095 acc: 0.242\n",
      "[1,   118] loss: 2.121 acc: 0.254\n",
      "[1,   119] loss: 2.096 acc: 0.281\n",
      "[1,   120] loss: 2.130 acc: 0.266\n",
      "[1,   121] loss: 2.113 acc: 0.258\n",
      "[1,   122] loss: 2.100 acc: 0.285\n",
      "[1,   123] loss: 2.133 acc: 0.258\n",
      "[1,   124] loss: 2.101 acc: 0.254\n",
      "[1,   125] loss: 2.127 acc: 0.289\n",
      "[1,   126] loss: 2.095 acc: 0.262\n",
      "[1,   127] loss: 2.104 acc: 0.246\n",
      "[1,   128] loss: 2.096 acc: 0.301\n",
      "[1,   129] loss: 2.109 acc: 0.227\n",
      "[1,   130] loss: 2.096 acc: 0.285\n",
      "[1,   131] loss: 2.132 acc: 0.215\n",
      "[1,   132] loss: 2.127 acc: 0.207\n",
      "[1,   133] loss: 2.110 acc: 0.234\n",
      "[1,   134] loss: 2.074 acc: 0.270\n",
      "[1,   135] loss: 2.101 acc: 0.246\n",
      "[1,   136] loss: 2.108 acc: 0.238\n",
      "[1,   137] loss: 2.128 acc: 0.230\n",
      "[1,   138] loss: 2.108 acc: 0.246\n",
      "[1,   139] loss: 2.086 acc: 0.285\n",
      "[1,   140] loss: 2.103 acc: 0.242\n",
      "[1,   141] loss: 2.117 acc: 0.297\n",
      "[1,   142] loss: 2.098 acc: 0.273\n",
      "[1,   143] loss: 2.084 acc: 0.285\n",
      "[1,   144] loss: 2.081 acc: 0.277\n",
      "[1,   145] loss: 2.112 acc: 0.297\n",
      "[1,   146] loss: 2.132 acc: 0.250\n",
      "[1,   147] loss: 2.118 acc: 0.242\n",
      "[1,   148] loss: 2.111 acc: 0.262\n",
      "[1,   149] loss: 2.147 acc: 0.242\n",
      "[1,   150] loss: 2.101 acc: 0.277\n",
      "[1,   151] loss: 2.085 acc: 0.281\n",
      "[1,   152] loss: 2.084 acc: 0.309\n",
      "[1,   153] loss: 2.080 acc: 0.297\n",
      "[1,   154] loss: 2.123 acc: 0.223\n",
      "[1,   155] loss: 2.103 acc: 0.238\n",
      "[1,   156] loss: 2.135 acc: 0.246\n",
      "[1,   157] loss: 2.098 acc: 0.281\n",
      "[1,   158] loss: 2.113 acc: 0.262\n",
      "[1,   159] loss: 2.045 acc: 0.309\n",
      "[1,   160] loss: 2.132 acc: 0.289\n",
      "[1,   161] loss: 2.134 acc: 0.234\n",
      "[1,   162] loss: 2.092 acc: 0.289\n",
      "[1,   163] loss: 2.132 acc: 0.281\n",
      "[1,   164] loss: 2.086 acc: 0.270\n",
      "[1,   165] loss: 2.098 acc: 0.266\n",
      "[1,   166] loss: 2.068 acc: 0.301\n",
      "[1,   167] loss: 2.133 acc: 0.254\n",
      "[1,   168] loss: 2.109 acc: 0.234\n",
      "[1,   169] loss: 2.107 acc: 0.238\n",
      "[1,   170] loss: 2.105 acc: 0.270\n",
      "[1,   171] loss: 2.092 acc: 0.230\n",
      "[1,   172] loss: 2.061 acc: 0.297\n",
      "[1,   173] loss: 2.107 acc: 0.223\n",
      "[1,   174] loss: 2.096 acc: 0.289\n",
      "[1,   175] loss: 2.081 acc: 0.293\n",
      "[1,   176] loss: 2.086 acc: 0.246\n",
      "[1,   177] loss: 2.087 acc: 0.281\n",
      "[1,   178] loss: 2.122 acc: 0.227\n",
      "[1,   179] loss: 2.108 acc: 0.250\n",
      "[1,   180] loss: 2.077 acc: 0.250\n",
      "[1,   181] loss: 2.081 acc: 0.277\n",
      "[1,   182] loss: 2.116 acc: 0.227\n",
      "[1,   183] loss: 2.108 acc: 0.273\n",
      "[1,   184] loss: 2.079 acc: 0.270\n",
      "[1,   185] loss: 2.068 acc: 0.262\n",
      "[1,   186] loss: 2.094 acc: 0.305\n",
      "[1,   187] loss: 2.082 acc: 0.301\n",
      "[1,   188] loss: 2.131 acc: 0.250\n",
      "[1,   189] loss: 2.071 acc: 0.285\n",
      "[1,   190] loss: 2.083 acc: 0.293\n",
      "[1,   191] loss: 2.084 acc: 0.277\n",
      "[1,   192] loss: 2.091 acc: 0.277\n",
      "[1,   193] loss: 2.055 acc: 0.297\n",
      "[1,   194] loss: 2.100 acc: 0.227\n",
      "[1,   195] loss: 2.093 acc: 0.262\n",
      "[1,   196] loss: 2.154 acc: 0.250\n",
      "[2,     1] loss: 2.090 acc: 0.238\n",
      "[2,     2] loss: 2.071 acc: 0.285\n",
      "[2,     3] loss: 2.062 acc: 0.309\n",
      "[2,     4] loss: 2.110 acc: 0.238\n",
      "[2,     5] loss: 2.043 acc: 0.301\n",
      "[2,     6] loss: 2.066 acc: 0.266\n",
      "[2,     7] loss: 2.079 acc: 0.270\n",
      "[2,     8] loss: 2.149 acc: 0.250\n",
      "[2,     9] loss: 2.097 acc: 0.258\n",
      "[2,    10] loss: 2.060 acc: 0.301\n",
      "[2,    11] loss: 2.069 acc: 0.270\n",
      "[2,    12] loss: 2.070 acc: 0.297\n",
      "[2,    13] loss: 2.094 acc: 0.227\n",
      "[2,    14] loss: 2.081 acc: 0.281\n",
      "[2,    15] loss: 2.069 acc: 0.297\n",
      "[2,    16] loss: 2.110 acc: 0.254\n",
      "[2,    17] loss: 2.098 acc: 0.234\n",
      "[2,    18] loss: 2.112 acc: 0.219\n",
      "[2,    19] loss: 2.034 acc: 0.340\n",
      "[2,    20] loss: 2.097 acc: 0.250\n",
      "[2,    21] loss: 2.089 acc: 0.293\n",
      "[2,    22] loss: 2.068 acc: 0.305\n",
      "[2,    23] loss: 2.054 acc: 0.254\n",
      "[2,    24] loss: 2.069 acc: 0.273\n",
      "[2,    25] loss: 2.077 acc: 0.289\n",
      "[2,    26] loss: 2.104 acc: 0.199\n",
      "[2,    27] loss: 2.126 acc: 0.184\n",
      "[2,    28] loss: 2.072 acc: 0.270\n",
      "[2,    29] loss: 2.081 acc: 0.234\n",
      "[2,    30] loss: 2.104 acc: 0.266\n",
      "[2,    31] loss: 2.075 acc: 0.270\n",
      "[2,    32] loss: 2.051 acc: 0.250\n",
      "[2,    33] loss: 2.093 acc: 0.281\n",
      "[2,    34] loss: 2.089 acc: 0.285\n",
      "[2,    35] loss: 2.116 acc: 0.242\n",
      "[2,    36] loss: 2.089 acc: 0.219\n",
      "[2,    37] loss: 2.081 acc: 0.281\n",
      "[2,    38] loss: 2.074 acc: 0.250\n",
      "[2,    39] loss: 2.084 acc: 0.289\n",
      "[2,    40] loss: 2.105 acc: 0.281\n",
      "[2,    41] loss: 2.095 acc: 0.258\n",
      "[2,    42] loss: 2.065 acc: 0.266\n",
      "[2,    43] loss: 2.064 acc: 0.285\n",
      "[2,    44] loss: 2.060 acc: 0.297\n",
      "[2,    45] loss: 2.116 acc: 0.242\n",
      "[2,    46] loss: 2.033 acc: 0.301\n",
      "[2,    47] loss: 2.104 acc: 0.258\n",
      "[2,    48] loss: 2.072 acc: 0.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,    49] loss: 2.086 acc: 0.281\n",
      "[2,    50] loss: 2.075 acc: 0.250\n",
      "[2,    51] loss: 2.107 acc: 0.250\n",
      "[2,    52] loss: 2.089 acc: 0.250\n",
      "[2,    53] loss: 2.076 acc: 0.328\n",
      "[2,    54] loss: 2.041 acc: 0.281\n",
      "[2,    55] loss: 2.090 acc: 0.254\n",
      "[2,    56] loss: 2.103 acc: 0.234\n",
      "[2,    57] loss: 2.091 acc: 0.258\n",
      "[2,    58] loss: 2.080 acc: 0.281\n",
      "[2,    59] loss: 2.089 acc: 0.270\n",
      "[2,    60] loss: 2.077 acc: 0.266\n",
      "[2,    61] loss: 2.104 acc: 0.266\n",
      "[2,    62] loss: 2.074 acc: 0.293\n",
      "[2,    63] loss: 2.088 acc: 0.219\n",
      "[2,    64] loss: 2.092 acc: 0.227\n",
      "[2,    65] loss: 2.082 acc: 0.293\n",
      "[2,    66] loss: 2.125 acc: 0.238\n",
      "[2,    67] loss: 2.086 acc: 0.250\n",
      "[2,    68] loss: 2.105 acc: 0.258\n",
      "[2,    69] loss: 2.113 acc: 0.254\n",
      "[2,    70] loss: 2.080 acc: 0.266\n",
      "[2,    71] loss: 2.068 acc: 0.281\n",
      "[2,    72] loss: 2.069 acc: 0.266\n",
      "[2,    73] loss: 2.097 acc: 0.242\n",
      "[2,    74] loss: 2.056 acc: 0.312\n",
      "[2,    75] loss: 2.095 acc: 0.285\n",
      "[2,    76] loss: 2.113 acc: 0.219\n",
      "[2,    77] loss: 2.041 acc: 0.297\n",
      "[2,    78] loss: 2.083 acc: 0.254\n",
      "[2,    79] loss: 2.087 acc: 0.297\n",
      "[2,    80] loss: 2.056 acc: 0.242\n",
      "[2,    81] loss: 2.081 acc: 0.277\n",
      "[2,    82] loss: 2.051 acc: 0.258\n",
      "[2,    83] loss: 2.079 acc: 0.270\n",
      "[2,    84] loss: 2.043 acc: 0.316\n",
      "[2,    85] loss: 2.035 acc: 0.297\n",
      "[2,    86] loss: 2.083 acc: 0.270\n",
      "[2,    87] loss: 2.088 acc: 0.230\n",
      "[2,    88] loss: 2.058 acc: 0.262\n",
      "[2,    89] loss: 2.054 acc: 0.277\n",
      "[2,    90] loss: 2.083 acc: 0.258\n",
      "[2,    91] loss: 2.068 acc: 0.273\n",
      "[2,    92] loss: 2.037 acc: 0.309\n",
      "[2,    93] loss: 2.031 acc: 0.309\n",
      "[2,    94] loss: 2.047 acc: 0.285\n",
      "[2,    95] loss: 2.023 acc: 0.289\n",
      "[2,    96] loss: 2.042 acc: 0.297\n",
      "[2,    97] loss: 2.058 acc: 0.281\n",
      "[2,    98] loss: 2.092 acc: 0.285\n",
      "[2,    99] loss: 2.037 acc: 0.301\n",
      "[2,   100] loss: 2.075 acc: 0.270\n",
      "[2,   101] loss: 2.040 acc: 0.320\n",
      "[2,   102] loss: 2.032 acc: 0.277\n",
      "[2,   103] loss: 2.052 acc: 0.270\n",
      "[2,   104] loss: 2.075 acc: 0.270\n",
      "[2,   105] loss: 2.035 acc: 0.320\n",
      "[2,   106] loss: 2.088 acc: 0.242\n",
      "[2,   107] loss: 2.088 acc: 0.270\n",
      "[2,   108] loss: 2.083 acc: 0.246\n",
      "[2,   109] loss: 2.076 acc: 0.258\n",
      "[2,   110] loss: 2.073 acc: 0.273\n",
      "[2,   111] loss: 2.083 acc: 0.258\n",
      "[2,   112] loss: 2.083 acc: 0.219\n",
      "[2,   113] loss: 2.046 acc: 0.285\n",
      "[2,   114] loss: 2.086 acc: 0.254\n",
      "[2,   115] loss: 2.066 acc: 0.270\n",
      "[2,   116] loss: 2.080 acc: 0.227\n",
      "[2,   117] loss: 2.041 acc: 0.234\n",
      "[2,   118] loss: 2.070 acc: 0.273\n",
      "[2,   119] loss: 2.047 acc: 0.320\n",
      "[2,   120] loss: 2.092 acc: 0.293\n",
      "[2,   121] loss: 2.064 acc: 0.273\n",
      "[2,   122] loss: 2.060 acc: 0.309\n",
      "[2,   123] loss: 2.078 acc: 0.289\n",
      "[2,   124] loss: 2.049 acc: 0.289\n",
      "[2,   125] loss: 2.080 acc: 0.297\n",
      "[2,   126] loss: 2.053 acc: 0.301\n",
      "[2,   127] loss: 2.057 acc: 0.262\n",
      "[2,   128] loss: 2.047 acc: 0.301\n",
      "[2,   129] loss: 2.060 acc: 0.246\n",
      "[2,   130] loss: 2.043 acc: 0.289\n",
      "[2,   131] loss: 2.090 acc: 0.250\n",
      "[2,   132] loss: 2.080 acc: 0.215\n",
      "[2,   133] loss: 2.065 acc: 0.258\n",
      "[2,   134] loss: 2.021 acc: 0.285\n",
      "[2,   135] loss: 2.056 acc: 0.262\n",
      "[2,   136] loss: 2.061 acc: 0.258\n",
      "[2,   137] loss: 2.085 acc: 0.246\n",
      "[2,   138] loss: 2.063 acc: 0.258\n",
      "[2,   139] loss: 2.036 acc: 0.297\n",
      "[2,   140] loss: 2.056 acc: 0.273\n",
      "[2,   141] loss: 2.076 acc: 0.289\n",
      "[2,   142] loss: 2.046 acc: 0.281\n",
      "[2,   143] loss: 2.024 acc: 0.293\n",
      "[2,   144] loss: 2.030 acc: 0.328\n",
      "[2,   145] loss: 2.070 acc: 0.301\n",
      "[2,   146] loss: 2.091 acc: 0.246\n",
      "[2,   147] loss: 2.077 acc: 0.266\n",
      "[2,   148] loss: 2.068 acc: 0.270\n",
      "[2,   149] loss: 2.114 acc: 0.266\n",
      "[2,   150] loss: 2.052 acc: 0.293\n",
      "[2,   151] loss: 2.037 acc: 0.277\n",
      "[2,   152] loss: 2.030 acc: 0.324\n",
      "[2,   153] loss: 2.025 acc: 0.332\n",
      "[2,   154] loss: 2.080 acc: 0.227\n",
      "[2,   155] loss: 2.066 acc: 0.242\n",
      "[2,   156] loss: 2.099 acc: 0.254\n",
      "[2,   157] loss: 2.055 acc: 0.312\n",
      "[2,   158] loss: 2.076 acc: 0.281\n",
      "[2,   159] loss: 1.988 acc: 0.340\n",
      "[2,   160] loss: 2.101 acc: 0.281\n",
      "[2,   161] loss: 2.094 acc: 0.250\n",
      "[2,   162] loss: 2.044 acc: 0.285\n",
      "[2,   163] loss: 2.088 acc: 0.273\n",
      "[2,   164] loss: 2.040 acc: 0.258\n",
      "[2,   165] loss: 2.055 acc: 0.273\n",
      "[2,   166] loss: 2.016 acc: 0.316\n",
      "[2,   167] loss: 2.096 acc: 0.270\n",
      "[2,   168] loss: 2.068 acc: 0.238\n",
      "[2,   169] loss: 2.066 acc: 0.238\n",
      "[2,   170] loss: 2.058 acc: 0.262\n",
      "[2,   171] loss: 2.040 acc: 0.250\n",
      "[2,   172] loss: 2.016 acc: 0.281\n",
      "[2,   173] loss: 2.061 acc: 0.250\n",
      "[2,   174] loss: 2.055 acc: 0.312\n",
      "[2,   175] loss: 2.046 acc: 0.281\n",
      "[2,   176] loss: 2.044 acc: 0.266\n",
      "[2,   177] loss: 2.045 acc: 0.293\n",
      "[2,   178] loss: 2.090 acc: 0.230\n",
      "[2,   179] loss: 2.060 acc: 0.297\n",
      "[2,   180] loss: 2.036 acc: 0.250\n",
      "[2,   181] loss: 2.031 acc: 0.297\n",
      "[2,   182] loss: 2.076 acc: 0.230\n",
      "[2,   183] loss: 2.067 acc: 0.301\n",
      "[2,   184] loss: 2.047 acc: 0.273\n",
      "[2,   185] loss: 2.018 acc: 0.289\n",
      "[2,   186] loss: 2.049 acc: 0.309\n",
      "[2,   187] loss: 2.040 acc: 0.324\n",
      "[2,   188] loss: 2.101 acc: 0.254\n",
      "[2,   189] loss: 2.028 acc: 0.316\n",
      "[2,   190] loss: 2.039 acc: 0.289\n",
      "[2,   191] loss: 2.039 acc: 0.270\n",
      "[2,   192] loss: 2.044 acc: 0.293\n",
      "[2,   193] loss: 2.005 acc: 0.324\n",
      "[2,   194] loss: 2.064 acc: 0.262\n",
      "[2,   195] loss: 2.052 acc: 0.273\n",
      "[2,   196] loss: 2.131 acc: 0.250\n",
      "[3,     1] loss: 2.041 acc: 0.266\n",
      "[3,     2] loss: 2.029 acc: 0.297\n",
      "[3,     3] loss: 2.019 acc: 0.312\n",
      "[3,     4] loss: 2.063 acc: 0.266\n",
      "[3,     5] loss: 1.995 acc: 0.301\n",
      "[3,     6] loss: 2.017 acc: 0.285\n",
      "[3,     7] loss: 2.034 acc: 0.281\n",
      "[3,     8] loss: 2.118 acc: 0.250\n",
      "[3,     9] loss: 2.053 acc: 0.281\n",
      "[3,    10] loss: 2.017 acc: 0.309\n",
      "[3,    11] loss: 2.029 acc: 0.301\n",
      "[3,    12] loss: 2.023 acc: 0.301\n",
      "[3,    13] loss: 2.055 acc: 0.230\n",
      "[3,    14] loss: 2.043 acc: 0.289\n",
      "[3,    15] loss: 2.025 acc: 0.305\n",
      "[3,    16] loss: 2.075 acc: 0.273\n",
      "[3,    17] loss: 2.055 acc: 0.262\n",
      "[3,    18] loss: 2.074 acc: 0.223\n",
      "[3,    19] loss: 1.991 acc: 0.344\n",
      "[3,    20] loss: 2.057 acc: 0.281\n",
      "[3,    21] loss: 2.050 acc: 0.293\n",
      "[3,    22] loss: 2.023 acc: 0.324\n",
      "[3,    23] loss: 2.009 acc: 0.277\n",
      "[3,    24] loss: 2.027 acc: 0.277\n",
      "[3,    25] loss: 2.031 acc: 0.285\n",
      "[3,    26] loss: 2.059 acc: 0.227\n",
      "[3,    27] loss: 2.089 acc: 0.207\n",
      "[3,    28] loss: 2.025 acc: 0.285\n",
      "[3,    29] loss: 2.035 acc: 0.238\n",
      "[3,    30] loss: 2.072 acc: 0.277\n",
      "[3,    31] loss: 2.034 acc: 0.277\n",
      "[3,    32] loss: 2.008 acc: 0.266\n",
      "[3,    33] loss: 2.056 acc: 0.289\n",
      "[3,    34] loss: 2.047 acc: 0.281\n",
      "[3,    35] loss: 2.083 acc: 0.258\n",
      "[3,    36] loss: 2.054 acc: 0.234\n",
      "[3,    37] loss: 2.043 acc: 0.266\n",
      "[3,    38] loss: 2.035 acc: 0.250\n",
      "[3,    39] loss: 2.041 acc: 0.309\n",
      "[3,    40] loss: 2.075 acc: 0.301\n",
      "[3,    41] loss: 2.057 acc: 0.273\n",
      "[3,    42] loss: 2.026 acc: 0.273\n",
      "[3,    43] loss: 2.021 acc: 0.293\n",
      "[3,    44] loss: 2.017 acc: 0.320\n",
      "[3,    45] loss: 2.078 acc: 0.242\n",
      "[3,    46] loss: 1.986 acc: 0.320\n",
      "[3,    47] loss: 2.067 acc: 0.270\n",
      "[3,    48] loss: 2.036 acc: 0.266\n",
      "[3,    49] loss: 2.049 acc: 0.289\n",
      "[3,    50] loss: 2.033 acc: 0.262\n",
      "[3,    51] loss: 2.067 acc: 0.266\n",
      "[3,    52] loss: 2.054 acc: 0.258\n",
      "[3,    53] loss: 2.040 acc: 0.328\n",
      "[3,    54] loss: 2.003 acc: 0.289\n",
      "[3,    55] loss: 2.054 acc: 0.270\n",
      "[3,    56] loss: 2.064 acc: 0.238\n",
      "[3,    57] loss: 2.059 acc: 0.246\n",
      "[3,    58] loss: 2.047 acc: 0.270\n",
      "[3,    59] loss: 2.061 acc: 0.277\n",
      "[3,    60] loss: 2.042 acc: 0.285\n",
      "[3,    61] loss: 2.073 acc: 0.281\n",
      "[3,    62] loss: 2.033 acc: 0.301\n",
      "[3,    63] loss: 2.053 acc: 0.203\n",
      "[3,    64] loss: 2.058 acc: 0.234\n",
      "[3,    65] loss: 2.050 acc: 0.297\n",
      "[3,    66] loss: 2.100 acc: 0.230\n",
      "[3,    67] loss: 2.048 acc: 0.250\n",
      "[3,    68] loss: 2.069 acc: 0.270\n",
      "[3,    69] loss: 2.074 acc: 0.273\n",
      "[3,    70] loss: 2.042 acc: 0.285\n",
      "[3,    71] loss: 2.026 acc: 0.316\n",
      "[3,    72] loss: 2.030 acc: 0.285\n",
      "[3,    73] loss: 2.063 acc: 0.238\n",
      "[3,    74] loss: 2.016 acc: 0.320\n",
      "[3,    75] loss: 2.060 acc: 0.320\n",
      "[3,    76] loss: 2.083 acc: 0.227\n",
      "[3,    77] loss: 2.001 acc: 0.316\n",
      "[3,    78] loss: 2.045 acc: 0.262\n",
      "[3,    79] loss: 2.050 acc: 0.297\n",
      "[3,    80] loss: 2.019 acc: 0.266\n",
      "[3,    81] loss: 2.052 acc: 0.285\n",
      "[3,    82] loss: 2.008 acc: 0.285\n",
      "[3,    83] loss: 2.043 acc: 0.277\n",
      "[3,    84] loss: 2.005 acc: 0.324\n",
      "[3,    85] loss: 1.989 acc: 0.312\n",
      "[3,    86] loss: 2.050 acc: 0.281\n",
      "[3,    87] loss: 2.056 acc: 0.250\n",
      "[3,    88] loss: 2.015 acc: 0.277\n",
      "[3,    89] loss: 2.009 acc: 0.301\n",
      "[3,    90] loss: 2.047 acc: 0.277\n",
      "[3,    91] loss: 2.030 acc: 0.270\n",
      "[3,    92] loss: 1.993 acc: 0.324\n",
      "[3,    93] loss: 1.988 acc: 0.316\n",
      "[3,    94] loss: 2.008 acc: 0.277\n",
      "[3,    95] loss: 1.980 acc: 0.324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,    96] loss: 1.998 acc: 0.309\n",
      "[3,    97] loss: 2.021 acc: 0.301\n",
      "[3,    98] loss: 2.058 acc: 0.281\n",
      "[3,    99] loss: 1.992 acc: 0.309\n",
      "[3,   100] loss: 2.033 acc: 0.285\n",
      "[3,   101] loss: 2.003 acc: 0.328\n",
      "[3,   102] loss: 1.981 acc: 0.320\n",
      "[3,   103] loss: 2.008 acc: 0.281\n",
      "[3,   104] loss: 2.045 acc: 0.297\n",
      "[3,   105] loss: 1.995 acc: 0.316\n",
      "[3,   106] loss: 2.055 acc: 0.262\n",
      "[3,   107] loss: 2.056 acc: 0.281\n",
      "[3,   108] loss: 2.045 acc: 0.273\n",
      "[3,   109] loss: 2.041 acc: 0.254\n",
      "[3,   110] loss: 2.039 acc: 0.301\n",
      "[3,   111] loss: 2.048 acc: 0.273\n",
      "[3,   112] loss: 2.050 acc: 0.211\n",
      "[3,   113] loss: 2.000 acc: 0.301\n",
      "[3,   114] loss: 2.054 acc: 0.258\n",
      "[3,   115] loss: 2.028 acc: 0.270\n",
      "[3,   116] loss: 2.046 acc: 0.238\n",
      "[3,   117] loss: 2.001 acc: 0.266\n",
      "[3,   118] loss: 2.031 acc: 0.285\n",
      "[3,   119] loss: 2.010 acc: 0.312\n",
      "[3,   120] loss: 2.065 acc: 0.301\n",
      "[3,   121] loss: 2.027 acc: 0.285\n",
      "[3,   122] loss: 2.031 acc: 0.289\n",
      "[3,   123] loss: 2.035 acc: 0.305\n",
      "[3,   124] loss: 2.009 acc: 0.285\n",
      "[3,   125] loss: 2.046 acc: 0.297\n",
      "[3,   126] loss: 2.022 acc: 0.297\n",
      "[3,   127] loss: 2.024 acc: 0.270\n",
      "[3,   128] loss: 2.011 acc: 0.305\n",
      "[3,   129] loss: 2.025 acc: 0.250\n",
      "[3,   130] loss: 2.003 acc: 0.301\n",
      "[3,   131] loss: 2.061 acc: 0.258\n",
      "[3,   132] loss: 2.047 acc: 0.246\n",
      "[3,   133] loss: 2.033 acc: 0.270\n",
      "[3,   134] loss: 1.981 acc: 0.293\n",
      "[3,   135] loss: 2.024 acc: 0.262\n",
      "[3,   136] loss: 2.028 acc: 0.277\n",
      "[3,   137] loss: 2.053 acc: 0.246\n",
      "[3,   138] loss: 2.031 acc: 0.250\n",
      "[3,   139] loss: 1.997 acc: 0.320\n",
      "[3,   140] loss: 2.021 acc: 0.270\n",
      "[3,   141] loss: 2.046 acc: 0.285\n",
      "[3,   142] loss: 2.006 acc: 0.297\n",
      "[3,   143] loss: 1.979 acc: 0.324\n",
      "[3,   144] loss: 1.991 acc: 0.324\n",
      "[3,   145] loss: 2.042 acc: 0.289\n",
      "[3,   146] loss: 2.060 acc: 0.270\n",
      "[3,   147] loss: 2.046 acc: 0.277\n",
      "[3,   148] loss: 2.036 acc: 0.270\n",
      "[3,   149] loss: 2.093 acc: 0.254\n",
      "[3,   150] loss: 2.016 acc: 0.293\n",
      "[3,   151] loss: 2.001 acc: 0.277\n",
      "[3,   152] loss: 1.990 acc: 0.340\n",
      "[3,   153] loss: 1.986 acc: 0.316\n",
      "[3,   154] loss: 2.047 acc: 0.230\n",
      "[3,   155] loss: 2.041 acc: 0.266\n",
      "[3,   156] loss: 2.072 acc: 0.254\n",
      "[3,   157] loss: 2.023 acc: 0.336\n",
      "[3,   158] loss: 2.050 acc: 0.281\n",
      "[3,   159] loss: 1.947 acc: 0.375\n",
      "[3,   160] loss: 2.081 acc: 0.281\n",
      "[3,   161] loss: 2.064 acc: 0.273\n",
      "[3,   162] loss: 2.008 acc: 0.277\n",
      "[3,   163] loss: 2.055 acc: 0.270\n",
      "[3,   164] loss: 2.006 acc: 0.277\n",
      "[3,   165] loss: 2.024 acc: 0.270\n",
      "[3,   166] loss: 1.976 acc: 0.320\n",
      "[3,   167] loss: 2.071 acc: 0.277\n",
      "[3,   168] loss: 2.040 acc: 0.238\n",
      "[3,   169] loss: 2.035 acc: 0.254\n",
      "[3,   170] loss: 2.022 acc: 0.277\n",
      "[3,   171] loss: 2.003 acc: 0.234\n",
      "[3,   172] loss: 1.986 acc: 0.293\n",
      "[3,   173] loss: 2.025 acc: 0.273\n",
      "[3,   174] loss: 2.024 acc: 0.324\n",
      "[3,   175] loss: 2.022 acc: 0.285\n",
      "[3,   176] loss: 2.012 acc: 0.281\n",
      "[3,   177] loss: 2.014 acc: 0.293\n",
      "[3,   178] loss: 2.067 acc: 0.227\n",
      "[3,   179] loss: 2.023 acc: 0.301\n",
      "[3,   180] loss: 2.009 acc: 0.270\n",
      "[3,   181] loss: 1.992 acc: 0.289\n",
      "[3,   182] loss: 2.047 acc: 0.227\n",
      "[3,   183] loss: 2.034 acc: 0.281\n",
      "[3,   184] loss: 2.027 acc: 0.273\n",
      "[3,   185] loss: 1.981 acc: 0.301\n",
      "[3,   186] loss: 2.015 acc: 0.336\n",
      "[3,   187] loss: 2.009 acc: 0.301\n",
      "[3,   188] loss: 2.080 acc: 0.258\n",
      "[3,   189] loss: 1.997 acc: 0.297\n",
      "[3,   190] loss: 2.007 acc: 0.316\n",
      "[3,   191] loss: 2.008 acc: 0.281\n",
      "[3,   192] loss: 2.008 acc: 0.289\n",
      "[3,   193] loss: 1.968 acc: 0.320\n",
      "[3,   194] loss: 2.038 acc: 0.281\n",
      "[3,   195] loss: 2.020 acc: 0.277\n",
      "[3,   196] loss: 2.116 acc: 0.263\n",
      "[4,     1] loss: 2.004 acc: 0.258\n",
      "[4,     2] loss: 1.998 acc: 0.320\n",
      "[4,     3] loss: 1.988 acc: 0.320\n",
      "[4,     4] loss: 2.028 acc: 0.270\n",
      "[4,     5] loss: 1.961 acc: 0.316\n",
      "[4,     6] loss: 1.979 acc: 0.293\n",
      "[4,     7] loss: 1.999 acc: 0.281\n",
      "[4,     8] loss: 2.094 acc: 0.254\n",
      "[4,     9] loss: 2.022 acc: 0.289\n",
      "[4,    10] loss: 1.986 acc: 0.320\n",
      "[4,    11] loss: 2.000 acc: 0.285\n",
      "[4,    12] loss: 1.987 acc: 0.305\n",
      "[4,    13] loss: 2.026 acc: 0.238\n",
      "[4,    14] loss: 2.015 acc: 0.293\n",
      "[4,    15] loss: 1.992 acc: 0.312\n",
      "[4,    16] loss: 2.050 acc: 0.266\n",
      "[4,    17] loss: 2.023 acc: 0.273\n",
      "[4,    18] loss: 2.044 acc: 0.246\n",
      "[4,    19] loss: 1.961 acc: 0.348\n",
      "[4,    20] loss: 2.028 acc: 0.285\n",
      "[4,    21] loss: 2.021 acc: 0.312\n",
      "[4,    22] loss: 1.992 acc: 0.320\n",
      "[4,    23] loss: 1.974 acc: 0.273\n",
      "[4,    24] loss: 1.995 acc: 0.301\n",
      "[4,    25] loss: 1.996 acc: 0.297\n",
      "[4,    26] loss: 2.025 acc: 0.246\n",
      "[4,    27] loss: 2.062 acc: 0.219\n",
      "[4,    28] loss: 1.988 acc: 0.293\n",
      "[4,    29] loss: 2.000 acc: 0.250\n",
      "[4,    30] loss: 2.047 acc: 0.285\n",
      "[4,    31] loss: 2.003 acc: 0.285\n",
      "[4,    32] loss: 1.977 acc: 0.281\n",
      "[4,    33] loss: 2.030 acc: 0.293\n",
      "[4,    34] loss: 2.016 acc: 0.293\n",
      "[4,    35] loss: 2.059 acc: 0.281\n",
      "[4,    36] loss: 2.031 acc: 0.250\n",
      "[4,    37] loss: 2.017 acc: 0.289\n",
      "[4,    38] loss: 2.006 acc: 0.254\n",
      "[4,    39] loss: 2.010 acc: 0.312\n",
      "[4,    40] loss: 2.056 acc: 0.289\n",
      "[4,    41] loss: 2.029 acc: 0.277\n",
      "[4,    42] loss: 1.998 acc: 0.285\n",
      "[4,    43] loss: 1.989 acc: 0.293\n",
      "[4,    44] loss: 1.988 acc: 0.316\n",
      "[4,    45] loss: 2.048 acc: 0.230\n",
      "[4,    46] loss: 1.951 acc: 0.328\n",
      "[4,    47] loss: 2.041 acc: 0.285\n",
      "[4,    48] loss: 2.008 acc: 0.277\n",
      "[4,    49] loss: 2.025 acc: 0.293\n",
      "[4,    50] loss: 2.002 acc: 0.277\n",
      "[4,    51] loss: 2.035 acc: 0.277\n",
      "[4,    52] loss: 2.031 acc: 0.254\n",
      "[4,    53] loss: 2.013 acc: 0.320\n",
      "[4,    54] loss: 1.976 acc: 0.293\n",
      "[4,    55] loss: 2.026 acc: 0.289\n",
      "[4,    56] loss: 2.034 acc: 0.246\n",
      "[4,    57] loss: 2.037 acc: 0.250\n",
      "[4,    58] loss: 2.023 acc: 0.293\n",
      "[4,    59] loss: 2.043 acc: 0.277\n",
      "[4,    60] loss: 2.017 acc: 0.285\n",
      "[4,    61] loss: 2.052 acc: 0.281\n",
      "[4,    62] loss: 2.001 acc: 0.316\n",
      "[4,    63] loss: 2.028 acc: 0.215\n",
      "[4,    64] loss: 2.033 acc: 0.242\n",
      "[4,    65] loss: 2.030 acc: 0.297\n",
      "[4,    66] loss: 2.083 acc: 0.242\n",
      "[4,    67] loss: 2.018 acc: 0.262\n",
      "[4,    68] loss: 2.041 acc: 0.289\n",
      "[4,    69] loss: 2.043 acc: 0.293\n",
      "[4,    70] loss: 2.016 acc: 0.293\n",
      "[4,    71] loss: 1.996 acc: 0.320\n",
      "[4,    72] loss: 2.001 acc: 0.297\n",
      "[4,    73] loss: 2.038 acc: 0.238\n",
      "[4,    74] loss: 1.987 acc: 0.320\n",
      "[4,    75] loss: 2.033 acc: 0.301\n",
      "[4,    76] loss: 2.060 acc: 0.227\n",
      "[4,    77] loss: 1.973 acc: 0.316\n",
      "[4,    78] loss: 2.016 acc: 0.285\n",
      "[4,    79] loss: 2.022 acc: 0.312\n",
      "[4,    80] loss: 1.992 acc: 0.273\n",
      "[4,    81] loss: 2.031 acc: 0.289\n",
      "[4,    82] loss: 1.977 acc: 0.293\n",
      "[4,    83] loss: 2.016 acc: 0.270\n",
      "[4,    84] loss: 1.976 acc: 0.344\n",
      "[4,    85] loss: 1.956 acc: 0.324\n",
      "[4,    86] loss: 2.026 acc: 0.305\n",
      "[4,    87] loss: 2.033 acc: 0.254\n",
      "[4,    88] loss: 1.981 acc: 0.297\n",
      "[4,    89] loss: 1.975 acc: 0.305\n",
      "[4,    90] loss: 2.020 acc: 0.277\n",
      "[4,    91] loss: 2.001 acc: 0.289\n",
      "[4,    92] loss: 1.960 acc: 0.320\n",
      "[4,    93] loss: 1.957 acc: 0.328\n",
      "[4,    94] loss: 1.980 acc: 0.293\n",
      "[4,    95] loss: 1.948 acc: 0.332\n",
      "[4,    96] loss: 1.963 acc: 0.328\n",
      "[4,    97] loss: 1.993 acc: 0.289\n",
      "[4,    98] loss: 2.031 acc: 0.281\n",
      "[4,    99] loss: 1.958 acc: 0.320\n",
      "[4,   100] loss: 2.002 acc: 0.285\n",
      "[4,   101] loss: 1.978 acc: 0.324\n",
      "[4,   102] loss: 1.942 acc: 0.328\n",
      "[4,   103] loss: 1.973 acc: 0.301\n",
      "[4,   104] loss: 2.025 acc: 0.289\n",
      "[4,   105] loss: 1.965 acc: 0.328\n",
      "[4,   106] loss: 2.032 acc: 0.281\n",
      "[4,   107] loss: 2.033 acc: 0.305\n",
      "[4,   108] loss: 2.016 acc: 0.270\n",
      "[4,   109] loss: 2.017 acc: 0.262\n",
      "[4,   110] loss: 2.015 acc: 0.301\n",
      "[4,   111] loss: 2.022 acc: 0.277\n",
      "[4,   112] loss: 2.025 acc: 0.230\n",
      "[4,   113] loss: 1.965 acc: 0.301\n",
      "[4,   114] loss: 2.030 acc: 0.258\n",
      "[4,   115] loss: 1.999 acc: 0.273\n",
      "[4,   116] loss: 2.020 acc: 0.254\n",
      "[4,   117] loss: 1.970 acc: 0.270\n",
      "[4,   118] loss: 2.003 acc: 0.289\n",
      "[4,   119] loss: 1.981 acc: 0.305\n",
      "[4,   120] loss: 2.045 acc: 0.285\n",
      "[4,   121] loss: 1.999 acc: 0.305\n",
      "[4,   122] loss: 2.010 acc: 0.293\n",
      "[4,   123] loss: 2.002 acc: 0.316\n",
      "[4,   124] loss: 1.978 acc: 0.289\n",
      "[4,   125] loss: 2.020 acc: 0.297\n",
      "[4,   126] loss: 2.000 acc: 0.301\n",
      "[4,   127] loss: 1.999 acc: 0.277\n",
      "[4,   128] loss: 1.984 acc: 0.312\n",
      "[4,   129] loss: 2.000 acc: 0.266\n",
      "[4,   130] loss: 1.974 acc: 0.309\n",
      "[4,   131] loss: 2.041 acc: 0.270\n",
      "[4,   132] loss: 2.025 acc: 0.254\n",
      "[4,   133] loss: 2.009 acc: 0.266\n",
      "[4,   134] loss: 1.951 acc: 0.293\n",
      "[4,   135] loss: 2.001 acc: 0.262\n",
      "[4,   136] loss: 2.006 acc: 0.297\n",
      "[4,   137] loss: 2.030 acc: 0.238\n",
      "[4,   138] loss: 2.010 acc: 0.262\n",
      "[4,   139] loss: 1.967 acc: 0.324\n",
      "[4,   140] loss: 1.994 acc: 0.270\n",
      "[4,   141] loss: 2.024 acc: 0.285\n",
      "[4,   142] loss: 1.975 acc: 0.301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   143] loss: 1.945 acc: 0.336\n",
      "[4,   144] loss: 1.963 acc: 0.324\n",
      "[4,   145] loss: 2.023 acc: 0.301\n",
      "[4,   146] loss: 2.035 acc: 0.273\n",
      "[4,   147] loss: 2.023 acc: 0.273\n",
      "[4,   148] loss: 2.012 acc: 0.285\n",
      "[4,   149] loss: 2.078 acc: 0.250\n",
      "[4,   150] loss: 1.989 acc: 0.312\n",
      "[4,   151] loss: 1.973 acc: 0.293\n",
      "[4,   152] loss: 1.960 acc: 0.348\n",
      "[4,   153] loss: 1.957 acc: 0.320\n",
      "[4,   154] loss: 2.022 acc: 0.230\n",
      "[4,   155] loss: 2.024 acc: 0.273\n",
      "[4,   156] loss: 2.053 acc: 0.262\n",
      "[4,   157] loss: 1.999 acc: 0.352\n",
      "[4,   158] loss: 2.030 acc: 0.293\n",
      "[4,   159] loss: 1.916 acc: 0.371\n",
      "[4,   160] loss: 2.067 acc: 0.277\n",
      "[4,   161] loss: 2.041 acc: 0.297\n",
      "[4,   162] loss: 1.981 acc: 0.289\n",
      "[4,   163] loss: 2.029 acc: 0.285\n",
      "[4,   164] loss: 1.980 acc: 0.293\n",
      "[4,   165] loss: 2.002 acc: 0.277\n",
      "[4,   166] loss: 1.945 acc: 0.320\n",
      "[4,   167] loss: 2.053 acc: 0.285\n",
      "[4,   168] loss: 2.020 acc: 0.258\n",
      "[4,   169] loss: 2.012 acc: 0.266\n",
      "[4,   170] loss: 1.996 acc: 0.266\n",
      "[4,   171] loss: 1.976 acc: 0.246\n",
      "[4,   172] loss: 1.965 acc: 0.301\n",
      "[4,   173] loss: 1.998 acc: 0.273\n",
      "[4,   174] loss: 2.000 acc: 0.328\n",
      "[4,   175] loss: 2.006 acc: 0.281\n",
      "[4,   176] loss: 1.988 acc: 0.289\n",
      "[4,   177] loss: 1.991 acc: 0.320\n",
      "[4,   178] loss: 2.051 acc: 0.242\n",
      "[4,   179] loss: 1.994 acc: 0.312\n",
      "[4,   180] loss: 1.989 acc: 0.277\n",
      "[4,   181] loss: 1.962 acc: 0.305\n",
      "[4,   182] loss: 2.027 acc: 0.242\n",
      "[4,   183] loss: 2.009 acc: 0.293\n",
      "[4,   184] loss: 2.014 acc: 0.289\n",
      "[4,   185] loss: 1.952 acc: 0.324\n",
      "[4,   186] loss: 1.988 acc: 0.328\n",
      "[4,   187] loss: 1.985 acc: 0.301\n",
      "[4,   188] loss: 2.064 acc: 0.258\n",
      "[4,   189] loss: 1.974 acc: 0.301\n",
      "[4,   190] loss: 1.984 acc: 0.316\n",
      "[4,   191] loss: 1.985 acc: 0.289\n",
      "[4,   192] loss: 1.982 acc: 0.301\n",
      "[4,   193] loss: 1.941 acc: 0.324\n",
      "[4,   194] loss: 2.020 acc: 0.293\n",
      "[4,   195] loss: 1.995 acc: 0.285\n",
      "[4,   196] loss: 2.106 acc: 0.287\n",
      "[5,     1] loss: 1.977 acc: 0.262\n",
      "[5,     2] loss: 1.974 acc: 0.332\n",
      "[5,     3] loss: 1.966 acc: 0.320\n",
      "[5,     4] loss: 2.002 acc: 0.281\n",
      "[5,     5] loss: 1.937 acc: 0.320\n",
      "[5,     6] loss: 1.951 acc: 0.305\n",
      "[5,     7] loss: 1.971 acc: 0.277\n",
      "[5,     8] loss: 2.076 acc: 0.270\n",
      "[5,     9] loss: 1.999 acc: 0.293\n",
      "[5,    10] loss: 1.964 acc: 0.320\n",
      "[5,    11] loss: 1.978 acc: 0.297\n",
      "[5,    12] loss: 1.961 acc: 0.305\n",
      "[5,    13] loss: 2.004 acc: 0.246\n",
      "[5,    14] loss: 1.993 acc: 0.312\n",
      "[5,    15] loss: 1.967 acc: 0.320\n",
      "[5,    16] loss: 2.032 acc: 0.270\n",
      "[5,    17] loss: 1.998 acc: 0.270\n",
      "[5,    18] loss: 2.021 acc: 0.242\n",
      "[5,    19] loss: 1.940 acc: 0.344\n",
      "[5,    20] loss: 2.008 acc: 0.297\n",
      "[5,    21] loss: 1.999 acc: 0.324\n",
      "[5,    22] loss: 1.968 acc: 0.328\n",
      "[5,    23] loss: 1.947 acc: 0.262\n",
      "[5,    24] loss: 1.971 acc: 0.328\n",
      "[5,    25] loss: 1.969 acc: 0.285\n",
      "[5,    26] loss: 2.000 acc: 0.258\n",
      "[5,    27] loss: 2.040 acc: 0.227\n",
      "[5,    28] loss: 1.960 acc: 0.293\n",
      "[5,    29] loss: 1.974 acc: 0.281\n",
      "[5,    30] loss: 2.028 acc: 0.301\n",
      "[5,    31] loss: 1.979 acc: 0.273\n",
      "[5,    32] loss: 1.954 acc: 0.277\n",
      "[5,    33] loss: 2.011 acc: 0.293\n",
      "[5,    34] loss: 1.992 acc: 0.293\n",
      "[5,    35] loss: 2.041 acc: 0.281\n",
      "[5,    36] loss: 2.013 acc: 0.254\n",
      "[5,    37] loss: 1.999 acc: 0.301\n",
      "[5,    38] loss: 1.984 acc: 0.262\n",
      "[5,    39] loss: 1.989 acc: 0.324\n",
      "[5,    40] loss: 2.043 acc: 0.301\n",
      "[5,    41] loss: 2.008 acc: 0.301\n",
      "[5,    42] loss: 1.978 acc: 0.309\n",
      "[5,    43] loss: 1.964 acc: 0.312\n",
      "[5,    44] loss: 1.966 acc: 0.336\n",
      "[5,    45] loss: 2.024 acc: 0.254\n",
      "[5,    46] loss: 1.925 acc: 0.328\n",
      "[5,    47] loss: 2.022 acc: 0.281\n",
      "[5,    48] loss: 1.987 acc: 0.285\n",
      "[5,    49] loss: 2.007 acc: 0.305\n",
      "[5,    50] loss: 1.979 acc: 0.281\n",
      "[5,    51] loss: 2.010 acc: 0.289\n",
      "[5,    52] loss: 2.015 acc: 0.262\n",
      "[5,    53] loss: 1.992 acc: 0.324\n",
      "[5,    54] loss: 1.957 acc: 0.293\n",
      "[5,    55] loss: 2.003 acc: 0.293\n",
      "[5,    56] loss: 2.010 acc: 0.246\n",
      "[5,    57] loss: 2.019 acc: 0.258\n",
      "[5,    58] loss: 2.005 acc: 0.285\n",
      "[5,    59] loss: 2.031 acc: 0.285\n",
      "[5,    60] loss: 1.998 acc: 0.297\n",
      "[5,    61] loss: 2.036 acc: 0.273\n",
      "[5,    62] loss: 1.976 acc: 0.312\n",
      "[5,    63] loss: 2.009 acc: 0.203\n",
      "[5,    64] loss: 2.013 acc: 0.238\n",
      "[5,    65] loss: 2.015 acc: 0.305\n",
      "[5,    66] loss: 2.069 acc: 0.254\n",
      "[5,    67] loss: 1.995 acc: 0.285\n",
      "[5,    68] loss: 2.019 acc: 0.297\n",
      "[5,    69] loss: 2.017 acc: 0.309\n",
      "[5,    70] loss: 1.995 acc: 0.297\n",
      "[5,    71] loss: 1.974 acc: 0.336\n",
      "[5,    72] loss: 1.979 acc: 0.316\n",
      "[5,    73] loss: 2.019 acc: 0.250\n",
      "[5,    74] loss: 1.964 acc: 0.324\n",
      "[5,    75] loss: 2.012 acc: 0.316\n",
      "[5,    76] loss: 2.041 acc: 0.250\n",
      "[5,    77] loss: 1.953 acc: 0.297\n",
      "[5,    78] loss: 1.994 acc: 0.301\n",
      "[5,    79] loss: 2.000 acc: 0.324\n",
      "[5,    80] loss: 1.974 acc: 0.266\n",
      "[5,    81] loss: 2.015 acc: 0.289\n",
      "[5,    82] loss: 1.952 acc: 0.305\n",
      "[5,    83] loss: 1.994 acc: 0.285\n",
      "[5,    84] loss: 1.953 acc: 0.359\n",
      "[5,    85] loss: 1.931 acc: 0.312\n",
      "[5,    86] loss: 2.009 acc: 0.316\n",
      "[5,    87] loss: 2.016 acc: 0.258\n",
      "[5,    88] loss: 1.954 acc: 0.289\n",
      "[5,    89] loss: 1.949 acc: 0.312\n",
      "[5,    90] loss: 1.999 acc: 0.277\n",
      "[5,    91] loss: 1.979 acc: 0.297\n",
      "[5,    92] loss: 1.935 acc: 0.320\n",
      "[5,    93] loss: 1.934 acc: 0.336\n",
      "[5,    94] loss: 1.958 acc: 0.312\n",
      "[5,    95] loss: 1.923 acc: 0.355\n",
      "[5,    96] loss: 1.936 acc: 0.344\n",
      "[5,    97] loss: 1.972 acc: 0.297\n",
      "[5,    98] loss: 2.008 acc: 0.293\n",
      "[5,    99] loss: 1.932 acc: 0.320\n",
      "[5,   100] loss: 1.979 acc: 0.316\n",
      "[5,   101] loss: 1.962 acc: 0.336\n",
      "[5,   102] loss: 1.910 acc: 0.336\n",
      "[5,   103] loss: 1.945 acc: 0.305\n",
      "[5,   104] loss: 2.010 acc: 0.289\n",
      "[5,   105] loss: 1.941 acc: 0.324\n",
      "[5,   106] loss: 2.015 acc: 0.293\n",
      "[5,   107] loss: 2.016 acc: 0.312\n",
      "[5,   108] loss: 1.994 acc: 0.285\n",
      "[5,   109] loss: 1.998 acc: 0.258\n",
      "[5,   110] loss: 1.998 acc: 0.305\n",
      "[5,   111] loss: 2.001 acc: 0.281\n",
      "[5,   112] loss: 2.006 acc: 0.223\n",
      "[5,   113] loss: 1.938 acc: 0.309\n",
      "[5,   114] loss: 2.010 acc: 0.266\n",
      "[5,   115] loss: 1.977 acc: 0.285\n",
      "[5,   116] loss: 2.000 acc: 0.254\n",
      "[5,   117] loss: 1.947 acc: 0.289\n",
      "[5,   118] loss: 1.982 acc: 0.301\n",
      "[5,   119] loss: 1.959 acc: 0.309\n",
      "[5,   120] loss: 2.030 acc: 0.297\n",
      "[5,   121] loss: 1.977 acc: 0.316\n",
      "[5,   122] loss: 1.993 acc: 0.293\n",
      "[5,   123] loss: 1.976 acc: 0.320\n",
      "[5,   124] loss: 1.954 acc: 0.297\n",
      "[5,   125] loss: 2.000 acc: 0.301\n",
      "[5,   126] loss: 1.984 acc: 0.297\n",
      "[5,   127] loss: 1.980 acc: 0.285\n",
      "[5,   128] loss: 1.964 acc: 0.328\n",
      "[5,   129] loss: 1.982 acc: 0.266\n",
      "[5,   130] loss: 1.952 acc: 0.320\n",
      "[5,   131] loss: 2.024 acc: 0.262\n",
      "[5,   132] loss: 2.009 acc: 0.270\n",
      "[5,   133] loss: 1.990 acc: 0.273\n",
      "[5,   134] loss: 1.926 acc: 0.297\n",
      "[5,   135] loss: 1.984 acc: 0.285\n",
      "[5,   136] loss: 1.989 acc: 0.305\n",
      "[5,   137] loss: 2.012 acc: 0.238\n",
      "[5,   138] loss: 1.993 acc: 0.285\n",
      "[5,   139] loss: 1.943 acc: 0.324\n",
      "[5,   140] loss: 1.973 acc: 0.273\n",
      "[5,   141] loss: 2.005 acc: 0.297\n",
      "[5,   142] loss: 1.950 acc: 0.312\n",
      "[5,   143] loss: 1.919 acc: 0.344\n",
      "[5,   144] loss: 1.940 acc: 0.332\n",
      "[5,   145] loss: 2.009 acc: 0.293\n",
      "[5,   146] loss: 2.016 acc: 0.266\n",
      "[5,   147] loss: 2.004 acc: 0.277\n",
      "[5,   148] loss: 1.994 acc: 0.297\n",
      "[5,   149] loss: 2.067 acc: 0.234\n",
      "[5,   150] loss: 1.969 acc: 0.332\n",
      "[5,   151] loss: 1.951 acc: 0.309\n",
      "[5,   152] loss: 1.937 acc: 0.367\n",
      "[5,   153] loss: 1.934 acc: 0.324\n",
      "[5,   154] loss: 2.002 acc: 0.246\n",
      "[5,   155] loss: 2.010 acc: 0.289\n",
      "[5,   156] loss: 2.039 acc: 0.266\n",
      "[5,   157] loss: 1.980 acc: 0.348\n",
      "[5,   158] loss: 2.014 acc: 0.297\n",
      "[5,   159] loss: 1.892 acc: 0.367\n",
      "[5,   160] loss: 2.056 acc: 0.289\n",
      "[5,   161] loss: 2.023 acc: 0.297\n",
      "[5,   162] loss: 1.960 acc: 0.289\n",
      "[5,   163] loss: 2.008 acc: 0.285\n",
      "[5,   164] loss: 1.959 acc: 0.301\n",
      "[5,   165] loss: 1.986 acc: 0.301\n",
      "[5,   166] loss: 1.920 acc: 0.328\n",
      "[5,   167] loss: 2.040 acc: 0.297\n",
      "[5,   168] loss: 2.003 acc: 0.254\n",
      "[5,   169] loss: 1.993 acc: 0.281\n",
      "[5,   170] loss: 1.975 acc: 0.270\n",
      "[5,   171] loss: 1.956 acc: 0.258\n",
      "[5,   172] loss: 1.949 acc: 0.324\n",
      "[5,   173] loss: 1.978 acc: 0.293\n",
      "[5,   174] loss: 1.981 acc: 0.320\n",
      "[5,   175] loss: 1.993 acc: 0.301\n",
      "[5,   176] loss: 1.968 acc: 0.285\n",
      "[5,   177] loss: 1.973 acc: 0.316\n",
      "[5,   178] loss: 2.037 acc: 0.266\n",
      "[5,   179] loss: 1.971 acc: 0.305\n",
      "[5,   180] loss: 1.975 acc: 0.289\n",
      "[5,   181] loss: 1.938 acc: 0.305\n",
      "[5,   182] loss: 2.011 acc: 0.246\n",
      "[5,   183] loss: 1.989 acc: 0.301\n",
      "[5,   184] loss: 2.005 acc: 0.301\n",
      "[5,   185] loss: 1.930 acc: 0.340\n",
      "[5,   186] loss: 1.967 acc: 0.340\n",
      "[5,   187] loss: 1.966 acc: 0.297\n",
      "[5,   188] loss: 2.052 acc: 0.266\n",
      "[5,   189] loss: 1.956 acc: 0.316\n",
      "[5,   190] loss: 1.967 acc: 0.320\n",
      "[5,   191] loss: 1.968 acc: 0.285\n",
      "[5,   192] loss: 1.962 acc: 0.301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,   193] loss: 1.919 acc: 0.359\n",
      "[5,   194] loss: 2.005 acc: 0.293\n",
      "[5,   195] loss: 1.974 acc: 0.293\n",
      "[5,   196] loss: 2.097 acc: 0.300\n",
      "[6,     1] loss: 1.956 acc: 0.281\n",
      "[6,     2] loss: 1.954 acc: 0.332\n",
      "[6,     3] loss: 1.948 acc: 0.332\n",
      "[6,     4] loss: 1.981 acc: 0.301\n",
      "[6,     5] loss: 1.920 acc: 0.332\n",
      "[6,     6] loss: 1.928 acc: 0.316\n",
      "[6,     7] loss: 1.948 acc: 0.293\n",
      "[6,     8] loss: 2.060 acc: 0.262\n",
      "[6,     9] loss: 1.980 acc: 0.301\n",
      "[6,    10] loss: 1.946 acc: 0.320\n",
      "[6,    11] loss: 1.960 acc: 0.312\n",
      "[6,    12] loss: 1.940 acc: 0.297\n",
      "[6,    13] loss: 1.986 acc: 0.258\n",
      "[6,    14] loss: 1.976 acc: 0.312\n",
      "[6,    15] loss: 1.947 acc: 0.328\n",
      "[6,    16] loss: 2.019 acc: 0.297\n",
      "[6,    17] loss: 1.978 acc: 0.277\n",
      "[6,    18] loss: 2.002 acc: 0.262\n",
      "[6,    19] loss: 1.923 acc: 0.340\n",
      "[6,    20] loss: 1.993 acc: 0.289\n",
      "[6,    21] loss: 1.982 acc: 0.328\n",
      "[6,    22] loss: 1.950 acc: 0.340\n",
      "[6,    23] loss: 1.925 acc: 0.266\n",
      "[6,    24] loss: 1.951 acc: 0.324\n",
      "[6,    25] loss: 1.947 acc: 0.289\n",
      "[6,    26] loss: 1.979 acc: 0.266\n",
      "[6,    27] loss: 2.021 acc: 0.230\n",
      "[6,    28] loss: 1.938 acc: 0.301\n",
      "[6,    29] loss: 1.953 acc: 0.293\n",
      "[6,    30] loss: 2.012 acc: 0.312\n",
      "[6,    31] loss: 1.960 acc: 0.273\n",
      "[6,    32] loss: 1.935 acc: 0.285\n",
      "[6,    33] loss: 1.996 acc: 0.293\n",
      "[6,    34] loss: 1.972 acc: 0.305\n",
      "[6,    35] loss: 2.027 acc: 0.285\n",
      "[6,    36] loss: 2.000 acc: 0.262\n",
      "[6,    37] loss: 1.985 acc: 0.297\n",
      "[6,    38] loss: 1.966 acc: 0.258\n",
      "[6,    39] loss: 1.973 acc: 0.340\n",
      "[6,    40] loss: 2.033 acc: 0.328\n",
      "[6,    41] loss: 1.991 acc: 0.316\n",
      "[6,    42] loss: 1.961 acc: 0.305\n",
      "[6,    43] loss: 1.945 acc: 0.320\n",
      "[6,    44] loss: 1.950 acc: 0.340\n",
      "[6,    45] loss: 2.004 acc: 0.242\n",
      "[6,    46] loss: 1.903 acc: 0.363\n",
      "[6,    47] loss: 2.008 acc: 0.281\n",
      "[6,    48] loss: 1.969 acc: 0.297\n",
      "[6,    49] loss: 1.993 acc: 0.320\n",
      "[6,    50] loss: 1.962 acc: 0.289\n",
      "[6,    51] loss: 1.989 acc: 0.289\n",
      "[6,    52] loss: 2.003 acc: 0.266\n",
      "[6,    53] loss: 1.975 acc: 0.332\n",
      "[6,    54] loss: 1.940 acc: 0.293\n",
      "[6,    55] loss: 1.984 acc: 0.312\n",
      "[6,    56] loss: 1.991 acc: 0.270\n",
      "[6,    57] loss: 2.004 acc: 0.262\n",
      "[6,    58] loss: 1.990 acc: 0.293\n",
      "[6,    59] loss: 2.022 acc: 0.297\n",
      "[6,    60] loss: 1.983 acc: 0.312\n",
      "[6,    61] loss: 2.023 acc: 0.273\n",
      "[6,    62] loss: 1.955 acc: 0.312\n",
      "[6,    63] loss: 1.994 acc: 0.223\n",
      "[6,    64] loss: 1.996 acc: 0.242\n",
      "[6,    65] loss: 2.003 acc: 0.301\n",
      "[6,    66] loss: 2.058 acc: 0.273\n",
      "[6,    67] loss: 1.975 acc: 0.293\n",
      "[6,    68] loss: 2.002 acc: 0.301\n",
      "[6,    69] loss: 1.995 acc: 0.312\n",
      "[6,    70] loss: 1.979 acc: 0.293\n",
      "[6,    71] loss: 1.956 acc: 0.332\n",
      "[6,    72] loss: 1.961 acc: 0.328\n",
      "[6,    73] loss: 2.004 acc: 0.250\n",
      "[6,    74] loss: 1.946 acc: 0.328\n",
      "[6,    75] loss: 1.994 acc: 0.320\n",
      "[6,    76] loss: 2.024 acc: 0.262\n",
      "[6,    77] loss: 1.937 acc: 0.305\n",
      "[6,    78] loss: 1.975 acc: 0.316\n",
      "[6,    79] loss: 1.982 acc: 0.320\n",
      "[6,    80] loss: 1.959 acc: 0.273\n",
      "[6,    81] loss: 2.002 acc: 0.305\n",
      "[6,    82] loss: 1.932 acc: 0.309\n",
      "[6,    83] loss: 1.976 acc: 0.285\n",
      "[6,    84] loss: 1.935 acc: 0.355\n",
      "[6,    85] loss: 1.911 acc: 0.328\n",
      "[6,    86] loss: 1.994 acc: 0.328\n",
      "[6,    87] loss: 2.002 acc: 0.262\n",
      "[6,    88] loss: 1.931 acc: 0.305\n",
      "[6,    89] loss: 1.929 acc: 0.324\n",
      "[6,    90] loss: 1.981 acc: 0.293\n",
      "[6,    91] loss: 1.960 acc: 0.312\n",
      "[6,    92] loss: 1.914 acc: 0.340\n",
      "[6,    93] loss: 1.916 acc: 0.340\n",
      "[6,    94] loss: 1.941 acc: 0.324\n",
      "[6,    95] loss: 1.903 acc: 0.367\n",
      "[6,    96] loss: 1.914 acc: 0.363\n",
      "[6,    97] loss: 1.955 acc: 0.320\n",
      "[6,    98] loss: 1.989 acc: 0.309\n",
      "[6,    99] loss: 1.911 acc: 0.324\n",
      "[6,   100] loss: 1.959 acc: 0.328\n",
      "[6,   101] loss: 1.949 acc: 0.348\n",
      "[6,   102] loss: 1.885 acc: 0.348\n",
      "[6,   103] loss: 1.921 acc: 0.305\n",
      "[6,   104] loss: 1.997 acc: 0.293\n",
      "[6,   105] loss: 1.922 acc: 0.324\n",
      "[6,   106] loss: 2.001 acc: 0.309\n",
      "[6,   107] loss: 2.003 acc: 0.320\n",
      "[6,   108] loss: 1.976 acc: 0.301\n",
      "[6,   109] loss: 1.983 acc: 0.266\n",
      "[6,   110] loss: 1.985 acc: 0.305\n",
      "[6,   111] loss: 1.983 acc: 0.293\n",
      "[6,   112] loss: 1.989 acc: 0.246\n",
      "[6,   113] loss: 1.916 acc: 0.328\n",
      "[6,   114] loss: 1.994 acc: 0.277\n",
      "[6,   115] loss: 1.958 acc: 0.297\n",
      "[6,   116] loss: 1.983 acc: 0.270\n",
      "[6,   117] loss: 1.927 acc: 0.293\n",
      "[6,   118] loss: 1.964 acc: 0.312\n",
      "[6,   119] loss: 1.940 acc: 0.320\n",
      "[6,   120] loss: 2.017 acc: 0.309\n",
      "[6,   121] loss: 1.957 acc: 0.332\n",
      "[6,   122] loss: 1.978 acc: 0.312\n",
      "[6,   123] loss: 1.954 acc: 0.344\n",
      "[6,   124] loss: 1.933 acc: 0.293\n",
      "[6,   125] loss: 1.984 acc: 0.301\n",
      "[6,   126] loss: 1.970 acc: 0.305\n",
      "[6,   127] loss: 1.963 acc: 0.293\n",
      "[6,   128] loss: 1.948 acc: 0.340\n",
      "[6,   129] loss: 1.967 acc: 0.270\n",
      "[6,   130] loss: 1.934 acc: 0.328\n",
      "[6,   131] loss: 2.011 acc: 0.270\n",
      "[6,   132] loss: 1.997 acc: 0.281\n",
      "[6,   133] loss: 1.976 acc: 0.270\n",
      "[6,   134] loss: 1.905 acc: 0.312\n",
      "[6,   135] loss: 1.970 acc: 0.285\n",
      "[6,   136] loss: 1.975 acc: 0.312\n",
      "[6,   137] loss: 1.997 acc: 0.258\n",
      "[6,   138] loss: 1.980 acc: 0.285\n",
      "[6,   139] loss: 1.923 acc: 0.328\n",
      "[6,   140] loss: 1.956 acc: 0.285\n",
      "[6,   141] loss: 1.989 acc: 0.316\n",
      "[6,   142] loss: 1.929 acc: 0.324\n",
      "[6,   143] loss: 1.898 acc: 0.363\n",
      "[6,   144] loss: 1.921 acc: 0.332\n",
      "[6,   145] loss: 1.998 acc: 0.305\n",
      "[6,   146] loss: 2.000 acc: 0.262\n",
      "[6,   147] loss: 1.989 acc: 0.305\n",
      "[6,   148] loss: 1.978 acc: 0.297\n",
      "[6,   149] loss: 2.058 acc: 0.242\n",
      "[6,   150] loss: 1.953 acc: 0.340\n",
      "[6,   151] loss: 1.933 acc: 0.301\n",
      "[6,   152] loss: 1.917 acc: 0.371\n",
      "[6,   153] loss: 1.915 acc: 0.320\n",
      "[6,   154] loss: 1.985 acc: 0.262\n",
      "[6,   155] loss: 1.998 acc: 0.301\n",
      "[6,   156] loss: 2.027 acc: 0.277\n",
      "[6,   157] loss: 1.963 acc: 0.352\n",
      "[6,   158] loss: 2.000 acc: 0.312\n",
      "[6,   159] loss: 1.872 acc: 0.371\n",
      "[6,   160] loss: 2.047 acc: 0.289\n",
      "[6,   161] loss: 2.008 acc: 0.309\n",
      "[6,   162] loss: 1.943 acc: 0.320\n",
      "[6,   163] loss: 1.991 acc: 0.289\n",
      "[6,   164] loss: 1.942 acc: 0.297\n",
      "[6,   165] loss: 1.973 acc: 0.309\n",
      "[6,   166] loss: 1.899 acc: 0.336\n",
      "[6,   167] loss: 2.028 acc: 0.297\n",
      "[6,   168] loss: 1.989 acc: 0.273\n",
      "[6,   169] loss: 1.976 acc: 0.281\n",
      "[6,   170] loss: 1.959 acc: 0.273\n",
      "[6,   171] loss: 1.939 acc: 0.266\n",
      "[6,   172] loss: 1.937 acc: 0.336\n",
      "[6,   173] loss: 1.961 acc: 0.289\n",
      "[6,   174] loss: 1.965 acc: 0.324\n",
      "[6,   175] loss: 1.982 acc: 0.309\n",
      "[6,   176] loss: 1.950 acc: 0.285\n",
      "[6,   177] loss: 1.957 acc: 0.332\n",
      "[6,   178] loss: 2.025 acc: 0.289\n",
      "[6,   179] loss: 1.952 acc: 0.297\n",
      "[6,   180] loss: 1.962 acc: 0.289\n",
      "[6,   181] loss: 1.918 acc: 0.312\n",
      "[6,   182] loss: 1.997 acc: 0.262\n",
      "[6,   183] loss: 1.971 acc: 0.309\n",
      "[6,   184] loss: 1.997 acc: 0.316\n",
      "[6,   185] loss: 1.912 acc: 0.355\n",
      "[6,   186] loss: 1.949 acc: 0.340\n",
      "[6,   187] loss: 1.950 acc: 0.305\n",
      "[6,   188] loss: 2.041 acc: 0.273\n",
      "[6,   189] loss: 1.941 acc: 0.324\n",
      "[6,   190] loss: 1.953 acc: 0.336\n",
      "[6,   191] loss: 1.953 acc: 0.273\n",
      "[6,   192] loss: 1.946 acc: 0.297\n",
      "[6,   193] loss: 1.902 acc: 0.352\n",
      "[6,   194] loss: 1.993 acc: 0.297\n",
      "[6,   195] loss: 1.957 acc: 0.309\n",
      "[6,   196] loss: 2.090 acc: 0.287\n",
      "[7,     1] loss: 1.938 acc: 0.289\n",
      "[7,     2] loss: 1.936 acc: 0.344\n",
      "[7,     3] loss: 1.933 acc: 0.332\n",
      "[7,     4] loss: 1.963 acc: 0.305\n",
      "[7,     5] loss: 1.906 acc: 0.324\n",
      "[7,     6] loss: 1.910 acc: 0.328\n",
      "[7,     7] loss: 1.929 acc: 0.305\n",
      "[7,     8] loss: 2.045 acc: 0.266\n",
      "[7,     9] loss: 1.965 acc: 0.305\n",
      "[7,    10] loss: 1.932 acc: 0.336\n",
      "[7,    11] loss: 1.945 acc: 0.316\n",
      "[7,    12] loss: 1.923 acc: 0.316\n",
      "[7,    13] loss: 1.971 acc: 0.250\n",
      "[7,    14] loss: 1.962 acc: 0.324\n",
      "[7,    15] loss: 1.929 acc: 0.344\n",
      "[7,    16] loss: 2.007 acc: 0.316\n",
      "[7,    17] loss: 1.961 acc: 0.281\n",
      "[7,    18] loss: 1.986 acc: 0.254\n",
      "[7,    19] loss: 1.908 acc: 0.348\n",
      "[7,    20] loss: 1.982 acc: 0.305\n",
      "[7,    21] loss: 1.968 acc: 0.328\n",
      "[7,    22] loss: 1.935 acc: 0.344\n",
      "[7,    23] loss: 1.907 acc: 0.289\n",
      "[7,    24] loss: 1.934 acc: 0.336\n",
      "[7,    25] loss: 1.928 acc: 0.289\n",
      "[7,    26] loss: 1.961 acc: 0.277\n",
      "[7,    27] loss: 2.005 acc: 0.250\n",
      "[7,    28] loss: 1.919 acc: 0.309\n",
      "[7,    29] loss: 1.935 acc: 0.293\n",
      "[7,    30] loss: 1.999 acc: 0.320\n",
      "[7,    31] loss: 1.943 acc: 0.293\n",
      "[7,    32] loss: 1.919 acc: 0.293\n",
      "[7,    33] loss: 1.983 acc: 0.293\n",
      "[7,    34] loss: 1.955 acc: 0.328\n",
      "[7,    35] loss: 2.014 acc: 0.301\n",
      "[7,    36] loss: 1.987 acc: 0.270\n",
      "[7,    37] loss: 1.973 acc: 0.293\n",
      "[7,    38] loss: 1.951 acc: 0.270\n",
      "[7,    39] loss: 1.958 acc: 0.344\n",
      "[7,    40] loss: 2.024 acc: 0.336\n",
      "[7,    41] loss: 1.978 acc: 0.316\n",
      "[7,    42] loss: 1.946 acc: 0.328\n",
      "[7,    43] loss: 1.928 acc: 0.312\n",
      "[7,    44] loss: 1.936 acc: 0.348\n",
      "[7,    45] loss: 1.986 acc: 0.258\n",
      "[7,    46] loss: 1.884 acc: 0.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,    47] loss: 1.996 acc: 0.277\n",
      "[7,    48] loss: 1.952 acc: 0.301\n",
      "[7,    49] loss: 1.981 acc: 0.324\n",
      "[7,    50] loss: 1.948 acc: 0.285\n",
      "[7,    51] loss: 1.970 acc: 0.312\n",
      "[7,    52] loss: 1.993 acc: 0.277\n",
      "[7,    53] loss: 1.960 acc: 0.340\n",
      "[7,    54] loss: 1.925 acc: 0.309\n",
      "[7,    55] loss: 1.966 acc: 0.312\n",
      "[7,    56] loss: 1.974 acc: 0.277\n",
      "[7,    57] loss: 1.989 acc: 0.273\n",
      "[7,    58] loss: 1.977 acc: 0.293\n",
      "[7,    59] loss: 2.013 acc: 0.301\n",
      "[7,    60] loss: 1.970 acc: 0.309\n",
      "[7,    61] loss: 2.011 acc: 0.277\n",
      "[7,    62] loss: 1.938 acc: 0.320\n",
      "[7,    63] loss: 1.982 acc: 0.227\n",
      "[7,    64] loss: 1.980 acc: 0.246\n",
      "[7,    65] loss: 1.993 acc: 0.301\n",
      "[7,    66] loss: 2.047 acc: 0.281\n",
      "[7,    67] loss: 1.958 acc: 0.305\n",
      "[7,    68] loss: 1.988 acc: 0.312\n",
      "[7,    69] loss: 1.975 acc: 0.320\n",
      "[7,    70] loss: 1.965 acc: 0.301\n",
      "[7,    71] loss: 1.941 acc: 0.340\n",
      "[7,    72] loss: 1.945 acc: 0.344\n",
      "[7,    73] loss: 1.990 acc: 0.266\n",
      "[7,    74] loss: 1.930 acc: 0.344\n",
      "[7,    75] loss: 1.977 acc: 0.316\n",
      "[7,    76] loss: 2.009 acc: 0.258\n",
      "[7,    77] loss: 1.924 acc: 0.305\n",
      "[7,    78] loss: 1.958 acc: 0.324\n",
      "[7,    79] loss: 1.966 acc: 0.324\n",
      "[7,    80] loss: 1.946 acc: 0.293\n",
      "[7,    81] loss: 1.989 acc: 0.309\n",
      "[7,    82] loss: 1.914 acc: 0.336\n",
      "[7,    83] loss: 1.961 acc: 0.289\n",
      "[7,    84] loss: 1.920 acc: 0.363\n",
      "[7,    85] loss: 1.894 acc: 0.340\n",
      "[7,    86] loss: 1.982 acc: 0.344\n",
      "[7,    87] loss: 1.989 acc: 0.254\n",
      "[7,    88] loss: 1.912 acc: 0.309\n",
      "[7,    89] loss: 1.912 acc: 0.332\n",
      "[7,    90] loss: 1.966 acc: 0.301\n",
      "[7,    91] loss: 1.944 acc: 0.324\n",
      "[7,    92] loss: 1.897 acc: 0.355\n",
      "[7,    93] loss: 1.901 acc: 0.332\n",
      "[7,    94] loss: 1.926 acc: 0.332\n",
      "[7,    95] loss: 1.886 acc: 0.355\n",
      "[7,    96] loss: 1.896 acc: 0.371\n",
      "[7,    97] loss: 1.941 acc: 0.320\n",
      "[7,    98] loss: 1.972 acc: 0.309\n",
      "[7,    99] loss: 1.893 acc: 0.328\n",
      "[7,   100] loss: 1.942 acc: 0.336\n",
      "[7,   101] loss: 1.938 acc: 0.359\n",
      "[7,   102] loss: 1.863 acc: 0.363\n",
      "[7,   103] loss: 1.900 acc: 0.320\n",
      "[7,   104] loss: 1.985 acc: 0.277\n",
      "[7,   105] loss: 1.906 acc: 0.320\n",
      "[7,   106] loss: 1.988 acc: 0.312\n",
      "[7,   107] loss: 1.990 acc: 0.324\n",
      "[7,   108] loss: 1.960 acc: 0.305\n",
      "[7,   109] loss: 1.969 acc: 0.270\n",
      "[7,   110] loss: 1.974 acc: 0.309\n",
      "[7,   111] loss: 1.967 acc: 0.289\n",
      "[7,   112] loss: 1.974 acc: 0.266\n",
      "[7,   113] loss: 1.897 acc: 0.344\n",
      "[7,   114] loss: 1.979 acc: 0.289\n",
      "[7,   115] loss: 1.942 acc: 0.309\n",
      "[7,   116] loss: 1.969 acc: 0.277\n",
      "[7,   117] loss: 1.910 acc: 0.301\n",
      "[7,   118] loss: 1.948 acc: 0.320\n",
      "[7,   119] loss: 1.923 acc: 0.328\n",
      "[7,   120] loss: 2.006 acc: 0.309\n",
      "[7,   121] loss: 1.940 acc: 0.344\n",
      "[7,   122] loss: 1.965 acc: 0.312\n",
      "[7,   123] loss: 1.936 acc: 0.359\n",
      "[7,   124] loss: 1.915 acc: 0.309\n",
      "[7,   125] loss: 1.969 acc: 0.320\n",
      "[7,   126] loss: 1.958 acc: 0.312\n",
      "[7,   127] loss: 1.948 acc: 0.293\n",
      "[7,   128] loss: 1.934 acc: 0.352\n",
      "[7,   129] loss: 1.954 acc: 0.262\n",
      "[7,   130] loss: 1.918 acc: 0.336\n",
      "[7,   131] loss: 1.998 acc: 0.277\n",
      "[7,   132] loss: 1.988 acc: 0.293\n",
      "[7,   133] loss: 1.963 acc: 0.273\n",
      "[7,   134] loss: 1.885 acc: 0.332\n",
      "[7,   135] loss: 1.958 acc: 0.297\n",
      "[7,   136] loss: 1.962 acc: 0.320\n",
      "[7,   137] loss: 1.983 acc: 0.262\n",
      "[7,   138] loss: 1.969 acc: 0.305\n",
      "[7,   139] loss: 1.905 acc: 0.344\n",
      "[7,   140] loss: 1.942 acc: 0.273\n",
      "[7,   141] loss: 1.974 acc: 0.316\n",
      "[7,   142] loss: 1.911 acc: 0.352\n",
      "[7,   143] loss: 1.881 acc: 0.363\n",
      "[7,   144] loss: 1.905 acc: 0.352\n",
      "[7,   145] loss: 1.987 acc: 0.316\n",
      "[7,   146] loss: 1.986 acc: 0.277\n",
      "[7,   147] loss: 1.975 acc: 0.316\n",
      "[7,   148] loss: 1.964 acc: 0.301\n",
      "[7,   149] loss: 2.050 acc: 0.246\n",
      "[7,   150] loss: 1.940 acc: 0.344\n",
      "[7,   151] loss: 1.918 acc: 0.305\n",
      "[7,   152] loss: 1.900 acc: 0.371\n",
      "[7,   153] loss: 1.899 acc: 0.328\n",
      "[7,   154] loss: 1.970 acc: 0.270\n",
      "[7,   155] loss: 1.987 acc: 0.297\n",
      "[7,   156] loss: 2.017 acc: 0.285\n",
      "[7,   157] loss: 1.948 acc: 0.355\n",
      "[7,   158] loss: 1.989 acc: 0.312\n",
      "[7,   159] loss: 1.856 acc: 0.371\n",
      "[7,   160] loss: 2.039 acc: 0.293\n",
      "[7,   161] loss: 1.995 acc: 0.301\n",
      "[7,   162] loss: 1.929 acc: 0.332\n",
      "[7,   163] loss: 1.976 acc: 0.301\n",
      "[7,   164] loss: 1.927 acc: 0.301\n",
      "[7,   165] loss: 1.962 acc: 0.309\n",
      "[7,   166] loss: 1.880 acc: 0.332\n",
      "[7,   167] loss: 2.017 acc: 0.305\n",
      "[7,   168] loss: 1.977 acc: 0.281\n",
      "[7,   169] loss: 1.961 acc: 0.289\n",
      "[7,   170] loss: 1.945 acc: 0.285\n",
      "[7,   171] loss: 1.925 acc: 0.273\n",
      "[7,   172] loss: 1.926 acc: 0.340\n",
      "[7,   173] loss: 1.947 acc: 0.293\n",
      "[7,   174] loss: 1.950 acc: 0.332\n",
      "[7,   175] loss: 1.972 acc: 0.309\n",
      "[7,   176] loss: 1.934 acc: 0.297\n",
      "[7,   177] loss: 1.944 acc: 0.332\n",
      "[7,   178] loss: 2.012 acc: 0.281\n",
      "[7,   179] loss: 1.935 acc: 0.309\n",
      "[7,   180] loss: 1.951 acc: 0.305\n",
      "[7,   181] loss: 1.899 acc: 0.316\n",
      "[7,   182] loss: 1.985 acc: 0.258\n",
      "[7,   183] loss: 1.955 acc: 0.316\n",
      "[7,   184] loss: 1.990 acc: 0.312\n",
      "[7,   185] loss: 1.896 acc: 0.363\n",
      "[7,   186] loss: 1.933 acc: 0.348\n",
      "[7,   187] loss: 1.936 acc: 0.312\n",
      "[7,   188] loss: 2.032 acc: 0.277\n",
      "[7,   189] loss: 1.927 acc: 0.316\n",
      "[7,   190] loss: 1.942 acc: 0.340\n",
      "[7,   191] loss: 1.941 acc: 0.285\n",
      "[7,   192] loss: 1.933 acc: 0.293\n",
      "[7,   193] loss: 1.887 acc: 0.367\n",
      "[7,   194] loss: 1.981 acc: 0.316\n",
      "[7,   195] loss: 1.941 acc: 0.305\n",
      "[7,   196] loss: 2.082 acc: 0.275\n",
      "[8,     1] loss: 1.922 acc: 0.293\n",
      "[8,     2] loss: 1.921 acc: 0.352\n",
      "[8,     3] loss: 1.919 acc: 0.336\n",
      "[8,     4] loss: 1.948 acc: 0.309\n",
      "[8,     5] loss: 1.894 acc: 0.328\n",
      "[8,     6] loss: 1.894 acc: 0.328\n",
      "[8,     7] loss: 1.912 acc: 0.312\n",
      "[8,     8] loss: 2.031 acc: 0.273\n",
      "[8,     9] loss: 1.951 acc: 0.312\n",
      "[8,    10] loss: 1.919 acc: 0.344\n",
      "[8,    11] loss: 1.932 acc: 0.316\n",
      "[8,    12] loss: 1.908 acc: 0.332\n",
      "[8,    13] loss: 1.958 acc: 0.273\n",
      "[8,    14] loss: 1.949 acc: 0.336\n",
      "[8,    15] loss: 1.914 acc: 0.352\n",
      "[8,    16] loss: 1.997 acc: 0.320\n",
      "[8,    17] loss: 1.945 acc: 0.309\n",
      "[8,    18] loss: 1.972 acc: 0.258\n",
      "[8,    19] loss: 1.895 acc: 0.355\n",
      "[8,    20] loss: 1.973 acc: 0.305\n",
      "[8,    21] loss: 1.955 acc: 0.332\n",
      "[8,    22] loss: 1.921 acc: 0.348\n",
      "[8,    23] loss: 1.891 acc: 0.297\n",
      "[8,    24] loss: 1.918 acc: 0.340\n",
      "[8,    25] loss: 1.911 acc: 0.297\n",
      "[8,    26] loss: 1.943 acc: 0.281\n",
      "[8,    27] loss: 1.990 acc: 0.262\n",
      "[8,    28] loss: 1.903 acc: 0.312\n",
      "[8,    29] loss: 1.919 acc: 0.312\n",
      "[8,    30] loss: 1.986 acc: 0.328\n",
      "[8,    31] loss: 1.928 acc: 0.305\n",
      "[8,    32] loss: 1.905 acc: 0.309\n",
      "[8,    33] loss: 1.971 acc: 0.297\n",
      "[8,    34] loss: 1.939 acc: 0.348\n",
      "[8,    35] loss: 2.003 acc: 0.312\n",
      "[8,    36] loss: 1.976 acc: 0.273\n",
      "[8,    37] loss: 1.963 acc: 0.297\n",
      "[8,    38] loss: 1.936 acc: 0.289\n",
      "[8,    39] loss: 1.945 acc: 0.348\n",
      "[8,    40] loss: 2.015 acc: 0.336\n",
      "[8,    41] loss: 1.967 acc: 0.320\n",
      "[8,    42] loss: 1.933 acc: 0.336\n",
      "[8,    43] loss: 1.913 acc: 0.324\n",
      "[8,    44] loss: 1.924 acc: 0.359\n",
      "[8,    45] loss: 1.971 acc: 0.273\n",
      "[8,    46] loss: 1.866 acc: 0.379\n",
      "[8,    47] loss: 1.984 acc: 0.273\n",
      "[8,    48] loss: 1.937 acc: 0.297\n",
      "[8,    49] loss: 1.971 acc: 0.328\n",
      "[8,    50] loss: 1.936 acc: 0.285\n",
      "[8,    51] loss: 1.953 acc: 0.320\n",
      "[8,    52] loss: 1.984 acc: 0.305\n",
      "[8,    53] loss: 1.946 acc: 0.336\n",
      "[8,    54] loss: 1.912 acc: 0.297\n",
      "[8,    55] loss: 1.950 acc: 0.324\n",
      "[8,    56] loss: 1.959 acc: 0.285\n",
      "[8,    57] loss: 1.976 acc: 0.297\n",
      "[8,    58] loss: 1.964 acc: 0.305\n",
      "[8,    59] loss: 2.003 acc: 0.312\n",
      "[8,    60] loss: 1.958 acc: 0.312\n",
      "[8,    61] loss: 1.999 acc: 0.281\n",
      "[8,    62] loss: 1.922 acc: 0.324\n",
      "[8,    63] loss: 1.970 acc: 0.234\n",
      "[8,    64] loss: 1.966 acc: 0.258\n",
      "[8,    65] loss: 1.982 acc: 0.309\n",
      "[8,    66] loss: 2.035 acc: 0.273\n",
      "[8,    67] loss: 1.942 acc: 0.309\n",
      "[8,    68] loss: 1.975 acc: 0.328\n",
      "[8,    69] loss: 1.957 acc: 0.332\n",
      "[8,    70] loss: 1.952 acc: 0.305\n",
      "[8,    71] loss: 1.928 acc: 0.344\n",
      "[8,    72] loss: 1.931 acc: 0.348\n",
      "[8,    73] loss: 1.978 acc: 0.293\n",
      "[8,    74] loss: 1.915 acc: 0.359\n",
      "[8,    75] loss: 1.961 acc: 0.328\n",
      "[8,    76] loss: 1.994 acc: 0.258\n",
      "[8,    77] loss: 1.912 acc: 0.309\n",
      "[8,    78] loss: 1.943 acc: 0.328\n",
      "[8,    79] loss: 1.952 acc: 0.332\n",
      "[8,    80] loss: 1.935 acc: 0.316\n",
      "[8,    81] loss: 1.977 acc: 0.312\n",
      "[8,    82] loss: 1.897 acc: 0.344\n",
      "[8,    83] loss: 1.947 acc: 0.305\n",
      "[8,    84] loss: 1.907 acc: 0.375\n",
      "[8,    85] loss: 1.880 acc: 0.363\n",
      "[8,    86] loss: 1.970 acc: 0.348\n",
      "[8,    87] loss: 1.978 acc: 0.262\n",
      "[8,    88] loss: 1.895 acc: 0.324\n",
      "[8,    89] loss: 1.897 acc: 0.352\n",
      "[8,    90] loss: 1.952 acc: 0.305\n",
      "[8,    91] loss: 1.929 acc: 0.324\n",
      "[8,    92] loss: 1.881 acc: 0.352\n",
      "[8,    93] loss: 1.887 acc: 0.336\n",
      "[8,    94] loss: 1.911 acc: 0.320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,    95] loss: 1.870 acc: 0.359\n",
      "[8,    96] loss: 1.880 acc: 0.375\n",
      "[8,    97] loss: 1.929 acc: 0.320\n",
      "[8,    98] loss: 1.957 acc: 0.309\n",
      "[8,    99] loss: 1.878 acc: 0.332\n",
      "[8,   100] loss: 1.927 acc: 0.336\n",
      "[8,   101] loss: 1.927 acc: 0.355\n",
      "[8,   102] loss: 1.844 acc: 0.367\n",
      "[8,   103] loss: 1.881 acc: 0.320\n",
      "[8,   104] loss: 1.974 acc: 0.281\n",
      "[8,   105] loss: 1.892 acc: 0.328\n",
      "[8,   106] loss: 1.977 acc: 0.336\n",
      "[8,   107] loss: 1.979 acc: 0.324\n",
      "[8,   108] loss: 1.946 acc: 0.320\n",
      "[8,   109] loss: 1.957 acc: 0.273\n",
      "[8,   110] loss: 1.964 acc: 0.324\n",
      "[8,   111] loss: 1.952 acc: 0.289\n",
      "[8,   112] loss: 1.960 acc: 0.270\n",
      "[8,   113] loss: 1.881 acc: 0.352\n",
      "[8,   114] loss: 1.965 acc: 0.301\n",
      "[8,   115] loss: 1.928 acc: 0.309\n",
      "[8,   116] loss: 1.956 acc: 0.277\n",
      "[8,   117] loss: 1.896 acc: 0.312\n",
      "[8,   118] loss: 1.932 acc: 0.316\n",
      "[8,   119] loss: 1.908 acc: 0.328\n",
      "[8,   120] loss: 1.995 acc: 0.320\n",
      "[8,   121] loss: 1.923 acc: 0.336\n",
      "[8,   122] loss: 1.952 acc: 0.320\n",
      "[8,   123] loss: 1.920 acc: 0.371\n",
      "[8,   124] loss: 1.899 acc: 0.309\n",
      "[8,   125] loss: 1.956 acc: 0.324\n",
      "[8,   126] loss: 1.947 acc: 0.324\n",
      "[8,   127] loss: 1.934 acc: 0.293\n",
      "[8,   128] loss: 1.921 acc: 0.352\n",
      "[8,   129] loss: 1.943 acc: 0.270\n",
      "[8,   130] loss: 1.905 acc: 0.332\n",
      "[8,   131] loss: 1.986 acc: 0.289\n",
      "[8,   132] loss: 1.980 acc: 0.293\n",
      "[8,   133] loss: 1.952 acc: 0.289\n",
      "[8,   134] loss: 1.868 acc: 0.359\n",
      "[8,   135] loss: 1.946 acc: 0.316\n",
      "[8,   136] loss: 1.949 acc: 0.328\n",
      "[8,   137] loss: 1.970 acc: 0.258\n",
      "[8,   138] loss: 1.959 acc: 0.320\n",
      "[8,   139] loss: 1.890 acc: 0.352\n",
      "[8,   140] loss: 1.929 acc: 0.277\n",
      "[8,   141] loss: 1.960 acc: 0.316\n",
      "[8,   142] loss: 1.895 acc: 0.355\n",
      "[8,   143] loss: 1.866 acc: 0.367\n",
      "[8,   144] loss: 1.891 acc: 0.355\n",
      "[8,   145] loss: 1.978 acc: 0.320\n",
      "[8,   146] loss: 1.973 acc: 0.289\n",
      "[8,   147] loss: 1.962 acc: 0.328\n",
      "[8,   148] loss: 1.951 acc: 0.312\n",
      "[8,   149] loss: 2.041 acc: 0.250\n",
      "[8,   150] loss: 1.928 acc: 0.352\n",
      "[8,   151] loss: 1.904 acc: 0.297\n",
      "[8,   152] loss: 1.884 acc: 0.367\n",
      "[8,   153] loss: 1.884 acc: 0.328\n",
      "[8,   154] loss: 1.957 acc: 0.277\n",
      "[8,   155] loss: 1.976 acc: 0.305\n",
      "[8,   156] loss: 2.008 acc: 0.297\n",
      "[8,   157] loss: 1.934 acc: 0.359\n",
      "[8,   158] loss: 1.978 acc: 0.320\n",
      "[8,   159] loss: 1.841 acc: 0.387\n",
      "[8,   160] loss: 2.031 acc: 0.297\n",
      "[8,   161] loss: 1.982 acc: 0.309\n",
      "[8,   162] loss: 1.915 acc: 0.336\n",
      "[8,   163] loss: 1.962 acc: 0.301\n",
      "[8,   164] loss: 1.914 acc: 0.301\n",
      "[8,   165] loss: 1.952 acc: 0.309\n",
      "[8,   166] loss: 1.863 acc: 0.344\n",
      "[8,   167] loss: 2.007 acc: 0.297\n",
      "[8,   168] loss: 1.965 acc: 0.293\n",
      "[8,   169] loss: 1.947 acc: 0.301\n",
      "[8,   170] loss: 1.933 acc: 0.301\n",
      "[8,   171] loss: 1.911 acc: 0.273\n",
      "[8,   172] loss: 1.915 acc: 0.355\n",
      "[8,   173] loss: 1.935 acc: 0.297\n",
      "[8,   174] loss: 1.937 acc: 0.336\n",
      "[8,   175] loss: 1.961 acc: 0.312\n",
      "[8,   176] loss: 1.919 acc: 0.316\n",
      "[8,   177] loss: 1.932 acc: 0.328\n",
      "[8,   178] loss: 2.000 acc: 0.289\n",
      "[8,   179] loss: 1.920 acc: 0.305\n",
      "[8,   180] loss: 1.940 acc: 0.305\n",
      "[8,   181] loss: 1.883 acc: 0.328\n",
      "[8,   182] loss: 1.974 acc: 0.254\n",
      "[8,   183] loss: 1.941 acc: 0.320\n",
      "[8,   184] loss: 1.983 acc: 0.316\n",
      "[8,   185] loss: 1.881 acc: 0.375\n",
      "[8,   186] loss: 1.919 acc: 0.348\n",
      "[8,   187] loss: 1.924 acc: 0.328\n",
      "[8,   188] loss: 2.023 acc: 0.281\n",
      "[8,   189] loss: 1.914 acc: 0.320\n",
      "[8,   190] loss: 1.931 acc: 0.336\n",
      "[8,   191] loss: 1.930 acc: 0.285\n",
      "[8,   192] loss: 1.922 acc: 0.305\n",
      "[8,   193] loss: 1.874 acc: 0.359\n",
      "[8,   194] loss: 1.970 acc: 0.316\n",
      "[8,   195] loss: 1.927 acc: 0.316\n",
      "[8,   196] loss: 2.075 acc: 0.287\n",
      "[9,     1] loss: 1.908 acc: 0.305\n",
      "[9,     2] loss: 1.906 acc: 0.355\n",
      "[9,     3] loss: 1.906 acc: 0.355\n",
      "[9,     4] loss: 1.934 acc: 0.309\n",
      "[9,     5] loss: 1.883 acc: 0.328\n",
      "[9,     6] loss: 1.880 acc: 0.301\n",
      "[9,     7] loss: 1.896 acc: 0.324\n",
      "[9,     8] loss: 2.017 acc: 0.281\n",
      "[9,     9] loss: 1.938 acc: 0.328\n",
      "[9,    10] loss: 1.908 acc: 0.344\n",
      "[9,    11] loss: 1.919 acc: 0.320\n",
      "[9,    12] loss: 1.894 acc: 0.340\n",
      "[9,    13] loss: 1.945 acc: 0.281\n",
      "[9,    14] loss: 1.938 acc: 0.336\n",
      "[9,    15] loss: 1.899 acc: 0.359\n",
      "[9,    16] loss: 1.987 acc: 0.332\n",
      "[9,    17] loss: 1.930 acc: 0.320\n",
      "[9,    18] loss: 1.959 acc: 0.270\n",
      "[9,    19] loss: 1.883 acc: 0.359\n",
      "[9,    20] loss: 1.965 acc: 0.316\n",
      "[9,    21] loss: 1.944 acc: 0.328\n",
      "[9,    22] loss: 1.908 acc: 0.352\n",
      "[9,    23] loss: 1.877 acc: 0.309\n",
      "[9,    24] loss: 1.904 acc: 0.352\n",
      "[9,    25] loss: 1.895 acc: 0.301\n",
      "[9,    26] loss: 1.927 acc: 0.297\n",
      "[9,    27] loss: 1.976 acc: 0.266\n",
      "[9,    28] loss: 1.888 acc: 0.320\n",
      "[9,    29] loss: 1.903 acc: 0.340\n",
      "[9,    30] loss: 1.973 acc: 0.332\n",
      "[9,    31] loss: 1.914 acc: 0.324\n",
      "[9,    32] loss: 1.892 acc: 0.324\n",
      "[9,    33] loss: 1.959 acc: 0.305\n",
      "[9,    34] loss: 1.925 acc: 0.363\n",
      "[9,    35] loss: 1.994 acc: 0.309\n",
      "[9,    36] loss: 1.966 acc: 0.285\n",
      "[9,    37] loss: 1.955 acc: 0.293\n",
      "[9,    38] loss: 1.922 acc: 0.297\n",
      "[9,    39] loss: 1.932 acc: 0.352\n",
      "[9,    40] loss: 2.007 acc: 0.340\n",
      "[9,    41] loss: 1.957 acc: 0.324\n",
      "[9,    42] loss: 1.920 acc: 0.348\n",
      "[9,    43] loss: 1.900 acc: 0.340\n",
      "[9,    44] loss: 1.912 acc: 0.359\n",
      "[9,    45] loss: 1.956 acc: 0.273\n",
      "[9,    46] loss: 1.849 acc: 0.383\n",
      "[9,    47] loss: 1.974 acc: 0.285\n",
      "[9,    48] loss: 1.922 acc: 0.297\n",
      "[9,    49] loss: 1.961 acc: 0.336\n",
      "[9,    50] loss: 1.926 acc: 0.301\n",
      "[9,    51] loss: 1.938 acc: 0.320\n",
      "[9,    52] loss: 1.975 acc: 0.301\n",
      "[9,    53] loss: 1.933 acc: 0.352\n",
      "[9,    54] loss: 1.899 acc: 0.301\n",
      "[9,    55] loss: 1.935 acc: 0.340\n",
      "[9,    56] loss: 1.946 acc: 0.289\n",
      "[9,    57] loss: 1.963 acc: 0.297\n",
      "[9,    58] loss: 1.952 acc: 0.312\n",
      "[9,    59] loss: 1.994 acc: 0.324\n",
      "[9,    60] loss: 1.947 acc: 0.293\n",
      "[9,    61] loss: 1.988 acc: 0.285\n",
      "[9,    62] loss: 1.909 acc: 0.332\n",
      "[9,    63] loss: 1.960 acc: 0.250\n",
      "[9,    64] loss: 1.952 acc: 0.266\n",
      "[9,    65] loss: 1.971 acc: 0.312\n",
      "[9,    66] loss: 2.024 acc: 0.277\n",
      "[9,    67] loss: 1.927 acc: 0.328\n",
      "[9,    68] loss: 1.964 acc: 0.336\n",
      "[9,    69] loss: 1.940 acc: 0.336\n",
      "[9,    70] loss: 1.940 acc: 0.297\n",
      "[9,    71] loss: 1.917 acc: 0.348\n",
      "[9,    72] loss: 1.918 acc: 0.352\n",
      "[9,    73] loss: 1.967 acc: 0.293\n",
      "[9,    74] loss: 1.902 acc: 0.359\n",
      "[9,    75] loss: 1.946 acc: 0.332\n",
      "[9,    76] loss: 1.979 acc: 0.266\n",
      "[9,    77] loss: 1.901 acc: 0.312\n",
      "[9,    78] loss: 1.928 acc: 0.332\n",
      "[9,    79] loss: 1.938 acc: 0.340\n",
      "[9,    80] loss: 1.925 acc: 0.332\n",
      "[9,    81] loss: 1.965 acc: 0.324\n",
      "[9,    82] loss: 1.881 acc: 0.355\n",
      "[9,    83] loss: 1.933 acc: 0.309\n",
      "[9,    84] loss: 1.895 acc: 0.379\n",
      "[9,    85] loss: 1.867 acc: 0.355\n",
      "[9,    86] loss: 1.959 acc: 0.352\n",
      "[9,    87] loss: 1.967 acc: 0.262\n",
      "[9,    88] loss: 1.879 acc: 0.340\n",
      "[9,    89] loss: 1.884 acc: 0.355\n",
      "[9,    90] loss: 1.939 acc: 0.309\n",
      "[9,    91] loss: 1.916 acc: 0.320\n",
      "[9,    92] loss: 1.867 acc: 0.371\n",
      "[9,    93] loss: 1.875 acc: 0.332\n",
      "[9,    94] loss: 1.898 acc: 0.332\n",
      "[9,    95] loss: 1.856 acc: 0.352\n",
      "[9,    96] loss: 1.866 acc: 0.375\n",
      "[9,    97] loss: 1.918 acc: 0.324\n",
      "[9,    98] loss: 1.941 acc: 0.316\n",
      "[9,    99] loss: 1.864 acc: 0.328\n",
      "[9,   100] loss: 1.913 acc: 0.336\n",
      "[9,   101] loss: 1.917 acc: 0.355\n",
      "[9,   102] loss: 1.828 acc: 0.383\n",
      "[9,   103] loss: 1.864 acc: 0.324\n",
      "[9,   104] loss: 1.963 acc: 0.273\n",
      "[9,   105] loss: 1.880 acc: 0.328\n",
      "[9,   106] loss: 1.966 acc: 0.324\n",
      "[9,   107] loss: 1.968 acc: 0.328\n",
      "[9,   108] loss: 1.933 acc: 0.324\n",
      "[9,   109] loss: 1.945 acc: 0.297\n",
      "[9,   110] loss: 1.955 acc: 0.320\n",
      "[9,   111] loss: 1.938 acc: 0.301\n",
      "[9,   112] loss: 1.946 acc: 0.277\n",
      "[9,   113] loss: 1.866 acc: 0.359\n",
      "[9,   114] loss: 1.951 acc: 0.297\n",
      "[9,   115] loss: 1.914 acc: 0.316\n",
      "[9,   116] loss: 1.944 acc: 0.293\n",
      "[9,   117] loss: 1.883 acc: 0.312\n",
      "[9,   118] loss: 1.918 acc: 0.316\n",
      "[9,   119] loss: 1.894 acc: 0.336\n",
      "[9,   120] loss: 1.985 acc: 0.332\n",
      "[9,   121] loss: 1.908 acc: 0.340\n",
      "[9,   122] loss: 1.940 acc: 0.332\n",
      "[9,   123] loss: 1.906 acc: 0.383\n",
      "[9,   124] loss: 1.885 acc: 0.312\n",
      "[9,   125] loss: 1.943 acc: 0.328\n",
      "[9,   126] loss: 1.936 acc: 0.332\n",
      "[9,   127] loss: 1.920 acc: 0.297\n",
      "[9,   128] loss: 1.910 acc: 0.363\n",
      "[9,   129] loss: 1.933 acc: 0.281\n",
      "[9,   130] loss: 1.892 acc: 0.332\n",
      "[9,   131] loss: 1.974 acc: 0.297\n",
      "[9,   132] loss: 1.973 acc: 0.297\n",
      "[9,   133] loss: 1.942 acc: 0.293\n",
      "[9,   134] loss: 1.852 acc: 0.383\n",
      "[9,   135] loss: 1.935 acc: 0.320\n",
      "[9,   136] loss: 1.937 acc: 0.320\n",
      "[9,   137] loss: 1.958 acc: 0.258\n",
      "[9,   138] loss: 1.949 acc: 0.312\n",
      "[9,   139] loss: 1.875 acc: 0.355\n",
      "[9,   140] loss: 1.918 acc: 0.281\n",
      "[9,   141] loss: 1.946 acc: 0.324\n",
      "[9,   142] loss: 1.881 acc: 0.355\n",
      "[9,   143] loss: 1.853 acc: 0.363\n",
      "[9,   144] loss: 1.879 acc: 0.352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,   145] loss: 1.968 acc: 0.320\n",
      "[9,   146] loss: 1.962 acc: 0.289\n",
      "[9,   147] loss: 1.950 acc: 0.340\n",
      "[9,   148] loss: 1.939 acc: 0.320\n",
      "[9,   149] loss: 2.033 acc: 0.258\n",
      "[9,   150] loss: 1.918 acc: 0.352\n",
      "[9,   151] loss: 1.892 acc: 0.301\n",
      "[9,   152] loss: 1.870 acc: 0.379\n",
      "[9,   153] loss: 1.871 acc: 0.340\n",
      "[9,   154] loss: 1.945 acc: 0.270\n",
      "[9,   155] loss: 1.965 acc: 0.312\n",
      "[9,   156] loss: 1.998 acc: 0.293\n",
      "[9,   157] loss: 1.921 acc: 0.352\n",
      "[9,   158] loss: 1.968 acc: 0.324\n",
      "[9,   159] loss: 1.828 acc: 0.387\n",
      "[9,   160] loss: 2.023 acc: 0.301\n",
      "[9,   161] loss: 1.970 acc: 0.305\n",
      "[9,   162] loss: 1.903 acc: 0.340\n",
      "[9,   163] loss: 1.950 acc: 0.309\n",
      "[9,   164] loss: 1.902 acc: 0.312\n",
      "[9,   165] loss: 1.943 acc: 0.320\n",
      "[9,   166] loss: 1.847 acc: 0.352\n",
      "[9,   167] loss: 1.998 acc: 0.309\n",
      "[9,   168] loss: 1.954 acc: 0.297\n",
      "[9,   169] loss: 1.933 acc: 0.305\n",
      "[9,   170] loss: 1.923 acc: 0.293\n",
      "[9,   171] loss: 1.899 acc: 0.297\n",
      "[9,   172] loss: 1.905 acc: 0.348\n",
      "[9,   173] loss: 1.924 acc: 0.309\n",
      "[9,   174] loss: 1.924 acc: 0.336\n",
      "[9,   175] loss: 1.950 acc: 0.320\n",
      "[9,   176] loss: 1.904 acc: 0.328\n",
      "[9,   177] loss: 1.920 acc: 0.340\n",
      "[9,   178] loss: 1.987 acc: 0.289\n",
      "[9,   179] loss: 1.906 acc: 0.316\n",
      "[9,   180] loss: 1.928 acc: 0.320\n",
      "[9,   181] loss: 1.867 acc: 0.332\n",
      "[9,   182] loss: 1.964 acc: 0.262\n",
      "[9,   183] loss: 1.928 acc: 0.324\n",
      "[9,   184] loss: 1.977 acc: 0.312\n",
      "[9,   185] loss: 1.868 acc: 0.379\n",
      "[9,   186] loss: 1.907 acc: 0.367\n",
      "[9,   187] loss: 1.912 acc: 0.324\n",
      "[9,   188] loss: 2.014 acc: 0.285\n",
      "[9,   189] loss: 1.902 acc: 0.316\n",
      "[9,   190] loss: 1.922 acc: 0.348\n",
      "[9,   191] loss: 1.920 acc: 0.285\n",
      "[9,   192] loss: 1.912 acc: 0.305\n",
      "[9,   193] loss: 1.862 acc: 0.375\n",
      "[9,   194] loss: 1.959 acc: 0.328\n",
      "[9,   195] loss: 1.913 acc: 0.324\n",
      "[9,   196] loss: 2.066 acc: 0.287\n",
      "[10,     1] loss: 1.894 acc: 0.320\n",
      "[10,     2] loss: 1.893 acc: 0.355\n",
      "[10,     3] loss: 1.894 acc: 0.371\n",
      "[10,     4] loss: 1.920 acc: 0.316\n",
      "[10,     5] loss: 1.873 acc: 0.332\n",
      "[10,     6] loss: 1.867 acc: 0.309\n",
      "[10,     7] loss: 1.881 acc: 0.320\n",
      "[10,     8] loss: 2.004 acc: 0.281\n",
      "[10,     9] loss: 1.926 acc: 0.340\n",
      "[10,    10] loss: 1.898 acc: 0.344\n",
      "[10,    11] loss: 1.908 acc: 0.332\n",
      "[10,    12] loss: 1.881 acc: 0.344\n",
      "[10,    13] loss: 1.933 acc: 0.285\n",
      "[10,    14] loss: 1.928 acc: 0.340\n",
      "[10,    15] loss: 1.885 acc: 0.371\n",
      "[10,    16] loss: 1.977 acc: 0.328\n",
      "[10,    17] loss: 1.916 acc: 0.336\n",
      "[10,    18] loss: 1.947 acc: 0.277\n",
      "[10,    19] loss: 1.872 acc: 0.367\n",
      "[10,    20] loss: 1.958 acc: 0.316\n",
      "[10,    21] loss: 1.933 acc: 0.324\n",
      "[10,    22] loss: 1.895 acc: 0.355\n",
      "[10,    23] loss: 1.864 acc: 0.320\n",
      "[10,    24] loss: 1.891 acc: 0.352\n",
      "[10,    25] loss: 1.880 acc: 0.309\n",
      "[10,    26] loss: 1.911 acc: 0.301\n",
      "[10,    27] loss: 1.962 acc: 0.277\n",
      "[10,    28] loss: 1.875 acc: 0.312\n",
      "[10,    29] loss: 1.889 acc: 0.336\n",
      "[10,    30] loss: 1.961 acc: 0.332\n",
      "[10,    31] loss: 1.901 acc: 0.332\n",
      "[10,    32] loss: 1.880 acc: 0.324\n",
      "[10,    33] loss: 1.948 acc: 0.309\n",
      "[10,    34] loss: 1.911 acc: 0.367\n",
      "[10,    35] loss: 1.984 acc: 0.309\n",
      "[10,    36] loss: 1.956 acc: 0.297\n",
      "[10,    37] loss: 1.946 acc: 0.285\n",
      "[10,    38] loss: 1.909 acc: 0.316\n",
      "[10,    39] loss: 1.919 acc: 0.352\n",
      "[10,    40] loss: 1.998 acc: 0.336\n",
      "[10,    41] loss: 1.947 acc: 0.328\n",
      "[10,    42] loss: 1.909 acc: 0.352\n",
      "[10,    43] loss: 1.888 acc: 0.355\n",
      "[10,    44] loss: 1.901 acc: 0.359\n",
      "[10,    45] loss: 1.943 acc: 0.285\n",
      "[10,    46] loss: 1.833 acc: 0.391\n",
      "[10,    47] loss: 1.964 acc: 0.297\n",
      "[10,    48] loss: 1.908 acc: 0.324\n",
      "[10,    49] loss: 1.951 acc: 0.336\n",
      "[10,    50] loss: 1.917 acc: 0.297\n",
      "[10,    51] loss: 1.923 acc: 0.320\n",
      "[10,    52] loss: 1.967 acc: 0.293\n",
      "[10,    53] loss: 1.920 acc: 0.359\n",
      "[10,    54] loss: 1.887 acc: 0.305\n",
      "[10,    55] loss: 1.922 acc: 0.340\n",
      "[10,    56] loss: 1.933 acc: 0.301\n",
      "[10,    57] loss: 1.950 acc: 0.309\n",
      "[10,    58] loss: 1.940 acc: 0.332\n",
      "[10,    59] loss: 1.984 acc: 0.332\n",
      "[10,    60] loss: 1.936 acc: 0.312\n",
      "[10,    61] loss: 1.977 acc: 0.289\n",
      "[10,    62] loss: 1.896 acc: 0.336\n",
      "[10,    63] loss: 1.950 acc: 0.254\n",
      "[10,    64] loss: 1.938 acc: 0.277\n",
      "[10,    65] loss: 1.960 acc: 0.332\n",
      "[10,    66] loss: 2.013 acc: 0.281\n",
      "[10,    67] loss: 1.913 acc: 0.328\n",
      "[10,    68] loss: 1.955 acc: 0.336\n",
      "[10,    69] loss: 1.924 acc: 0.348\n",
      "[10,    70] loss: 1.929 acc: 0.301\n",
      "[10,    71] loss: 1.905 acc: 0.359\n",
      "[10,    72] loss: 1.905 acc: 0.352\n",
      "[10,    73] loss: 1.956 acc: 0.297\n",
      "[10,    74] loss: 1.890 acc: 0.367\n",
      "[10,    75] loss: 1.932 acc: 0.336\n",
      "[10,    76] loss: 1.965 acc: 0.266\n",
      "[10,    77] loss: 1.890 acc: 0.328\n",
      "[10,    78] loss: 1.915 acc: 0.340\n",
      "[10,    79] loss: 1.926 acc: 0.340\n",
      "[10,    80] loss: 1.915 acc: 0.336\n",
      "[10,    81] loss: 1.953 acc: 0.324\n",
      "[10,    82] loss: 1.867 acc: 0.367\n",
      "[10,    83] loss: 1.919 acc: 0.305\n",
      "[10,    84] loss: 1.884 acc: 0.387\n",
      "[10,    85] loss: 1.855 acc: 0.363\n",
      "[10,    86] loss: 1.948 acc: 0.348\n",
      "[10,    87] loss: 1.956 acc: 0.258\n",
      "[10,    88] loss: 1.865 acc: 0.328\n",
      "[10,    89] loss: 1.872 acc: 0.348\n",
      "[10,    90] loss: 1.927 acc: 0.316\n",
      "[10,    91] loss: 1.904 acc: 0.332\n",
      "[10,    92] loss: 1.854 acc: 0.375\n",
      "[10,    93] loss: 1.864 acc: 0.332\n",
      "[10,    94] loss: 1.884 acc: 0.344\n",
      "[10,    95] loss: 1.842 acc: 0.344\n",
      "[10,    96] loss: 1.853 acc: 0.371\n",
      "[10,    97] loss: 1.908 acc: 0.320\n",
      "[10,    98] loss: 1.927 acc: 0.309\n",
      "[10,    99] loss: 1.852 acc: 0.328\n",
      "[10,   100] loss: 1.899 acc: 0.348\n",
      "[10,   101] loss: 1.907 acc: 0.355\n",
      "[10,   102] loss: 1.813 acc: 0.391\n",
      "[10,   103] loss: 1.848 acc: 0.332\n",
      "[10,   104] loss: 1.952 acc: 0.277\n",
      "[10,   105] loss: 1.869 acc: 0.336\n",
      "[10,   106] loss: 1.956 acc: 0.324\n",
      "[10,   107] loss: 1.958 acc: 0.332\n",
      "[10,   108] loss: 1.921 acc: 0.332\n",
      "[10,   109] loss: 1.934 acc: 0.309\n",
      "[10,   110] loss: 1.947 acc: 0.324\n",
      "[10,   111] loss: 1.925 acc: 0.297\n",
      "[10,   112] loss: 1.933 acc: 0.293\n",
      "[10,   113] loss: 1.852 acc: 0.375\n",
      "[10,   114] loss: 1.937 acc: 0.312\n",
      "[10,   115] loss: 1.902 acc: 0.320\n",
      "[10,   116] loss: 1.932 acc: 0.301\n",
      "[10,   117] loss: 1.872 acc: 0.320\n",
      "[10,   118] loss: 1.904 acc: 0.312\n",
      "[10,   119] loss: 1.882 acc: 0.336\n",
      "[10,   120] loss: 1.975 acc: 0.340\n",
      "[10,   121] loss: 1.893 acc: 0.340\n",
      "[10,   122] loss: 1.928 acc: 0.344\n",
      "[10,   123] loss: 1.893 acc: 0.395\n",
      "[10,   124] loss: 1.871 acc: 0.324\n",
      "[10,   125] loss: 1.931 acc: 0.332\n",
      "[10,   126] loss: 1.925 acc: 0.332\n",
      "[10,   127] loss: 1.907 acc: 0.297\n",
      "[10,   128] loss: 1.899 acc: 0.359\n",
      "[10,   129] loss: 1.923 acc: 0.289\n",
      "[10,   130] loss: 1.881 acc: 0.340\n",
      "[10,   131] loss: 1.963 acc: 0.305\n",
      "[10,   132] loss: 1.967 acc: 0.297\n",
      "[10,   133] loss: 1.932 acc: 0.309\n",
      "[10,   134] loss: 1.837 acc: 0.383\n",
      "[10,   135] loss: 1.925 acc: 0.340\n",
      "[10,   136] loss: 1.925 acc: 0.324\n",
      "[10,   137] loss: 1.946 acc: 0.270\n",
      "[10,   138] loss: 1.939 acc: 0.312\n",
      "[10,   139] loss: 1.860 acc: 0.352\n",
      "[10,   140] loss: 1.907 acc: 0.301\n",
      "[10,   141] loss: 1.933 acc: 0.328\n",
      "[10,   142] loss: 1.868 acc: 0.363\n",
      "[10,   143] loss: 1.842 acc: 0.387\n",
      "[10,   144] loss: 1.867 acc: 0.355\n",
      "[10,   145] loss: 1.959 acc: 0.328\n",
      "[10,   146] loss: 1.951 acc: 0.289\n",
      "[10,   147] loss: 1.939 acc: 0.344\n",
      "[10,   148] loss: 1.927 acc: 0.324\n",
      "[10,   149] loss: 2.024 acc: 0.266\n",
      "[10,   150] loss: 1.908 acc: 0.348\n",
      "[10,   151] loss: 1.881 acc: 0.301\n",
      "[10,   152] loss: 1.856 acc: 0.387\n",
      "[10,   153] loss: 1.858 acc: 0.352\n",
      "[10,   154] loss: 1.933 acc: 0.266\n",
      "[10,   155] loss: 1.955 acc: 0.320\n",
      "[10,   156] loss: 1.989 acc: 0.297\n",
      "[10,   157] loss: 1.908 acc: 0.355\n",
      "[10,   158] loss: 1.958 acc: 0.328\n",
      "[10,   159] loss: 1.815 acc: 0.387\n",
      "[10,   160] loss: 2.014 acc: 0.309\n",
      "[10,   161] loss: 1.959 acc: 0.316\n",
      "[10,   162] loss: 1.893 acc: 0.355\n",
      "[10,   163] loss: 1.939 acc: 0.328\n",
      "[10,   164] loss: 1.890 acc: 0.312\n",
      "[10,   165] loss: 1.934 acc: 0.328\n",
      "[10,   166] loss: 1.832 acc: 0.355\n",
      "[10,   167] loss: 1.988 acc: 0.316\n",
      "[10,   168] loss: 1.944 acc: 0.305\n",
      "[10,   169] loss: 1.920 acc: 0.309\n",
      "[10,   170] loss: 1.913 acc: 0.301\n",
      "[10,   171] loss: 1.887 acc: 0.320\n",
      "[10,   172] loss: 1.896 acc: 0.340\n",
      "[10,   173] loss: 1.914 acc: 0.312\n",
      "[10,   174] loss: 1.911 acc: 0.332\n",
      "[10,   175] loss: 1.939 acc: 0.320\n",
      "[10,   176] loss: 1.889 acc: 0.340\n",
      "[10,   177] loss: 1.910 acc: 0.332\n",
      "[10,   178] loss: 1.974 acc: 0.305\n",
      "[10,   179] loss: 1.892 acc: 0.328\n",
      "[10,   180] loss: 1.917 acc: 0.320\n",
      "[10,   181] loss: 1.853 acc: 0.336\n",
      "[10,   182] loss: 1.954 acc: 0.250\n",
      "[10,   183] loss: 1.916 acc: 0.328\n",
      "[10,   184] loss: 1.970 acc: 0.320\n",
      "[10,   185] loss: 1.855 acc: 0.395\n",
      "[10,   186] loss: 1.894 acc: 0.375\n",
      "[10,   187] loss: 1.902 acc: 0.328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,   188] loss: 2.005 acc: 0.293\n",
      "[10,   189] loss: 1.891 acc: 0.320\n",
      "[10,   190] loss: 1.914 acc: 0.348\n",
      "[10,   191] loss: 1.911 acc: 0.293\n",
      "[10,   192] loss: 1.903 acc: 0.297\n",
      "[10,   193] loss: 1.851 acc: 0.367\n",
      "[10,   194] loss: 1.949 acc: 0.324\n",
      "[10,   195] loss: 1.900 acc: 0.320\n",
      "[10,   196] loss: 2.058 acc: 0.287\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#         if i >= 1:\n",
    "#             break\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01200b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c06257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58777a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        if i >= 1:\n",
    "            break\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdf1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero the parameter gradients\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "parameter_lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(parameter_lst)):\n",
    "    print(parameter_lst[i].shape)\n",
    "\n",
    "for i in range(len(parameter_lst)):\n",
    "    if parameter_lst[i].grad != None:\n",
    "        print(parameter_lst[i].grad.shape)\n",
    "    else:\n",
    "        print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aa7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc28f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f253c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84185a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    acc = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        if i >= 1:\n",
    "            break\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        print(inputs[0][0])\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142373ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "\n",
    "inputs, labels = data\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "\n",
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst[0].grad.shape)\n",
    "vec = torch.reshape(lst[0].grad,(-1,1)).numpy()\n",
    "print(vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95743fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
