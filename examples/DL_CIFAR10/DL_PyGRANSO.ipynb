{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2092f3b6ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32ae4910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Net(nn.Module):\n",
    "                def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "                        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "                        self.pool = nn.MaxPool2d(2, 2)\n",
    "                        self.conv2 = nn.Conv2d(6, 8, 9)\n",
    "                        self.conv2_bn = nn.BatchNorm2d(8)\n",
    "                        self.fc1 = nn.Linear(8 * 3 * 3, 30)\n",
    "                        self.fc1_bn = nn.BatchNorm1d(30)\n",
    "                        self.fc2 = nn.Linear(30, 20)\n",
    "                        self.fc2_bn = nn.BatchNorm1d(20)\n",
    "                        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "                def forward(self, x):\n",
    "                        x = self.pool(F.elu( self.conv1_bn(self.conv1(x))  ))\n",
    "                        x = self.pool(F.elu( self.conv2_bn(self.conv2(x))  ))\n",
    "                        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "                        x = F.elu( self.fc1_bn(self.fc1(x)) )\n",
    "                        x = F.elu( self.fc2_bn(self.fc2(x)) )\n",
    "                        x = self.fc3(x)\n",
    "                        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaef0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5237f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Buyun\\anaconda3\\envs\\profiling_DL_pygranso_env\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2.395 acc: 0.102  norm of grandient =  1.4739505626526026\n",
      "[11,     1] loss: 2.313 acc: 0.140  norm of grandient =  1.2999841136117043\n",
      "[21,     1] loss: 2.211 acc: 0.182  norm of grandient =  0.9388309518502852\n",
      "[31,     1] loss: 2.144 acc: 0.230  norm of grandient =  0.7679406073610656\n",
      "[41,     1] loss: 2.093 acc: 0.254  norm of grandient =  0.6863222340002361\n",
      "[51,     1] loss: 2.050 acc: 0.278  norm of grandient =  0.6494327724560763\n",
      "[61,     1] loss: 2.011 acc: 0.309  norm of grandient =  0.6151031316248401\n",
      "[71,     1] loss: 1.975 acc: 0.329  norm of grandient =  0.5811320765850078\n",
      "[81,     1] loss: 1.943 acc: 0.348  norm of grandient =  0.5520620886651045\n",
      "[91,     1] loss: 1.913 acc: 0.362  norm of grandient =  0.5331753432728125\n",
      "[101,     1] loss: 1.885 acc: 0.372  norm of grandient =  0.5247140102051571\n",
      "[111,     1] loss: 1.858 acc: 0.397  norm of grandient =  0.5188447808296087\n",
      "[121,     1] loss: 1.832 acc: 0.419  norm of grandient =  0.5137068705803638\n",
      "[131,     1] loss: 1.806 acc: 0.429  norm of grandient =  0.5104270750615876\n",
      "[141,     1] loss: 1.780 acc: 0.442  norm of grandient =  0.505661909558276\n",
      "[151,     1] loss: 1.755 acc: 0.458  norm of grandient =  0.501313479025927\n",
      "[161,     1] loss: 1.730 acc: 0.459  norm of grandient =  0.49604001543899995\n",
      "[171,     1] loss: 1.706 acc: 0.474  norm of grandient =  0.4932159745087104\n",
      "[181,     1] loss: 1.682 acc: 0.487  norm of grandient =  0.4902138349507674\n",
      "[191,     1] loss: 1.658 acc: 0.499  norm of grandient =  0.4896952234692782\n",
      "[201,     1] loss: 1.635 acc: 0.511  norm of grandient =  0.48999922117955913\n",
      "[211,     1] loss: 1.611 acc: 0.521  norm of grandient =  0.4904805911386288\n",
      "[221,     1] loss: 1.587 acc: 0.535  norm of grandient =  0.4914225242490259\n",
      "[231,     1] loss: 1.563 acc: 0.541  norm of grandient =  0.4939579469921137\n",
      "[241,     1] loss: 1.539 acc: 0.554  norm of grandient =  0.4932206315516019\n",
      "[251,     1] loss: 1.516 acc: 0.560  norm of grandient =  0.4946788740938346\n",
      "[261,     1] loss: 1.492 acc: 0.569  norm of grandient =  0.49560990695138485\n",
      "[271,     1] loss: 1.467 acc: 0.583  norm of grandient =  0.49890519687038615\n",
      "[281,     1] loss: 1.443 acc: 0.603  norm of grandient =  0.4976949254080082\n",
      "[291,     1] loss: 1.419 acc: 0.617  norm of grandient =  0.49973608960581206\n",
      "[301,     1] loss: 1.394 acc: 0.624  norm of grandient =  0.5026322979435004\n",
      "[311,     1] loss: 1.370 acc: 0.632  norm of grandient =  0.5057164130820878\n",
      "[321,     1] loss: 1.345 acc: 0.652  norm of grandient =  0.5074662895383951\n",
      "[331,     1] loss: 1.320 acc: 0.666  norm of grandient =  0.5036335726379392\n",
      "[341,     1] loss: 1.295 acc: 0.674  norm of grandient =  0.5040649000524786\n",
      "[351,     1] loss: 1.271 acc: 0.690  norm of grandient =  0.5047722741344435\n",
      "[361,     1] loss: 1.246 acc: 0.696  norm of grandient =  0.5061652062965186\n",
      "[371,     1] loss: 1.221 acc: 0.698  norm of grandient =  0.5062471197965515\n",
      "[381,     1] loss: 1.196 acc: 0.715  norm of grandient =  0.5052713149655927\n",
      "[391,     1] loss: 1.172 acc: 0.727  norm of grandient =  0.5060188469091877\n",
      "[401,     1] loss: 1.147 acc: 0.743  norm of grandient =  0.503394412412241\n",
      "[411,     1] loss: 1.123 acc: 0.759  norm of grandient =  0.5035171945258947\n",
      "[421,     1] loss: 1.099 acc: 0.766  norm of grandient =  0.5003069943871671\n",
      "[431,     1] loss: 1.075 acc: 0.772  norm of grandient =  0.5038100828740244\n",
      "[441,     1] loss: 1.051 acc: 0.782  norm of grandient =  0.5040817332347741\n",
      "[451,     1] loss: 1.026 acc: 0.788  norm of grandient =  0.5023805253880952\n",
      "[461,     1] loss: 1.002 acc: 0.797  norm of grandient =  0.49596952178046405\n",
      "[471,     1] loss: 0.979 acc: 0.809  norm of grandient =  0.4922276037855815\n",
      "[481,     1] loss: 0.956 acc: 0.817  norm of grandient =  0.4903464373060085\n",
      "[491,     1] loss: 0.933 acc: 0.827  norm of grandient =  0.4937087829422833\n",
      "[501,     1] loss: 0.910 acc: 0.836  norm of grandient =  0.4868789069278852\n",
      "[511,     1] loss: 0.888 acc: 0.842  norm of grandient =  0.48784685470340783\n",
      "[521,     1] loss: 0.866 acc: 0.851  norm of grandient =  0.48306291326622675\n",
      "[531,     1] loss: 0.844 acc: 0.865  norm of grandient =  0.48930278162057295\n",
      "[541,     1] loss: 0.822 acc: 0.872  norm of grandient =  0.4821110370110333\n",
      "[551,     1] loss: 0.800 acc: 0.879  norm of grandient =  0.4790262463368822\n",
      "[561,     1] loss: 0.778 acc: 0.891  norm of grandient =  0.4837278955449136\n",
      "[571,     1] loss: 0.757 acc: 0.898  norm of grandient =  0.5063175947573153\n",
      "[581,     1] loss: 0.736 acc: 0.904  norm of grandient =  0.47592309669525584\n",
      "[591,     1] loss: 0.715 acc: 0.913  norm of grandient =  0.46929400986403486\n",
      "[601,     1] loss: 0.695 acc: 0.919  norm of grandient =  0.48286550396689565\n",
      "[611,     1] loss: 0.675 acc: 0.925  norm of grandient =  0.4802691974201219\n",
      "[621,     1] loss: 0.656 acc: 0.933  norm of grandient =  0.4620961480655722\n",
      "[631,     1] loss: 0.636 acc: 0.943  norm of grandient =  0.4765695624068054\n",
      "[641,     1] loss: 0.618 acc: 0.946  norm of grandient =  0.45959310199372955\n",
      "[651,     1] loss: 0.599 acc: 0.949  norm of grandient =  0.4911174064009121\n",
      "[661,     1] loss: 0.581 acc: 0.954  norm of grandient =  0.503743367838108\n",
      "[671,     1] loss: 0.564 acc: 0.957  norm of grandient =  0.510303337741258\n",
      "[681,     1] loss: 0.547 acc: 0.959  norm of grandient =  0.5084575317798687\n",
      "[691,     1] loss: 0.531 acc: 0.965  norm of grandient =  0.44152986321275306\n",
      "[701,     1] loss: 0.515 acc: 0.966  norm of grandient =  0.42036604193066185\n",
      "[711,     1] loss: 0.499 acc: 0.969  norm of grandient =  0.43288888122179575\n",
      "[721,     1] loss: 0.484 acc: 0.970  norm of grandient =  0.6630715343577755\n",
      "[731,     1] loss: 0.469 acc: 0.973  norm of grandient =  0.4067561613085617\n",
      "[741,     1] loss: 0.455 acc: 0.974  norm of grandient =  0.4092848240464859\n",
      "[751,     1] loss: 0.441 acc: 0.977  norm of grandient =  0.6169348325423789\n",
      "[761,     1] loss: 0.428 acc: 0.983  norm of grandient =  0.4010963764900051\n",
      "[771,     1] loss: 0.415 acc: 0.985  norm of grandient =  0.4825756803912206\n",
      "[781,     1] loss: 0.402 acc: 0.985  norm of grandient =  0.6009848156821297\n",
      "[791,     1] loss: 0.390 acc: 0.986  norm of grandient =  0.4945309386520908\n",
      "[801,     1] loss: 0.378 acc: 0.991  norm of grandient =  0.5447022968967669\n",
      "[811,     1] loss: 0.367 acc: 0.992  norm of grandient =  0.6172079959733044\n",
      "[821,     1] loss: 0.356 acc: 0.995  norm of grandient =  0.6382287839307236\n",
      "[831,     1] loss: 0.345 acc: 0.995  norm of grandient =  0.8142136912192599\n",
      "[841,     1] loss: 0.335 acc: 0.995  norm of grandient =  0.44836280575257875\n",
      "[851,     1] loss: 0.325 acc: 0.996  norm of grandient =  0.7788786899999307\n",
      "[861,     1] loss: 0.316 acc: 0.998  norm of grandient =  0.37869884617879584\n",
      "[871,     1] loss: 0.307 acc: 0.999  norm of grandient =  0.7281612206344644\n",
      "[881,     1] loss: 0.298 acc: 0.999  norm of grandient =  0.33941116461002874\n",
      "[891,     1] loss: 0.289 acc: 0.999  norm of grandient =  0.8396287006348396\n",
      "[901,     1] loss: 0.281 acc: 0.999  norm of grandient =  0.5769777514766127\n",
      "[911,     1] loss: 0.273 acc: 0.999  norm of grandient =  0.3065243880138522\n",
      "[921,     1] loss: 0.265 acc: 0.999  norm of grandient =  0.33150661371949275\n",
      "[931,     1] loss: 0.258 acc: 0.999  norm of grandient =  0.6158205792282475\n",
      "[941,     1] loss: 0.251 acc: 0.999  norm of grandient =  0.5760866546172746\n",
      "[951,     1] loss: 0.244 acc: 0.999  norm of grandient =  0.6174313060045994\n",
      "[961,     1] loss: 0.237 acc: 0.999  norm of grandient =  0.3706254547546403\n",
      "[971,     1] loss: 0.231 acc: 0.999  norm of grandient =  0.7732195121059445\n",
      "[981,     1] loss: 0.225 acc: 0.999  norm of grandient =  0.6086557422275309\n",
      "[991,     1] loss: 0.219 acc: 0.999  norm of grandient =  0.4271546897996588\n",
      "[1001,     1] loss: 0.213 acc: 0.999  norm of grandient =  0.28297771066616934\n",
      "[1011,     1] loss: 0.207 acc: 0.999  norm of grandient =  0.7705076658367973\n",
      "[1021,     1] loss: 0.202 acc: 0.999  norm of grandient =  0.37865839685786723\n",
      "[1031,     1] loss: 0.197 acc: 0.999  norm of grandient =  0.5575797008112294\n",
      "[1041,     1] loss: 0.192 acc: 0.999  norm of grandient =  0.29613190755819974\n",
      "[1051,     1] loss: 0.187 acc: 0.999  norm of grandient =  0.5289647458617799\n",
      "[1061,     1] loss: 0.182 acc: 0.999  norm of grandient =  0.705956412986789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1071,     1] loss: 0.178 acc: 0.999  norm of grandient =  0.41669019303349797\n",
      "[1081,     1] loss: 0.174 acc: 0.999  norm of grandient =  0.39816541116817467\n",
      "[1091,     1] loss: 0.169 acc: 1.000  norm of grandient =  0.4295673735625767\n",
      "[1101,     1] loss: 0.165 acc: 1.000  norm of grandient =  0.2754686265166675\n",
      "[1111,     1] loss: 0.161 acc: 1.000  norm of grandient =  0.4526502819520021\n",
      "[1121,     1] loss: 0.158 acc: 1.000  norm of grandient =  0.41782445982682864\n",
      "[1131,     1] loss: 0.154 acc: 1.000  norm of grandient =  0.39123559946434244\n",
      "[1141,     1] loss: 0.151 acc: 1.000  norm of grandient =  0.3965735452545899\n",
      "[1151,     1] loss: 0.147 acc: 1.000  norm of grandient =  0.5810197250541373\n",
      "[1161,     1] loss: 0.144 acc: 1.000  norm of grandient =  0.1981287833705844\n",
      "[1171,     1] loss: 0.141 acc: 1.000  norm of grandient =  0.47722628108920484\n",
      "[1181,     1] loss: 0.138 acc: 1.000  norm of grandient =  0.19625501750385443\n",
      "[1191,     1] loss: 0.135 acc: 1.000  norm of grandient =  0.41520872358058425\n",
      "[1201,     1] loss: 0.132 acc: 1.000  norm of grandient =  0.2205674342627218\n",
      "[1211,     1] loss: 0.129 acc: 1.000  norm of grandient =  0.18894040700309261\n",
      "[1221,     1] loss: 0.126 acc: 1.000  norm of grandient =  0.34881043622218727\n",
      "[1231,     1] loss: 0.124 acc: 1.000  norm of grandient =  0.33790211741502285\n",
      "[1241,     1] loss: 0.121 acc: 1.000  norm of grandient =  0.4442214386343977\n",
      "[1251,     1] loss: 0.119 acc: 1.000  norm of grandient =  0.3919937437960924\n",
      "[1261,     1] loss: 0.116 acc: 1.000  norm of grandient =  0.18355835312127983\n",
      "[1271,     1] loss: 0.114 acc: 1.000  norm of grandient =  0.19186693080604617\n",
      "[1281,     1] loss: 0.112 acc: 1.000  norm of grandient =  0.24218549601804146\n",
      "[1291,     1] loss: 0.110 acc: 1.000  norm of grandient =  0.2750163031087681\n",
      "[1301,     1] loss: 0.107 acc: 1.000  norm of grandient =  0.23625179380493058\n",
      "[1311,     1] loss: 0.105 acc: 1.000  norm of grandient =  0.2555956534585427\n",
      "[1321,     1] loss: 0.103 acc: 1.000  norm of grandient =  0.16747765349502686\n",
      "[1331,     1] loss: 0.102 acc: 1.000  norm of grandient =  0.3100752152849583\n",
      "[1341,     1] loss: 0.100 acc: 1.000  norm of grandient =  0.23723478878583282\n",
      "[1351,     1] loss: 0.098 acc: 1.000  norm of grandient =  0.2861454391501889\n",
      "[1361,     1] loss: 0.096 acc: 1.000  norm of grandient =  0.1482619484335105\n",
      "[1371,     1] loss: 0.094 acc: 1.000  norm of grandient =  0.15034576827778293\n",
      "[1381,     1] loss: 0.093 acc: 1.000  norm of grandient =  0.19778712335291324\n",
      "[1391,     1] loss: 0.091 acc: 1.000  norm of grandient =  0.3542970807225321\n",
      "[1401,     1] loss: 0.090 acc: 1.000  norm of grandient =  0.25766921956672595\n",
      "[1411,     1] loss: 0.088 acc: 1.000  norm of grandient =  0.29062151646380974\n",
      "[1421,     1] loss: 0.087 acc: 1.000  norm of grandient =  0.19387986239608795\n",
      "[1431,     1] loss: 0.085 acc: 1.000  norm of grandient =  0.1679118500735428\n",
      "[1441,     1] loss: 0.084 acc: 1.000  norm of grandient =  0.316299449726846\n",
      "[1451,     1] loss: 0.082 acc: 1.000  norm of grandient =  0.2245439765029378\n",
      "[1461,     1] loss: 0.081 acc: 1.000  norm of grandient =  0.1640994986930392\n",
      "[1471,     1] loss: 0.080 acc: 1.000  norm of grandient =  0.25275510012844776\n",
      "[1481,     1] loss: 0.079 acc: 1.000  norm of grandient =  0.22446895287990234\n",
      "[1491,     1] loss: 0.077 acc: 1.000  norm of grandient =  0.18998891699635592\n",
      "[1501,     1] loss: 0.076 acc: 1.000  norm of grandient =  0.22498221425386083\n",
      "[1511,     1] loss: 0.075 acc: 1.000  norm of grandient =  0.18195032327985142\n",
      "[1521,     1] loss: 0.074 acc: 1.000  norm of grandient =  0.20763196322050234\n",
      "[1531,     1] loss: 0.073 acc: 1.000  norm of grandient =  0.19745037757089334\n",
      "[1541,     1] loss: 0.072 acc: 1.000  norm of grandient =  0.1260966019500913\n",
      "[1551,     1] loss: 0.071 acc: 1.000  norm of grandient =  0.23136218379852308\n",
      "[1561,     1] loss: 0.070 acc: 1.000  norm of grandient =  0.14312320427575728\n",
      "[1571,     1] loss: 0.069 acc: 1.000  norm of grandient =  0.1250443386611086\n",
      "[1581,     1] loss: 0.068 acc: 1.000  norm of grandient =  0.12667851535332067\n",
      "[1591,     1] loss: 0.067 acc: 1.000  norm of grandient =  0.1426299769840187\n",
      "[1601,     1] loss: 0.066 acc: 1.000  norm of grandient =  0.11922908045761602\n",
      "[1611,     1] loss: 0.065 acc: 1.000  norm of grandient =  0.19316059455107226\n",
      "[1621,     1] loss: 0.064 acc: 1.000  norm of grandient =  0.118733955359936\n",
      "[1631,     1] loss: 0.063 acc: 1.000  norm of grandient =  0.15908194287964436\n",
      "[1641,     1] loss: 0.062 acc: 1.000  norm of grandient =  0.19749259032504962\n",
      "[1651,     1] loss: 0.061 acc: 1.000  norm of grandient =  0.16637038661290274\n",
      "[1661,     1] loss: 0.061 acc: 1.000  norm of grandient =  0.14604008242213165\n",
      "[1671,     1] loss: 0.060 acc: 1.000  norm of grandient =  0.12445642117612674\n",
      "[1681,     1] loss: 0.059 acc: 1.000  norm of grandient =  0.20963394075446745\n",
      "[1691,     1] loss: 0.058 acc: 1.000  norm of grandient =  0.13497803513400336\n",
      "[1701,     1] loss: 0.057 acc: 1.000  norm of grandient =  0.1034582896668891\n",
      "[1711,     1] loss: 0.057 acc: 1.000  norm of grandient =  0.15776399590867626\n",
      "[1721,     1] loss: 0.056 acc: 1.000  norm of grandient =  0.12391595608682963\n",
      "[1731,     1] loss: 0.055 acc: 1.000  norm of grandient =  0.17483145039353182\n",
      "[1741,     1] loss: 0.055 acc: 1.000  norm of grandient =  0.15971958926930846\n",
      "[1751,     1] loss: 0.054 acc: 1.000  norm of grandient =  0.12059065216205557\n",
      "[1761,     1] loss: 0.053 acc: 1.000  norm of grandient =  0.12057132309379087\n",
      "[1771,     1] loss: 0.053 acc: 1.000  norm of grandient =  0.14210317973165373\n",
      "[1781,     1] loss: 0.052 acc: 1.000  norm of grandient =  0.09457263682107442\n",
      "[1791,     1] loss: 0.052 acc: 1.000  norm of grandient =  0.08919021525335097\n",
      "[1801,     1] loss: 0.051 acc: 1.000  norm of grandient =  0.09139112073265504\n",
      "[1811,     1] loss: 0.050 acc: 1.000  norm of grandient =  0.1262581986317993\n",
      "[1821,     1] loss: 0.050 acc: 1.000  norm of grandient =  0.16716120035767215\n",
      "[1831,     1] loss: 0.049 acc: 1.000  norm of grandient =  0.15953776370543235\n",
      "[1841,     1] loss: 0.049 acc: 1.000  norm of grandient =  0.08564500931529412\n",
      "[1851,     1] loss: 0.048 acc: 1.000  norm of grandient =  0.09100095702417724\n",
      "[1861,     1] loss: 0.048 acc: 1.000  norm of grandient =  0.16263031625146618\n",
      "[1871,     1] loss: 0.047 acc: 1.000  norm of grandient =  0.17536566251933688\n",
      "[1881,     1] loss: 0.047 acc: 1.000  norm of grandient =  0.09066894998412653\n",
      "[1891,     1] loss: 0.046 acc: 1.000  norm of grandient =  0.0989933441808838\n",
      "[1901,     1] loss: 0.046 acc: 1.000  norm of grandient =  0.08507489175083345\n",
      "[1911,     1] loss: 0.045 acc: 1.000  norm of grandient =  0.09218578918137652\n",
      "[1921,     1] loss: 0.045 acc: 1.000  norm of grandient =  0.14451714931199144\n",
      "[1931,     1] loss: 0.044 acc: 1.000  norm of grandient =  0.09429634685915528\n",
      "[1941,     1] loss: 0.044 acc: 1.000  norm of grandient =  0.13906887527434603\n",
      "[1951,     1] loss: 0.043 acc: 1.000  norm of grandient =  0.10201717590420568\n",
      "[1961,     1] loss: 0.043 acc: 1.000  norm of grandient =  0.12652956140240043\n",
      "[1971,     1] loss: 0.042 acc: 1.000  norm of grandient =  0.10083094080504326\n",
      "[1981,     1] loss: 0.042 acc: 1.000  norm of grandient =  0.1295235666683333\n",
      "[1991,     1] loss: 0.041 acc: 1.000  norm of grandient =  0.11256437344671234\n",
      "Finished Training\n",
      "total time = 195.96647572517395 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "start = time.time()\n",
    "for epoch in range(2000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss = loss.item()\n",
    "\n",
    "    acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, 0 + 1, running_loss, acc ), end = \"\")\n",
    "        \n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        print( \"  norm of grandient = \", total_norm )\n",
    "        \n",
    "#     print(\"\\n\\n\\n\\n\\nPrint Gradient\\n\\n\\n\\n\\n\")\n",
    "#     lst = list(model.parameters())\n",
    "\n",
    "#     for i in range(len(lst)):\n",
    "# #         print(lst[i].grad.shape)\n",
    "#         print( \"norm of grandient = \", torch.norm(lst[i].grad) )\n",
    "\n",
    "print('Finished Training')\n",
    "end = time.time()\n",
    "print(\"total time = {} s\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82074dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]\n",
    "# labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvar = 10160\n",
    "# x = .1 * np.ones((nvar,1))\n",
    "# x_torch = torch.from_numpy(x).cuda()\n",
    "# torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    print(lst[i].grad.shape)\n",
    "    print(lst[i].grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caa17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = 10160\n",
    "x = .1 * np.ones((nvar,1))\n",
    "x_torch = torch.from_numpy(x).cuda()\n",
    "torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 196 == 195:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 196))\n",
    "            running_loss = 0.0\n",
    "            acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "            print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#         if i >= 1:\n",
    "#             break\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01200b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c06257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58777a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        if i >= 1:\n",
    "            break\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cdf1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n",
      "torch.Size([8, 6, 9, 9])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([30, 72])\n",
      "torch.Size([30])\n",
      "torch.Size([30])\n",
      "torch.Size([30])\n",
      "torch.Size([20, 30])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "parameter_lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(parameter_lst)):\n",
    "    print(parameter_lst[i].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aa7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc28f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f253c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84185a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    acc = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        if i >= 1:\n",
    "            break\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        print(inputs[0][0])\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142373ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "\n",
    "inputs, labels = data\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "\n",
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst[0].grad.shape)\n",
    "vec = torch.reshape(lst[0].grad,(-1,1)).numpy()\n",
    "print(vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95743fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
