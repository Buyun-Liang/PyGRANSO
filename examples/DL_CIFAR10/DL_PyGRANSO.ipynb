{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1beb2134ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32ae4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Net(nn.Module):\n",
    "                def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.conv1 = nn.Conv2d(3, 4, 5)\n",
    "                        self.conv1_bn = nn.BatchNorm2d(4)\n",
    "                        self.pool = nn.MaxPool2d(2, 2)\n",
    "                        self.conv2 = nn.Conv2d(4,5,2)\n",
    "                        self.conv2_bn = nn.BatchNorm2d(5)\n",
    "                        self.fc1 = nn.Linear(5* 3 * 3,30)\n",
    "                        self.fc1_bn = nn.BatchNorm1d(30)\n",
    "                        self.fc2 = nn.Linear(30, 20)\n",
    "                        self.fc2_bn = nn.BatchNorm1d(20)\n",
    "                        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "                def forward(self, x):\n",
    "                        x = self.pool ( self.pool(F.elu( self.conv1_bn(self.conv1(x))  )) )\n",
    "                        x = self.pool(F.elu( self.conv2_bn(self.conv2(x))  ))\n",
    "                        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "                        x = F.elu( self.fc1_bn(self.fc1(x)) )\n",
    "                        x = F.elu( self.fc2_bn(self.fc2(x)) )\n",
    "                        x = self.fc3(x)\n",
    "                        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0fabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Net(nn.Module):\n",
    "                def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "                        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "                        self.pool = nn.MaxPool2d(2, 2)\n",
    "                        self.conv2 = nn.Conv2d(6, 8, 9)\n",
    "                        self.conv2_bn = nn.BatchNorm2d(8)\n",
    "                        self.fc1 = nn.Linear(8 * 3 * 3, 30)\n",
    "                        self.fc1_bn = nn.BatchNorm1d(30)\n",
    "                        self.fc2 = nn.Linear(30, 20)\n",
    "                        self.fc2_bn = nn.BatchNorm1d(20)\n",
    "                        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "                def forward(self, x):\n",
    "                        x = self.pool(F.elu( self.conv1_bn(self.conv1(x))  ))\n",
    "                        x = self.pool(F.elu( self.conv2_bn(self.conv2(x))  ))\n",
    "                        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "                        x = F.elu( self.fc1_bn(self.fc1(x)) )\n",
    "                        x = F.elu( self.fc2_bn(self.fc2(x)) )\n",
    "                        x = self.fc3(x)\n",
    "                        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaef0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5237f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2.353 acc: 0.128  norm of grandient =  1.4402415957978174\n",
      "[11,     1] loss: 2.276 acc: 0.169  norm of grandient =  1.241718225798892\n",
      "[21,     1] loss: 2.181 acc: 0.227  norm of grandient =  0.9353518917866934\n",
      "[31,     1] loss: 2.111 acc: 0.261  norm of grandient =  0.814030358475578\n",
      "[41,     1] loss: 2.054 acc: 0.285  norm of grandient =  0.7362462299154036\n",
      "[51,     1] loss: 2.004 acc: 0.303  norm of grandient =  0.675922450179253\n",
      "[61,     1] loss: 1.961 acc: 0.319  norm of grandient =  0.6284776375610205\n",
      "[71,     1] loss: 1.923 acc: 0.338  norm of grandient =  0.5929160294429443\n",
      "[81,     1] loss: 1.889 acc: 0.357  norm of grandient =  0.5645120698608272\n",
      "[91,     1] loss: 1.858 acc: 0.373  norm of grandient =  0.545179861204982\n",
      "[101,     1] loss: 1.829 acc: 0.392  norm of grandient =  0.5284698978996761\n",
      "[111,     1] loss: 1.802 acc: 0.414  norm of grandient =  0.5157439417949534\n",
      "[121,     1] loss: 1.776 acc: 0.418  norm of grandient =  0.5100982695094054\n",
      "[131,     1] loss: 1.750 acc: 0.438  norm of grandient =  0.5057945802905739\n",
      "[141,     1] loss: 1.725 acc: 0.452  norm of grandient =  0.5057779557883976\n",
      "[151,     1] loss: 1.699 acc: 0.467  norm of grandient =  0.5055498008186761\n",
      "[161,     1] loss: 1.674 acc: 0.476  norm of grandient =  0.5033567019351761\n",
      "[171,     1] loss: 1.649 acc: 0.484  norm of grandient =  0.5044948242430651\n",
      "[181,     1] loss: 1.624 acc: 0.498  norm of grandient =  0.5068858858634638\n",
      "[191,     1] loss: 1.599 acc: 0.509  norm of grandient =  0.5048254591457578\n",
      "[201,     1] loss: 1.574 acc: 0.528  norm of grandient =  0.5056926972690755\n",
      "[211,     1] loss: 1.549 acc: 0.539  norm of grandient =  0.5013983095109371\n",
      "[221,     1] loss: 1.524 acc: 0.550  norm of grandient =  0.5034716519125966\n",
      "[231,     1] loss: 1.500 acc: 0.576  norm of grandient =  0.5033883495420458\n",
      "[241,     1] loss: 1.475 acc: 0.584  norm of grandient =  0.5000351455216108\n",
      "[251,     1] loss: 1.451 acc: 0.596  norm of grandient =  0.5007165736149145\n",
      "[261,     1] loss: 1.426 acc: 0.611  norm of grandient =  0.49759514371721303\n",
      "[271,     1] loss: 1.402 acc: 0.620  norm of grandient =  0.5009309120313177\n",
      "[281,     1] loss: 1.377 acc: 0.633  norm of grandient =  0.4994561579893799\n",
      "[291,     1] loss: 1.353 acc: 0.647  norm of grandient =  0.502349200855393\n",
      "[301,     1] loss: 1.329 acc: 0.658  norm of grandient =  0.5035248656604315\n",
      "[311,     1] loss: 1.304 acc: 0.670  norm of grandient =  0.5052763014630605\n",
      "[321,     1] loss: 1.280 acc: 0.677  norm of grandient =  0.5057876115320956\n",
      "[331,     1] loss: 1.255 acc: 0.688  norm of grandient =  0.5034017028294879\n",
      "[341,     1] loss: 1.230 acc: 0.700  norm of grandient =  0.506414313895095\n",
      "[351,     1] loss: 1.206 acc: 0.711  norm of grandient =  0.5032487507653556\n",
      "[361,     1] loss: 1.181 acc: 0.717  norm of grandient =  0.502846352577562\n",
      "[371,     1] loss: 1.157 acc: 0.734  norm of grandient =  0.5035790960543753\n",
      "[381,     1] loss: 1.132 acc: 0.747  norm of grandient =  0.5041847789078867\n",
      "[391,     1] loss: 1.108 acc: 0.753  norm of grandient =  0.50372075325065\n",
      "[401,     1] loss: 1.084 acc: 0.760  norm of grandient =  0.5019079398645283\n",
      "[411,     1] loss: 1.059 acc: 0.774  norm of grandient =  0.49968694815201575\n",
      "[421,     1] loss: 1.036 acc: 0.782  norm of grandient =  0.49947738394294383\n",
      "[431,     1] loss: 1.012 acc: 0.788  norm of grandient =  0.4954060871489555\n",
      "[441,     1] loss: 0.988 acc: 0.799  norm of grandient =  0.49542096258953394\n",
      "[451,     1] loss: 0.965 acc: 0.803  norm of grandient =  0.49552928853845046\n",
      "[461,     1] loss: 0.941 acc: 0.813  norm of grandient =  0.49542588459949893\n",
      "[471,     1] loss: 0.918 acc: 0.822  norm of grandient =  0.49550968757587766\n",
      "[481,     1] loss: 0.894 acc: 0.828  norm of grandient =  0.49236051452333707\n",
      "[491,     1] loss: 0.871 acc: 0.838  norm of grandient =  0.4897568184733395\n",
      "[501,     1] loss: 0.849 acc: 0.847  norm of grandient =  0.48974113363850286\n",
      "[511,     1] loss: 0.826 acc: 0.858  norm of grandient =  0.48416308988278656\n",
      "[521,     1] loss: 0.804 acc: 0.870  norm of grandient =  0.4832385637683481\n",
      "[531,     1] loss: 0.782 acc: 0.878  norm of grandient =  0.47693902472859856\n",
      "[541,     1] loss: 0.760 acc: 0.886  norm of grandient =  0.474463958391905\n",
      "[551,     1] loss: 0.739 acc: 0.896  norm of grandient =  0.4662653896455247\n",
      "[561,     1] loss: 0.719 acc: 0.903  norm of grandient =  0.4673693614843859\n",
      "[571,     1] loss: 0.698 acc: 0.908  norm of grandient =  0.46274279789457173\n",
      "[581,     1] loss: 0.679 acc: 0.912  norm of grandient =  0.45815607734292413\n",
      "[591,     1] loss: 0.659 acc: 0.920  norm of grandient =  0.4523738325911019\n",
      "[601,     1] loss: 0.640 acc: 0.925  norm of grandient =  0.4476900104468411\n",
      "[611,     1] loss: 0.622 acc: 0.929  norm of grandient =  0.4381452936559415\n",
      "[621,     1] loss: 0.604 acc: 0.931  norm of grandient =  0.4375480051005804\n",
      "[631,     1] loss: 0.587 acc: 0.933  norm of grandient =  0.43064952308488\n",
      "[641,     1] loss: 0.570 acc: 0.937  norm of grandient =  0.43621670509112137\n",
      "[651,     1] loss: 0.553 acc: 0.941  norm of grandient =  0.4256782544330956\n",
      "[661,     1] loss: 0.537 acc: 0.945  norm of grandient =  0.4174149269310124\n",
      "[671,     1] loss: 0.521 acc: 0.949  norm of grandient =  0.4133204986969687\n",
      "[681,     1] loss: 0.505 acc: 0.953  norm of grandient =  0.4081266907668757\n",
      "[691,     1] loss: 0.490 acc: 0.957  norm of grandient =  0.4038321536872957\n",
      "[701,     1] loss: 0.476 acc: 0.961  norm of grandient =  0.3950019765244613\n",
      "[711,     1] loss: 0.462 acc: 0.970  norm of grandient =  0.38871034721421377\n",
      "[721,     1] loss: 0.448 acc: 0.972  norm of grandient =  0.3959216057033913\n",
      "[731,     1] loss: 0.434 acc: 0.975  norm of grandient =  0.38913038631629426\n",
      "[741,     1] loss: 0.421 acc: 0.978  norm of grandient =  0.38131187309794745\n",
      "[751,     1] loss: 0.409 acc: 0.980  norm of grandient =  0.3702225736537974\n",
      "[761,     1] loss: 0.396 acc: 0.981  norm of grandient =  0.3669500974237727\n",
      "[771,     1] loss: 0.384 acc: 0.981  norm of grandient =  0.3607143576789799\n",
      "[781,     1] loss: 0.373 acc: 0.982  norm of grandient =  0.37271375542540985\n",
      "[791,     1] loss: 0.362 acc: 0.985  norm of grandient =  0.3600398494846823\n",
      "[801,     1] loss: 0.351 acc: 0.986  norm of grandient =  0.3492391357667381\n",
      "[811,     1] loss: 0.341 acc: 0.987  norm of grandient =  0.34223970468902737\n",
      "[821,     1] loss: 0.331 acc: 0.989  norm of grandient =  0.330895845562275\n",
      "[831,     1] loss: 0.321 acc: 0.989  norm of grandient =  0.33277933332253207\n",
      "[841,     1] loss: 0.311 acc: 0.989  norm of grandient =  0.338391737829396\n",
      "[851,     1] loss: 0.302 acc: 0.990  norm of grandient =  0.3164679772375035\n",
      "[861,     1] loss: 0.294 acc: 0.992  norm of grandient =  0.3253224845270809\n",
      "[871,     1] loss: 0.285 acc: 0.993  norm of grandient =  0.3091135897349628\n",
      "[881,     1] loss: 0.277 acc: 0.993  norm of grandient =  0.31180873002826104\n",
      "[891,     1] loss: 0.269 acc: 0.994  norm of grandient =  0.3037844098676362\n",
      "[901,     1] loss: 0.262 acc: 0.995  norm of grandient =  0.29996468121226116\n",
      "[911,     1] loss: 0.254 acc: 0.996  norm of grandient =  0.3095488310454301\n",
      "[921,     1] loss: 0.247 acc: 0.996  norm of grandient =  0.31042034708445054\n",
      "[931,     1] loss: 0.240 acc: 0.996  norm of grandient =  0.28991543310433415\n",
      "[941,     1] loss: 0.234 acc: 0.996  norm of grandient =  0.2760329402843274\n",
      "[951,     1] loss: 0.228 acc: 0.998  norm of grandient =  0.2700444110722229\n",
      "[961,     1] loss: 0.221 acc: 0.998  norm of grandient =  0.27254070509104705\n",
      "[971,     1] loss: 0.215 acc: 0.998  norm of grandient =  0.288892384181848\n",
      "[981,     1] loss: 0.210 acc: 0.998  norm of grandient =  0.26609958201277767\n",
      "[991,     1] loss: 0.204 acc: 0.998  norm of grandient =  0.2655828043203438\n",
      "[1001,     1] loss: 0.199 acc: 0.998  norm of grandient =  0.2745043847397555\n",
      "[1011,     1] loss: 0.194 acc: 0.999  norm of grandient =  0.24405135977741668\n",
      "[1021,     1] loss: 0.189 acc: 0.999  norm of grandient =  0.27518719120371365\n",
      "[1031,     1] loss: 0.184 acc: 0.999  norm of grandient =  0.2574702595952034\n",
      "[1041,     1] loss: 0.179 acc: 0.999  norm of grandient =  0.23979722876579448\n",
      "[1051,     1] loss: 0.175 acc: 0.999  norm of grandient =  0.22819127240593703\n",
      "[1061,     1] loss: 0.171 acc: 0.999  norm of grandient =  0.22587664992358084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1071,     1] loss: 0.167 acc: 1.000  norm of grandient =  0.2227607614843219\n",
      "[1081,     1] loss: 0.163 acc: 1.000  norm of grandient =  0.23749203969550453\n",
      "[1091,     1] loss: 0.159 acc: 1.000  norm of grandient =  0.23254958723517544\n",
      "[1101,     1] loss: 0.155 acc: 1.000  norm of grandient =  0.2188262557430781\n",
      "[1111,     1] loss: 0.151 acc: 1.000  norm of grandient =  0.22589757915538733\n",
      "[1121,     1] loss: 0.148 acc: 1.000  norm of grandient =  0.24833010968399097\n",
      "[1131,     1] loss: 0.145 acc: 1.000  norm of grandient =  0.20345589324625313\n",
      "[1141,     1] loss: 0.141 acc: 1.000  norm of grandient =  0.21560788089892494\n",
      "[1151,     1] loss: 0.138 acc: 1.000  norm of grandient =  0.22242828389398955\n",
      "[1161,     1] loss: 0.135 acc: 1.000  norm of grandient =  0.2291940197549515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12372/3712239399.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\notebook\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\notebook\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "\n",
    "for epoch in range(2000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss = loss.item()\n",
    "\n",
    "    acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, 0 + 1, running_loss, acc ), end = \"\")\n",
    "        \n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        print( \"  norm of grandient = \", total_norm )\n",
    "        \n",
    "#     print(\"\\n\\n\\n\\n\\nPrint Gradient\\n\\n\\n\\n\\n\")\n",
    "#     lst = list(model.parameters())\n",
    "\n",
    "#     for i in range(len(lst)):\n",
    "# #         print(lst[i].grad.shape)\n",
    "#         print( \"norm of grandient = \", torch.norm(lst[i].grad) )\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82074dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]\n",
    "# labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvar = 10160\n",
    "# x = .1 * np.ones((nvar,1))\n",
    "# x_torch = torch.from_numpy(x).cuda()\n",
    "# torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    print(lst[i].grad.shape)\n",
    "    print(lst[i].grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caa17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = 10160\n",
    "x = .1 * np.ones((nvar,1))\n",
    "x_torch = torch.from_numpy(x).cuda()\n",
    "torch.nn.utils.vector_to_parameters(x_torch, model.parameters())\n",
    "\n",
    "lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 196 == 195:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 196))\n",
    "            running_loss = 0.0\n",
    "            acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "            print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#         if i >= 1:\n",
    "#             break\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01200b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c06257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58777a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc0 = nn.Linear(3*4*4, 20)\n",
    "        self.fc01 = nn.Linear(20, 10)\n",
    "#         self.fc1 = nn.Linear(3 * 16 * 16, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc01(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#                 def __init__(self):\n",
    "#                         super().__init__()\n",
    "#                         self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#                         self.fc1 = nn.Linear(3 * 8 * 8, 120)\n",
    "#                         self.fc2 = nn.Linear(120, 84)\n",
    "#                         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#                 def forward(self, x):\n",
    "#                         x = self.pool(x)\n",
    "#                         x = self.pool(x)\n",
    "#                         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#                         x = self.fc1(x)\n",
    "#                         x = self.fc2(x)\n",
    "#                         x = self.fc3(x)\n",
    "#                         return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        if i >= 1:\n",
    "            break\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cdf1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n",
      "torch.Size([6])\n",
      "torch.Size([8, 6, 9, 9])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([30, 72])\n",
      "torch.Size([30])\n",
      "torch.Size([30])\n",
      "torch.Size([30])\n",
      "torch.Size([20, 30])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "parameter_lst = list(model.parameters())\n",
    "\n",
    "for i in range(len(parameter_lst)):\n",
    "    print(parameter_lst[i].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aa7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc28f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f253c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84185a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    acc = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        if i >= 1:\n",
    "            break\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        print(inputs[0][0])\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        acc = (outputs.max(1)[1] == labels).sum().item()/labels.size(0)\n",
    "        \n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss, acc ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142373ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "\n",
    "inputs, labels = data\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "\n",
    "lst = list(model.parameters())\n",
    "for i in range(len(lst)):\n",
    "#     print(lst[i].shape)\n",
    "    print(lst[i].grad.shape)\n",
    "    if i == 0:\n",
    "        print(lst[0].grad[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst[0].grad.shape)\n",
    "vec = torch.reshape(lst[0].grad,(-1,1)).numpy()\n",
    "print(vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95743fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
